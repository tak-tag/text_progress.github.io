[["index.html", "マーケティングリサーチの概要・理屈・実践 テキスト進捗ページ Chapter 1 はじめに（本書について）", " マーケティングリサーチの概要・理屈・実践 テキスト進捗ページ 田頭拓己 神戸大学大学院経営学研究科 2025-03-15 Chapter 1 はじめに（本書について） 本書は、マーケティング領域において定量的な研究（マーケティングリサーチ1）を行うために必要な知識や原理原則に加え、研究を実践するうえで必要な考え方も含めて解説するための教科書である。特に、暗黙知的になってしまい、大学や大学院の講義で解説されにくい点も含め、定量的なマーケティングリサーチを行うために必要な知識やその背景にある原理原則を解説することを目指す。本書は主に、学部上級から大学院レベルの学生（MBAも含む）で、講義で学んだ統計的な手法を用いて研究を行い、論文を執筆したいと考える人たちを対象に執筆した。そのため本書では、マーケティング領域で頻繁に用いられる手法の紹介だけではなく、学術的な研究を進めるための考え方についても言及している。このような特徴を持つ本資料の目的は以下のようにまとめることができる： マーケティング領域における研究の全体像を理解する。 基本的な分析手法についての結果を適切に解釈できるようになる。 基礎的な分析手法を実行する能力を習得する。 なお、本書は基礎的な統計的・計量経済学的分析手法をマーケティング領域に応用することに焦点を合わせている。統計学や計量経済学の理論的議論や、より発展的な分析手法については、すでに優れた教科書がたくさん存在している。しかしながら、それらで学べる原理原則や手法をどのようにマーケティング領域に応用し、その結果をどのように解釈すればよいのかという点については十分に解説されていない。また、マーケティングリサーチのテキストとして、実践的な分析法やソフトウェアの操作方法を紹介してくれている教科書もたくさん存在する。しかしこれらの多くが実務的かつ実践的な内容が多く、学術的な研究に取り組むための知識が十分に言語化されていない。そこで本書では、マーケティングリサーチ手法を理解するために必要な理屈（理論や原理原則の概要）にも焦点を合わせる。本書は、（1）定量的な分析手法のマーケティングリサーチ文脈への応用、（2）理屈、（3）R (R studio) を用いた分析の実行法を紹介することにより、読者が本書を通じてリサーチに従事するための基礎を固めることを期待する。 なお本書は、神戸大学経営学部で開講されている「経営データ分析（マーケティング）」および、一橋大学商学部、同大学院経営学研究科で開講されていた「マーケティングリサーチ」用に筆者が作成した講義ノートをもとに執筆されている。これらの講義を通じた経験が本書の内容や構成に活かされている。初めて定量的な分析を実践する学生がどのような点に戸惑い、それをどのように克服していくのか、というプロセスを多くの学生と共有することで、筆者自身も多くの学びを得た。忌憚ない質問や鋭いコメントをくれた学生たちに心から謝意を示したい。 本書は実務的なマーケティングリサーチのハウトゥー本ではなく、学術的な研究を実施することを踏まえた構成になっている。筆者は、この構成が研究者として定量的な実証分析を実行し論文を書くことに加え、実務的な観点からも重要になると期待している。上記の目標を達成することで、リサーチプロセスをブラックボックスとして捉え、「調査会社がこう言ってたからこれが正しい」という態度で調査・分析を依頼、理解することを避けられると考えている。これを読んでいる読者の中には、現在または将来管理者としてマーケティングリサーチプロジェクトに関わる人もいるだろう。その場合、高度な統計的分析を自身で実施することはよりも、リサーチの全体像を理解し、各プロセスについての原理原則を理解することの方が重要になるかもしれない。 また、リサーチそのものは他社にアウトソースすることもあるだろう。その場合であっても、調査会社が提出した結果をきちんと理解、検討する必要がある。したがって、統計的な分析手法の背後にある理論や原理原則についても理解が必要になるのである。加えて、もしあなたが小規模な組織に所属しているのであれば、分析作業も自分でできたほうが良いかもしれない。その場合実際にソフトウェア上でデータを扱って分析作業をこなす能力も必要になる。上記の目的は、これらの実務的重要性も有していると考える。 本書の各章は、（1）概要、（2）理屈、（3）実践の三要素から構成される様執筆している。このような構成を採用したのは、幅広い理解度の読者を想定しているためである。本格的な学術研究に臨むためには、着目する手法の背後にある理屈（理論や原理原則）を理解することが必要になる。しかしながら、初学者にとってはその内容が学習の障壁になるかもしれない。そのため、各章の冒頭に、その章で着目する手法やトピックについての概要（背景、目的、重要な点）を整理し、抑えてほしいポイントを事前に整理している。 理屈では、手法において特に重要な点については「なぜそうなるのか」も含めて示すよう執筆している。そのため、論文内で自身が採用した手法や結果に関する説明や根拠を示すために必要な情報がこの要素に含まれている。しかし、本書の理論に関する記載は、あくまで各手法を理解するための必要な最低限の説明であるため、より詳細かつ発展的な説明については、本書内で示している参考文献を参照してほしい。 そのうえで以降の章では以下のように構成される。まず2章 では、Rおよび R studioの概要及び基本的な操作方法について説明する。本資料ではフリーソフトウェアである R (R studio環境)を用いるため、実際の分析作業に移る前に各自操作に慣れてほしい。続いて前半部（3章と4章）では、研究プロセスや論文の構成について学ぶ。特に3章は本資料の特色を表しており、分析方法やRの操作方法だけでなく「なんのために分析を実行するのか？」という問いに着目する。前半部では、リサーチ過程の全体像と論文の構成を始点とし、研究課題の設定や理論・仮説の役割に加えて、アンケート調査票設計について学ぶ。一方後半部分（5章以降）ではR studioを用いて、データ処理（5章）や統計的仮説検定（6章）、回帰分析（7章）について学ぶ。その後、マーケティングで広く用いられている探索的なデータ分析手法として、クラスター分析（11章）、因子分析（12章）、価格感度測定（13章）を学ぶ。 なお、本書では便宜的にマーケティングリサーチという言葉を、学術的な定量的なマーケティング研究に対しても用いるため、注意をしてほしい。↩︎ "],["rusage.html", "Chapter 2 R と R studioに慣れる", " Chapter 2 R と R studioに慣れる R（あーる）は統計、データ解析、統計グラフ作成のためのオープンソースソフトウェアである。Rは以下のサイト（http://cran.ism.ac.jp/）等からダウンロードし、インストールが可能である。ソフトウェアには、Windows用、Mac用、Linux用があり、ユーザー自身の環境に適したバージョンを選択してほしい。Rを用いる際には、多くの場合) R studio、Jupyter notebookや、Rコマンダーのようなユーザーインターフェイスが利用される。そしてRを使用する際には (Rコマンダーを使わないかぎりは) 基本的にソースコードを入力し計算、分析を行う。しかし、本講義においては後述する Cloud環境を利用するため、個人の意思でデスクトップ版をインストールする場合を除き、Rのインストールについては気にしなくて良い。 Rを用いる際に最もよく使われる環境（アプリケーション）のひとつがR studioである。そのため、本講義においても基本的にはR studioを用いることを前提とするが、R studioをデスクトップにインストールし利用する場合には、Rそのものもインストールしておく必要があることに注意が必要である。R studioは現在、Positとも呼ばれており、以下のサイトからアクセスが可能である（https://posit.co/）。 本講義では、Posit Cloudという、アカウント登録を行うことでブラウザ上でR studioを利用できる環境を勧める。R studio Desktop版 の利用においては、ディレクトリ設定などによってエラーが生じることが多々あり、個別のPC環境に合わせて対応、設定を行う必要がある。そのため、まとまった人数に対応する必要がある本講義においてはクラウド版を利用する。本講義を通じてRおよびR studioの使い方に慣れ、自身の研究や仕事等でデータ処理や分析を行う場合にはR studio デスクトップ版（通常はR studio IDEのフリーバージョンで十分）をインストールし、利用してほしい。もちろんはじめからデスクトップ版を利用してもらっても構わないが、その際には環境設定について色々と注意してほしい。本資料内では、第 2.2 節にて、デスクトップ版の利用についての説明をしているため、関心のある読者は参照してみて欲しい。 "],["posit-cloudを始める.html", "2.1 Posit cloudを始める", " 2.1 Posit cloudを始める R studioは、Rを利用するためのアプリケーションである。R単体で使うよりも便利な機能が搭載されており、R studioを使うことでプログラミング作業を容易にすることが可能になる。最も大きな特徴としては、Rでの操作、分析を実行するための「コンソール画面」と、実行したい操作、分析のコードを記述しておく「Rスクリプト」と呼ばれるテキストファイルを一つの画面内に同時に表示できることである。そのため、Rに実行してほしいコマンドをテキストデータのように記述、修正し書き溜めておける一方で、その実行もスムーズに行え、結果も同画面内で確認することができる。 R studioをより手軽に利用できるサービスがPosit Cloudである。Posit Cloudはブラウザを通じてR studio環境を利用できるサービスであり、アカウント登録をするだけでよく、コンピュータへのRおよびR studioのダウンロードとインストールが不要である。 Posit Cloudの利用方法はとても簡単である。大まかな利用までの流れは以下のとおりである。 以下のリンク（https://posit.co/）からサイトへアクセスし、ProductsタブからPosit Cloudを選択する。 Posit Cloud 画面 その後、進んだ画面で “Get Started” \\(\\rightarrow\\) （特別な理由がなければ）Free planを選択し “Sign up” \\(\\rightarrow\\) 好きな方法でアカウントを作成する。 Sign up 画面 登録が完了すると、自身のアカウントのホーム画面へ移動する。新しいR studio セッションを開始するためには、画面右上の New projectボタンを押し、“New Rstudio Project” を選択する。 New project画面 New projectのセットアップが完了すると、Studio環境画面が表示される。 新しいRstudio 画面 Rstudioは、上記の図のような画面構成をしている。Rstudioの画面を構成する主なウィンドウはペインと呼ばれ、(1) RスクリプトでRコードの入力・編集に用いる”Source”、(2) Rの命令を直接入力し結果も表示される”Console”がなどが主な要素としてある。また、その他利用しているデータ情報、パッケージ、履歴など様々なタブが存在する。Rstudioの初回起動時にはSourceのペインは収納されているため、 Rスクリプトファイルを作成する必要がある。Rstudioは基本的に4分割画面で表示され、各ペインの配置については、Tools \\(\\rightarrow\\) Global option \\(\\rightarrow\\) Pane Layoutより変更が可能になる。Rstudioを操作する上で、基本的に重要となる情報は、(1) Source、(2) Console、(3) データやプロットに関する環境情報の3点であるので、以下のような配置がおすすめである。 左上 or 下: Source 左下 or 上: History (ただし、さほど重要ではないので畳んだ状態にしておく) 右上 or 下: Console 右下 or 上: 複数タブをまとめ 配置の目的はあくまで、必要な情報を同一画面上に表示することであるため、自身のやりやすい配置を考えてアレンジしてほしい。なお、本講義ノート内に掲載している R studio 操作画面のキャプション画像では、Posit cloudではなくデスクトップ版の画面を用いている場合もあるが、ご容赦いただきたい。 "],["desktop.html", "2.2 （補足）R studio デスクトップ版の利用", " 2.2 （補足）R studio デスクトップ版の利用 Posit cloud のフリーアカウントには、利用可能な時間やデータ容量に制限が存在する。自身の利用スタイルを鑑みて、Posit cloudのフリープランでは不十分である場合には、有料版へのアップグレードやDesktop版のインストールによって対応する必要がある。ここでは、 R studio Desktop の利用について紹介する。 R studioをオフライン環境で使う場合には、R と R studioの両方をインストールする必要がある。Rは以下のサイト（http://cran.ism.ac.jp/）等からダウンロードし,インストールが可能である。ソフトウェアには、Windows用，Mac用，Linux用があり、ユーザー自身の環境に適したバージョンを選択してほしい。 R studio のインストールは、以下のリンクから “Download” ボタンをクリックすることで始まる（https://posit.co/downloads/）。なお、特別な事情がない限り、無料版で十分分析が可能である。 無料版のダウンロードが完了したら、指示に従いインストールを実施する。その際の設定はすべてデフォルトで構わない。 Rstudio Desktop インストール ただし、WindowsでのRおよびR studioのインストールには注意が必要である。特に、Rを用いる講義を受け持っていると、新たなパッケージのインストールができないなどのトラブルが頻発する。これらの問題点に調べると、(1) 文字コードによる文字化けの問題、(2) ユーザーアカウントのホームディレクトリ名に日本語（全角）が利用されていること、(3) Rのライブラリが(勝手に) One drive 上に作成されることが原因であることが多かった。これに対して、 R の version 4.20以降からは、UTF-8の文字コードに対応したり、デフォルトでのRのインストール場所の変更（One drive上でない）が行われたりと、問題の改善が図られている。自身のホームディレクトリの名前が全角文字であるときは、ホームディレクトリ以外のローカルディレクトリを設定したほうが良い。この点に関する対応には、三重大学の奥村先生によって以下のウェブサイトに説明が記載されている（https://okumuralab.org/~okumura/stat/R-win.html）。 "],["補足デスクトップ版の利用とプロジェクト機能.html", "2.3 （補足）デスクトップ版の利用とプロジェクト機能", " 2.3 （補足）デスクトップ版の利用とプロジェクト機能 R および R studio のインストールが完了したら、アプリケーションを起動する。R studio の利用方法については基本的に Posit cloudの説明と同様である。ただし、デスクトップ版で R studio を利用する際には、「プロジェクト」機能を使うことを勧める。プロジェクトは、互いに関連し合ったファイルの集まりを指す。Rを通じた分析では、たくさんのファイルを扱うことになる。例えば、複数のRスクリプトやデータセット、加工したデータセットの保存、分析結果、出力された図表などがある。これらのファイルを手作業で一括管理することは困難である。むしろそのような管理作業に認知的な負担を費やしたくないというのが分析者の本音である。プロジェクト機能を使うことにより、作業ディレクトリとファイルの保存先をひとまとまりに指定できるため、ファイル管理の手間がなくなる。 新しいプロジェクトを作成するシンプルな方法が、Fileから作成する方法である。具体的には、File -&gt; New Project -&gt; New Directory -&gt; Create New Project -&gt;Directory nameの指定 -&gt; プロジェクトの設置場所（ディレクト）の指定、という手順で作成する。 プロジェクト作成手順1 プロジェクト作成手順2 R をデスクトップ上で利用する際には、基本的には自身のPC内にある（もしくはディレクトリにアクセス可能である）データの所在地（ディレクトリ）を特定することでデータの操作や分析を行う。これに対してプロジェクト機能を利用することでそのプロジェクトを実行している際に参照するワーキングディレクトリを固定することが可能になる。この機能によってR studioを通じたデータ処理や分析作業が容易になり、不要なトラブルを避けることが可能になるため、デスクトップでR studioを使う場合には可能な限りプロジェクト機能を利用してほしい。 "],["rの基本操作.html", "2.4 Rの基本操作", " 2.4 Rの基本操作 ここでは、Rを使用する上での基本的な操作方法を紹介する。Rはコマンド（命令）をconsoleを通じて実行することで動かすことができる。例えば四則演算であれば、以下のように命令し、計算が実行できる。 1 + 2 ## [1] 3 5 - 10 ## [1] -5 3 * 8 ## [1] 24 1/2 ## [1] 0.5 基本的に一つのコマンドは1行に書き、数字、演算記号、スペースは半角で入力する。以下は、べき乗、平方根、自然対数を計算するためのコマンドで計算できる。 2^3 ## [1] 8 sqrt(2) ## [1] 1.414214 log(2) ## [1] 0.6931472 Rは、ベクトルや行列の計算も可能である。c() という関数を用いると、ベクトルを作成できる。例えば、c(1, 3, 5) というコマンドによって(1, 3, 5)というベクトルが作成できる。作成したベクトルを使って以下のような計算も可能である。 c(1, 3, 5) + 1 ## [1] 2 4 6 ベクトルは、連続した数字の列を生成するための演算子である : を用いても作成することができる。例えば、1から100の整数を要素とするベクトルは以下のように作成することが可能である。 1:100 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 また、ベクトルの要素は文字列でも構わない。 cities &lt;- c(&quot;Tokyo&quot;,&quot;Osaka&quot;,&quot;Kobe&quot;) cities ## [1] &quot;Tokyo&quot; &quot;Osaka&quot; &quot;Kobe&quot; 上記の計算方法に加え、Rが持つ重要な特徴に、オブジェクトの定義がある。Rでは、任意の行列、ベクトル、数値などに名前をつけ定義したうえで、それを用いた計算を行うことができる。なお、Console上で以下のように定義（実行）したオブジェクトはenvironmentタブ内に表示されるため、各自確認をしてほしい。なお、定義したオブジェクトの確認・出力も簡単に行えるが、大文字と小文字は区別されるため、注意が必要である。 a &lt;- 1 b &lt;- 2 a ## [1] 1 A ## Error in eval(expr, envir, enclos): object &#39;A&#39; not found また、定義したオブジェクトを用いた計算も実行できるため、各自以下の計算を実行し、結果を確認してほしい。 a + b a / b a ^ b なお、先程のベクトル操作と組み合わせ、ベクトル名 [i] とすることで、ベクトルの i 番目の要素にアクセスすることができる。例えば、以下のaとbというベクトルから特定の要素を取り出すことを考える。なおこの場合、同時に複数の要素を取り出すこともできる。 a &lt;- seq(10, 100, length = 10) b &lt;- 10:1 aの2番目の要素 a[2] ## [1] 20 bの2番目の要素 b[2] ## [1] 9 aの3-5番目の要素 a[3:5] ## [1] 30 40 50 aの1,3,5番目の要素 a[c(1,3,5)] ## [1] 10 30 50 分析で繰り返し必要になる機能がRで使えないときは、function()関数を使って、新たな関数を作成できる。例えば、最大値と最小値を並べて表示したい場合を考える。そのために、ここでは ‘mm’ という新たな(オブジェクトxの最小値と最大値で構成されるベクトルを返す)関数を作ってみる。 mm &lt;- function(x){ c(min(x), max(x)) } そして上記の関数を利用して、以下のオブジェクト a, b の最小値と最大値を出力する。 a &lt;- c(1, 5, 100, 2, -8, 7) b &lt;- c(1, 6, 8, 0, 120) mm(a) ## [1] -8 100 mm(b) ## [1] 0 120 しかし、すべての関数を自作するのは難しい。Rでは様々な計算を実行するための関数が用意されており、多くのマーケティング研究においては既存の関数を用いることで対応が可能である。実は上記のmm関数の中で使っている “min”や”max”も、それぞれ最小値と最大値を返す関数である。他にも例えば、meanや median があり、これらはそれぞれ平均値と中央値を計算するための関数である。 関数の利用においては例えば、’f()’のように関数名’f’のあとにカッコをつけて表記する。()の中には、引数（arguments）を用い、計算に必要な情報を指定することが必要となる。例えば、seq()という関数を用いて、2以上20以下の偶数の数列(sequence)を作ることが可能である。以下では、二通りの表記を提示するが、どちらも同じ結果を返す。 seq(from = 2, to = 20, by = 2) ## [1] 2 4 6 8 10 12 14 16 18 20 seq(2, 20, 2) ## [1] 2 4 6 8 10 12 14 16 18 20 特定の関数に対する引数を確認したい場合は ‘?関数名’とconsoleに命令することで確認が可能になる。例えば、seq関数について知りたければ、’?seq’ で確認できる。 Rに元から含まれている関数以外にも他者が開発してくれた関数も存在する。そしていくつかの関数をまとめたpackagesが多数存在する。これまで世界中の開発者たちが作成したパッケージを公開してくれている。パッケージは何らかの目的や課題を達成することを目的に構成されたコードライブラリであり、それらをインストールし、各セッションごとに起動することで利用できる。 CRANで公開されているパッケージは、install.packages()でインストール可能である。また、Rstudio の場合、ペインからpackagesタブ\\(\\rightarrow\\)Install\\(\\rightarrow\\)パッケージ名の入力という手順でもインストールが可能である。そしてインストール済パッケージは、library() によって起動することで、活用可能にある。ここで注意しておきたいのは、library()によるパッケージの起動はセッションごとに実行しないといけないという点である。まずは以下の通り、本講義で用いる “tidyverse” パッケージをインストールし起動してみる。“tidyverse”は、複数のパッケージから構成されているパッケージであり、データの整形・分析を行うために役立つ複数のパッケージをまとめてインストール・起動できる。 install.packages(&quot;tidyverse&quot;) library(&quot;tidyverse&quot;) "],["r-スクリプトのすゝめ.html", "2.5 R スクリプトのすゝめ", " 2.5 R スクリプトのすゝめ Rstudio環境で作業を行う際には、Consoleに直接コマンドを入力するのではなく、‘.R’ という拡張子のファイルを使って、「Rスクリプト」を作成することを勧める。RスクリプトはRでの分析に対応しRコマンドの集まりとして記されるファイルであり、SourceエディタからRスクリプトに記載されたコマンドを、Consoleを通じて実行する。 Rスクリプトを用いることの重要性は、コマンドの修正可能性と、分析の再現性という二点から理解できる。まず修正可能性として、そもそもコマンドを書くうえでは大小様々な誤りが付き物である。このような間違いに対応し適宜修正を加えていくためには、実行するコマンドを一つ一つ console に直接記載するのではなく、Rスクリプトとして分析過程を記録し、その内容に基づき記載、修正を加えることが好ましい。 第二に再現性においては、Rスクリプトとしてデータ整形・分析のプロセスを文書ファイルとして保存しておくことで研究者自身もしくは第三者が分析を再現することが可能になる。これは研究プロセスの客観性を高め、分析結果の信頼性を高めるために非常に重要な要素である。 これらに加え、Rスクリプトの利用は研究者個人の研究遂行上の利点もある。自身が行った研究であっても分析の細部に関しては時間経過とともに忘れてしまうものである。その際に、Rスクリプトによる分析プロセスの追跡可能性が役に立つ。また、同様の分析を再度別データで実施する場合も、既存のRスクリプトを応用することで効率的に分析が可能になる。これらに加え、共同研究において他の研究者と分析プロセスを共有する場合にもRスクリプトが役に立つ。 2.5.1 Rスクリプトを書く 先述のRスクリプトの利点を活かすために、Rスクリプトの作成においては、いつ作成されたなんのためのファイルなのか、そしてファイル内に記載されているコマンドがどのような意図によるものなのかがわかるように書くべきである。そのための重要になるのがコメント機能である。一つの行の中で#記号よりも後ろの部分はコメントとして処理（コメントアウト）される。この機能を使い、コメントで自然言語による説明を加えることで、コマンドの説明や意図等、自分や他人がスクリプトを見返して内容を理解できるようにする。例えば、Xという変数の平均値を求める場合、以下のようにRスクリプトを書くようにする。 #Xの平均値を求める。 mean(X) Rスクリプトは、Rstudioの左上にある+ボタンから新規作成可能である。ここでは試しに、新規Rスクリプトを作成し、“mktg01.R” という名前で保存してほしい。保存したRスクリプトはファイルから開くことができる。“mktg01.R”ファイルを作成したら、試しに以下のコマンドを書き込み、実行してほしい。Rスクリプトからコマンドを実行する際には、コマンド記入後、Rスクリプト上で実行したい行にカーソルを合わせた状態でcommand (control) + Return (Enter)を入力する。もう一度同じキーを押すと、2行目のコマンドが実行される。これらを実行することで、Rコンソール上に、下記と同じ結果が出ていることを確認してほしい。なお、コマンドを記入の際には、こまめに command (control) + s により保存することを心がけるようにしてほしい。 a &lt;- 9 sqrt(a) ## [1] 3 また、Rスクリプトを作成する際には、ファイルの冒頭に以下の説明を書き込む習慣をつけると後々見返すときに便利である。 ファイル名 目的 作成者 作成日 最新更新日 例えば、上記の内容とコマンドを含めた “mktg01.R” ファイルは、以下のようになる。 Rスクリプト例 "],["本章のまとめ.html", "2.6 本章のまとめ", " 2.6 本章のまとめ Rはデータの管理、分析、図表の作成を行うことができる統計分析プログラミング言語である。 Rを動かすには、コマンドと呼ばれる命令をコンソールを通じて実行する。 コマンドは基本的にRスクリプトに書き込んでからcommand (control) + Return (Enter)で実行する。 Rスクリプトにはコマンドだけでなくコメントを使った説明も追加する。 分析には既存の関数やパッケージを使うことが多い。 "],["参考文献.html", "2.7 参考文献", " 2.7 参考文献 浅野雅彦・矢内勇生 (2018) 「Rによる計量政治学」, オーム社. ランダージャレド（2015）「みんなのR 第2版」，高柳新市・牧山幸史・蓑田高志訳，マイナビ. "],["design.html", "Chapter 3 リサーチデザインと問いの設定 ", " Chapter 3 リサーチデザインと問いの設定 "],["本章の概要.html", "3.1 本章の概要", " 3.1 本章の概要 本書は定量的マーケティングリサーチ過程の全体像を理解し、それを監督・実行できるようになることを目的とする。そのためには、基本的な分析方法およびその結果を適切に解釈できるようになることに加え、リサーチプロジェクトの終着点を理解し、そこを目指した研究方法を定める必要がある。 研究者が研究を実行したいと考えても、ただ思いつきのままにデータを取り、それを羅列しても適切な定量的研究にはならない。リサーチの実行においてはリサーチの主要作業についての枠組みを計画する必要がある。田村（2006）によれば、（1）研究課題の設定、（2）現象を捉えるための理論の決定、（3）調査・データ収集方法や、（4）分析手法の決定、といった研究の主要作業を構想することをリサーチデザインと呼ぶ。本章は、リサーチデザインを中心に、リサーチそのものを概観することを目的とする。この目的を達成するために本章では、以下の点について議論する： マーケティング実務とリサーチの関係 異なる研究アプローチのタイプと代表的アプローチの全体像 研究課題や仮説の導出 学術的研究に求められる科学哲学や理論に関する基礎的な考え方 第一に、マーケティングリサーチにおける実務との関係や違いを整理する。マーケティングに関する研究では、それが企業内での研究活動と学術的な研究活動のどちらにおいても実務との関係は切り離せない。そのため、マーケティングリサーチにおいて実務的な関心や課題が重要ではあるものの、実務とリサーチとで異なる視点や問いを持つことを理解することが必要になる。特に、実務的視点や問いでは「どうするべきか」のような行動に着目した議論が中心に置かれることが多い。一方でリサーチにおける視点では「〇〇は××を向上させるか」のような、情報の取得に着目している。そのうえでリサーチでは、得た情報に基づき実務的含意として意思決定者への提案をすることが求められる。 第二に、マーケティングにおいて近年議論されている二つの異なる研究アプローチを紹介する。近年のマーケティング研究においては、理論重視（Theory-First: TF）アプローチと対比する形で、観測重視（Empirics-First: EF）2 アプローチに関する重要性が論じられている（Golder et al., 2023）。これらの異なる研究アプローチでは、それぞれに適した目的も研究プロセスも異なる。そのうえで本章では、大多数のマーケティング研究で採用されているTFアプローチ研究において典型的な研究プロセスを理解するために、論文の構成について説明する。理論重視アプローチにおける典型的な研究論文では、（1）イントロダクション、（2）現実的（substantive）トピックの文献レビュー、（3）理論的（theoretical）基盤の文献レビュー、（4）仮説の提示、（5）調査分析手法の説明、（6）分析結果、（7）まとめと議論、という構成として整理できる。研究の実行においては自身の立てた問いに対して一貫した回答を得る必要があるため、上記の（1）から（5）までの流れを事前に想定したうえで研究を行う必要がある。そのため、実際に研究を行う前に、研究の全体像について理解することはとても重要になる。 第三に、研究での問いと仮説について、その設定方法も踏まえて説明する。マーケティング研究においては実務的課題（実務的視点に基づく問題点）が重要であるものの、実務的課題のまま研究を開始するのは難しい。そのため、自身が実務的課題を持っている場合には、それを研究課題に変換する必要がある。実務的課題から研究課題に変換する際の考え方として本書では（浅野・矢内, 2018）を参考に「参照枠組みの変更」と「暗黙の前提を問う」という二つを紹介する。また、田村（2006）では研究課題の価値は個人的な関心だけで判断されるのではなく、学術的重要性と実務・社会的重要性の両面から評価されると強調されている。そのため、本格的な研究を行うためには学術的な知識や実務・社会的な知識も必要になる。 また研究では、研究課題に答えることができる仮説（理論的に予想される問いへの回答）を提示する事が必要である。本書が着目する研究アプローチでは、仮説を「理論を検証するために引き出された特定の変数間の関係に関する記述」として捉え、検証可能な記述を提示することが必要だと強調する。より具体的には、仮説は理論と整合的かつ測定（検証）可能な変数間の関係として論じる必要がある。 第四に、学術的原則の背後にある考え方として、科学哲学と理論的貢献に関する考え方を紹介する。本書では、研究課題、仮説、手法の一貫性の重要性を強調している。このような考え方の背後にある演繹的な思考法の源泉になっている科学哲学について紹介する。また、研究課題の価値を判断するうえで重視される学術的貢献について、実践的な考え方である Theory elaboration （理論の精緻化）を紹介する。これらはより学術的な側面が強い内容であるが、研究活動そのものについてより深い理解を促すものとしての役割を担っている。特に、「科学的であるとはどういうことか」や、「定量的な手法によって得た結果は科学的にどのような意味を持つのか」という点について考えるうえで非常に重要な内容である。 マーケティング・リサーチを実行する際には、企業活動における顧客や市場のリサーチと、学術研究としてのリサーチが存在する。しかし、どちらにおいてもリサーチデザインの決定は必要なプロセスであり、企業活動におけるリサーチにおいても学術研究で培われるスキルや考え方は応用できる。本章の内容は研究者志望の大学院生にとって重要であることはもちろんだが、企業においてマーケティングリサーチに従事する人たちにとっても重要であると期待する。特に、本章では「マネジメントとリサーチの異なる視点」、「実務的課題と研究課題」といった実務的意思決定とリサーチ業務の対比によるマーケティングにおけるリサーチの考え方についての議論も提供している。筆者は、これらの内容は企業内でリサーチ業務を実施する際にも有用だと考える。 Empiricsは通常「経験」と訳されるが、「経験重視」という言葉を用いることで生じうる誤解を避けるため、このような訳を採用している。↩︎ "],["マーケティング実務とリサーチ.html", "3.2 マーケティング実務とリサーチ", " 3.2 マーケティング実務とリサーチ 3.2.1 実務とリサーチの異なる視点 経営学領域に関心のあるしている読者であれば、「マーケティング」に関する様々な講義や教科書に触れてきたかもしれない。しかしながら、その際にそれらの講義や教科書がどのような視点でマーケティングを捉え、議論しているのかに注意することが重要になる。マーケティングを議論する際には、（1）マーケティング意思決定者の視点と、（2）マーケティングリサーチャーの視点が存在する。マーケティング意思決定者の視点からは、マーケティングに関する意思決定を行い、実行することを目的に議論が行われる。「マーケティング・マネジメント」はこの視点の典型的な領域である。ここでは、意思決定のサポートするための既存のフレームワークを紹介することが多い。例えば、マーケティングで紹介される4Ps（Product, Price, Promotion, Place）や、STP（Segmentation, Targeting, Positioning）などは、意思決定や実行を手助けするための指針となることを目的としている。そのために、提示される議論や枠組みの精緻さよりも実務的有用性が優先されることも多い。 第二に、マーケティングリサーチャーの視点では、マーケティングや消費者、顧客に関連する信頼度の高い情報や知識を得ることを目的に議論が行われる。「マーケティングリサーチ」や「マーケティングサイエンス」はこの視点の典型的な領域である。マーケティングに関する研究では、既存の知識を疑ったり、不足している知識を発見し補う事を求める。そのうえで、学術的研究の場合には発見物による一般化可能性を、企業での研究では自社顧客への深い理解を求めることが多い。そのため、この視点では議論されている内容の精緻さや文書内の論理的一貫性が優先される。 企業内におけるマーケティング実務においては当然マーケティング意思決定者の視点が優先されるわけだが、マーケティングリサーチもこれと無関係ではない。企業内におけるマーケティングリサーチは、実務的な意思決定を助けるために実施される（Malhotra, 2019）。マーケティング意思決定者は通常、マーケティングに関する実務的課題（3.4節参照）を抱えておりそれを解決したいと考えている。しかしながら、意思決定者は闇雲にマーケティング戦略や戦術を決定すれば良いわけではなく、市場や消費者、既存顧客などに関する情報をもとにより効果的な方策を模索している。企業としてのマーケティングリサーチでは、（1）実務的課題から必要な情報（研究課題）の特定化、（2）情報（調査結果）の提供、を通じてマーケティング意思決定者をサポートすることが期待される。 マーケティング実務においては、「どうするべきか」という行動に即した問いや規範的な議論が注視されるが、「どうするべきか」という規範的な問いに直接的に答えるようにリサーチを設計することは避けたほうが良い。そのためにも、「リサーチの結果に基づく含意として実務的な指針が導かれる」ということを意識すると実務的にも関連性の高いリサーチを設計しやすい。 例えば、「どのような広告内容を採用するのが良いか」という問いに対して直接的に答えるリサーチを計画するのは好ましくない。この問いが問題である理由はいくつか存在する。第一に、これは分析手法においても説明するが、リサーチにより正確に未来を予測することは不可能である。そのため、「こうすべき」という規範的な回答は基本的に避けたほうが良い。規範的な問いに対しては、研究の結果得た知識をもとに議論を行うことで得る「実務的含意」に基づき迫ることが好ましい。なお、企業内での研究ではもちろんだが、学術的な研究においても結果から得る実務的含意はとても重要である。マーケティングに関する研究は社会科学の応用領域である。そのため、我々が行っている研究およびその結果がどのように社会や実務につながるのかについて、研究者は意識的になる必要がある。 第二に、問題設定が不明確である点についても問題を指摘することができる。何を良しとするかの基準が明確ではなく、消費者が商品に対して覚えてくれる（想起集合に入る）ことや、好意を抱く（態度形成）、買いたいと思う（購買意図の形成）に加え、実際に買う（選択）など、何を目的とするのかが明確でなく、何をもって「良い広告」とするのかが漠然としている。同様に、広告設計において企業が調整できる内容も様々ある。広告内で用いるメッセージや、有名人利用の有無から、広告内で用いる文字のフォントやサイズまで、色々なものを選択しなければならない。このように、「何と何の関係を捉えようとしているのか」が曖昧であることも上記の問いの問題点である。学生の中には、「網羅的かつ包括的にこの問題を捉えたい」という志のもと、上記のような曖昧な問いを採用するケースも散見される。しかしながら基本的には、曖昧な問いからなにか明確な回答を得ることは困難である。つまり、包括性の名のもとにこのような問いを設計してしまうと、結局は実務的にも何も言えない結果しか得ることができないことに注意が必要である。 3.2.2 マーケティングリサーチと実務的含意 本章のこれまでの内容をまとめると、「実務的課題と研究課題は別物だが、研究によって得る実務的含意は重要である。」と言える。企業内でのマーケティングリサーチは意思決定者をサポートするために実行される（Malhotra, 2019）。そのため、リサーチ結果が意思決定者に指示を与えるわけではないということに注意が必要である。以下の図（a）のように、実務家が直面している課題に対していきなり解決策を求めようとすると、個人や組織の勘や経験、思いつきに頼ることになる。このような意思決定を避けようとするのが、基本的なマーケティング・マネジメントの考え方である。そこで、直面している問題の背後に存在する理由について推察し、どのような情報を得る（問いに答える）ことができればその理由が正しいか否かを理解できるのかを考えることがまず必要になる。つまり、問題の背後にある何らかのメカニズムについて仮説を構築し、その仮説自体を検証するための問いを考えることが重要になる。その後、どうやったらその問いに答えるための方法を考察・実行し、得た結果を知見として整理する。ここでもう一つ重要になるのが、研究結果として得る情報自体は必ずしも直接的な解決策にはならないという点である。研究上の問いは、あくまで問題の背後にあるなんらかのメカニズムを捉えていた。そのため、研究の結果得られるのは、メカニズム対する何らかの知識である。研究者は得た結果に基づき、「このようなメカニズムがなりたっているという前提で考えるならば、実務としてはこのような方法が好ましいのではないか」という提案を考察することが好ましい。下記図（b）は、実務的含意と問題設定の関係について示したものである。 実務課題と実務的含意 3.2.3 非計画購買に関する研究紹介（Hui et al. 2013） 上記の研究による発見と実務的含意との関係について、Hui et al. (2013) による消費者の非計画購買に関する研究を例に取り考えてみる。消費者の購買行動は、買い物のために店舗を訪れる前に買う製品を計画する計画購買と、それ以外の製品を買う非計画購買に分けることができる。つまり、事前に計画し購入するような購買行動以外は全て非計画購買に分類されることになる3。 非計画購買は小売企業にとって追加的な収益となるため、どのようにすれば消費者の非計画購買を促すことができるのかはとても重要な問題であった。そのため、1960年代以前から研究が行われてきましたが、伝統的に非計画購買の確率を向上させるために重要なのは、店舗内での移動距離や滞在時間であると議論されてきた（Granbois, 1968）。しかしながら、店舗内移動距離が長い「から」非計画購買が起こりやすくのだろうか。学術的には、衝動的購買傾向 (Compulsive Buying Tendency)という個人の特徴としての心理尺度も存在する。つまり、「店舗内を長く歩くから非計画購買をする」のか、「非計画購買をするような人が長く店舗を歩く」のか両方向の流れが成立する可能性がある。もし後者の方が正しい場合、小売店舗が講じる、店内移動距離を伸ばすような施策は不要だということになる。そのため、店舗内を長く歩くから非計画購買をするのかについて検証することは、マーケティング実務の面から見ても重要な問いであったものの、2010年代においてこの問いに明示的に答える研究は出版されていなかった。 これに対しHui et al. (2013) はアメリカの北西部の都市にある中規模食料品店において300人の買い物客に対するフィールド実験（現実の状況における実験調査）を行った。この調査手順で著者らはまず、調査対象者に店舗エントランスにてアンケート調査を行った。このアンケートでは、買い物リストの有無、予測される支出額、一人での買い物か否か、店舗内の製品配置をどれだけ把握しているかなどを確認した後、店内の99の製品カテゴリに対し購買する予定がある物にすべてチェックをつけてもらった。このようなアンケート調査の後、被験者はRFIDのパストラッカーベルトをつけることで、各被験者の店舗内移動距離を計算した。そして各買い物客の実際の取引履歴については店舗から情報を収集した。 この研究では、先述の両方向性の問題に対応するために、Random Product Placement Model（RPPM）4に依拠する形で、実際の移動距離（Path距）と参照移動距離（TSP距）を計算し、操作変数法という手法を用いている。つまり、Hui et al. (2013) はランダム化フィールド実験を行うことで入手できた理論上の移動距離変数（TSP距など）を操作変数として用いて、店舗内移動距離が非計画購買に与える影響について分析した。分析の結果、実際の店内移動距離の増加によって非計画購買額が高まることがわかった。具体的には、各消費者が10％（平均ではおよそ140フィート; 約43メートル）長く移動すると、約15.7%非計画購買額が高まる。 この結果は、「店舗内移動距離が非計画購買にどのような影響を与えるか」という情報を記述的に示しているのみであり、結果自体が実務的な提案そのものではないことを理解してほしい。しかしながら、この情報から実務的に有効そうな施策を推測することは可能である。例えば、小売店による店内の陳列またはレイアウトの見直し、注目を集めやすい値下げ商品を店舗奥に陳列するなどの工夫や、モバイルアプリの利用などの店内移動距離を伸ばすような取り組みが挙げられる。これらのような、研究結果から推察・議論できる何らかの提案を「実務的含意」と呼ぶ。 ただし、ここまでの議論では、実務的含意として述べられている方策の有効性自体は検証していないことには注意が必要である。例えば、「小売企業は顧客の店内移動距離を伸ばすためのレイアウト変更を実行すべきか」という実務的課題に着目するならば、「レイアウトの変更によってどの程度非計画購買が増加するのか」という研究課題を扱うことが考えられる。なおHui et al. (2013) は、上記の問いに加え、具体的な方策による効果についてもシミュレーションや追加調査で明らかにしている点でも優れた研究であるといえる。具体的には、店内移動距離を伸ばすような製品陳列場所の再配置によって約7.2%の非計画購買の上昇が試算された。また、この論文ではモバイルクーポンを、店内移動距離を伸ばすためのきっかけとして用いる方法を提案している。 Hui et al. (2013) では2つ目のフィールド実験として、ピッツバーグにある小売店にて調査を実施し、本当に移動距離を伸ばすようなモバイルクーポンが有効なのかを確認した。ただし、研究当時はまだモバイルアプリがそこまで発達しておらず、実際のモバイルアプリを用いた広告効果の測定はできなかった。そこで、実験参加者の買い物リストを入力すると自動的に広告対象となる製品カテゴリを決定する仕組みをノートPC上で作成し、擬似的なモバイル広告機能を満たす実験環境を整えた。この実験では、被験者から買い物リスト聞き、そのリストに基づきクーポンを提示した。この時、被験者は（1）リストにある製品に対して店舗内移動距離を最大化する製品カテゴリをオファーするクーポン（リストを入力すると1SLA距を最大化する製品を割り出すアルゴリズムを作成している）を受け取るグループと（2）リストにある製品に近い製品をオファーするクーポンを受け取るグループに分けられた。これらの各種類の中でそれぞれさらにクーポンの値引き額についても2種類の条件（$1 vs. $2）に分けている。 両グループのクーポン利用率に大きな差はなかったものの、分析の結果、遠いクーポン条件の場合の平均非計画購買額が$21.29であるのに対して、近いクーポンでは$13.83であることがわかった5つまり、筆者らの予測通り、顧客の移動距離を伸ばすようなクーポンオファーの方が非計画購買の増加に対して効果的であるということが示された。 研究は基本的に、ある問いを立ててそれに答えるという構造を有しており、ある研究を達成したことによって生じた新たな問いはまた別の研究によって回答することが重要になる。なお、学術的な論文であってもマーケティング領域においてはこの実務的含意を議論することが求められるため、研究者志望の大学院生もこの点を十分に理解することが求められる。 3.2.4 実務とリサーチの分離 マーケティング研究を通じた実務的意思決定への知見の提供では、同一企業内で調査・分析を実施する場合もあれば、調査会社等にそれらをアウトソースする場合もある。いずれの場合においても、企業活動のためにマーケティング研究を実施する際には、「マネジメントとリサーチの分離」が重要になる。マーケティング実務に関する調査分析を行う場合、研究を発注する主体でありマネジメントを主な仕事とする「クライアント」と、それを受け研究に従事する「リサーチャー」が存在する。リサーチャーは、クライアントが直面している実務的問題に基づき研究課題を設計し、調査や分析を実行する。そしてその研究成果をレポートにまとめ、クライアントに報告する必要がある。 リサーチャーは、レポート執筆において「ありのままを書く」必要がある。具体的には研究方法、結果、結論について正確に記述し情報を提供しなければならない。データや結果の改ざんは重大な研究不正である。企業におけるリサーチであっても読み手であるクライアントが期待する、または望んでいる結果にレポートの内容を書き換えてはいけない。しかしこの問題は、個人の倫理観だけではなく、リサーチャーとクライアントを取り巻く環境にも関係して論じられるべき問題である。例えば、クライアントにとっては特定のマーケティング戦略（例、モバイル広告）を実行し成果をあげることに自身の出世や昇給がかかっており、モバイル広告がその企業にとって有益であるというエビデンスを「欲しがっている」かもしれない。その上で、リサーチャーによる調査と分析の結果、モバイル広告による成果の向上が確認されなかった場合を仮定する。このとき、クライアントとリサーチャーの間に利害関係がなければ、「ありのまま」を伝えるのは難しくないかもしれない。しかしながら、もしこのクライアントがリサーチャーの直属の上司であるならばどうだろうか。ひどい場合であれば、クライアントの不利になる結果を提示したとしてリサーチャーが悪い評価を受けるかもしれないし、客観的な評価に影響がなくても、個人的な反感を買うかもしれない。もしくは、リサーチャー自身がこのような可能性を危惧し、「ありのまま」を伝えることに躊躇してしまうこともあるだろう。大前提として、研究結果に基づき（クライアントとリサーチャーどちらも）従業員の評価を行うことは避けるべき問題であり、研究結果は研究結果として受け入れることが重要である。しかしながら、人間である以上このような正論では割り切れない部分も出てくるかもしれない。そのため、クライアントとは利害関係のないリサーチャーに研究を発注することが何より重要であり、クライアント側の組織においても研究結果に基づき人事的評価を行わないという前提やルール作りが必要になる。これらが満たされていないと、クライアントにとって都合の良い結果を求めることで適切な研究が設計されなかったり、不利な研究結果受け入れないといった行動を取る誘因がクライアント側に存在することになる。 上記の例は、企業内における利害関係を捉えたものであるが、学術的研究においても注意が必要である。自身の経験や知識に基づいて、「このマーケティング戦略は効果があるはずだ」や「この戦略の有用性を示したい」と考えている場合、意識的もしくは無意識的に自身の都合の良い方法や結果を選別してしまうかもしれない。また、研究者が企業との共同研究に従事している場合には、先述のクライアントとリサーチャーの関係と同様の問題に直面するかもしれない。そのため、あなたが企業、学術どちらの研究に従事するとしても、マネジメントとリサーチとを分離し、研究結果をありのままに記述、報告できる状況を整えることが何より重要である。 非計画購買は主に4つのタイプに分類することができる。非計画購買の第1のタイプは想起購買と言われる、来店時には潜在化していた商品の必要性を来店後に認識し購買に至る行動である。例えば、店舗に着いてから「自宅のトイレットペーパーが切れていたから買わないと」と潜在的な製品の必要性を認識するタイプの購買が想起購買である。第2が、関連購買という、購入された製品との関連性から他の製品の必要性を認識し、購買に至る行動である。例えば、スーパーで刺し身を買ったことに関連し、わさびも買おうと考え購買に至る場合がこれにあたる。第3のタイプが条件購買と言われる、来店時には明確な購買意図は持ってないけれども、価格やその他の条件が整えば購買に至る行動である。条件購買は例えば、Point of Purchase (POP) 広告を見て、旬の食材が安くなっていることを知り、購買に至るような場合が挙げられ る。最後に、上記の３つの類型いずれにも属さないタイプの購買行動を衝動購買という。これは商品の新奇性に影響を受けた購買や真に衝動的な購買などを指す。↩︎ RPPMは、以下のような手順・前提を持つアプローチで対応している。第一に、各お客さんはいくつかの製品を購入することを計画して入店する。第二に、製品のレイアウトが各顧客の入店直前に迅速にランダムに変更されるならば、購入予定の製品の場所はそれぞれの客に対してランダムに割り当てられることになる。第三にそれぞれの客が店舗内移動距離を最小化するような経路を計画する（巡回セールスマン問題という）と仮定する。しかしながら、実際の移動距離（Path距）は3で求められる参照移動距離（TSP距）とは異なる（Path距＝TSP距+誤差）。↩︎ この結果は統計的にも有意だった。統計的仮説検定については 6 を参照してほしい。↩︎ "],["リサーチデザインとアプローチ類型.html", "3.3 リサーチデザインとアプローチ類型", " 3.3 リサーチデザインとアプローチ類型 3.3.1 異なる研究アプローチ（理論重視と観察重視アプローチ） 良い研究を遂行するためには、自身の立てた研究上の問い、議論される仮説、調査・分析手法や結論といった、研究を構成する要素間の一貫性を保った形でリサーチデザインを決定する必要がある。近年のマーケティング研究においては、理論重視（Theory-First: TF）アプローチと対比する形で、観測重視（Empirics-First: EF）アプローチに関する重要性が論じられている（Golder et al., 2023）。TFは理論の検証や、拡張、開発を目的として、演繹的6に実践されるアプローチである。 一方でEFは、「現実のマーケティング事象、問題や観測に基づき、データの取得と分析を含み、理論の検証や開発を必ずしも必要としない形で、マーケティングに関する妥当なインサイトを得ることを目的とした研究アプローチ」である（Golder et al., 2023, p.320）。そのため、EFに基づく研究は探索的であり、必ずしも直接的な理論構築への貢献を目指す必要がないという特徴を持つ。 EFは、既存の理論のもつ説明範囲を超える減少に対してマーケティングや社会科学一般として知見を届ける必要性と可能性が向上したことから注目が集まっている。例えば近年、COVID-19や気候変動など、マーケティング外部での重要な問題への対応が必要になったり、新たなデータへのアクセス可能性や分析手法の開発による研究機会の拡張がEFの重要性につながっている（Golder et al., 2023）。しかしながら、現代の大学、大学院教育および、学術的議論の主流はTFであり、Golder et al. (2023) においても、EFにコミットしたいと考える若手研究者は、それがどれだけコミュニティ内で受け入れられているかを慎重に見極めることを推奨している。そのため、本書は主にTFに基づく研究アプローチに重きを置き構成する。具体的には、本性においてTFとしての典型的なリサーチデザインを伝え、それに対応する仮説検証型の分析手法を7 〜 10章にかけて紹介する。一方でEFについては、11章にてその内容を紹介し、マーケティング領域で頻繁に用いられる探索的なデータ分析法を12 〜 14章にかけて紹介する。 3.3.2 理論重視研究と論文の構成 リサーチデザインを適切に決定するためには、研究の全体像を把握しておく必要がある。TFアプローチは基本的に科学的な発見物を伝えるためのストーリーを構成することが求められる（例えば、イントロダクション \\(\\rightarrow\\)理論・概念フレームワーク\\(\\rightarrow\\)仮説\\(\\rightarrow\\)方法\\(\\rightarrow\\)結果\\(\\rightarrow\\)ディスカッション）。そこで、本節では、研究の全体像を把握・理解するために学術論文の構成を整理しつつ、それぞれの構成要素の役割について把握する。なお、ここで紹介される論文の構成は修士・卒業論文の執筆においても応用できる。また、本節の後半では実務的なマーケティングリサーチレポートにおける注意点も追記する。 マーケティングに関する学術論文は（様々な形態があるが、一般的には）以下の構成として整理できる。なお、以下の内容はマーケティングに関する定量的な実証研究についての形式であるため、アプローチが異なれば構成は変わると考えられる。例えば、定性的なアプローチを用いたテキストとしてはベルクほか（2016）による著書が挙げられる。また、項目の順番も論文の特性によって前後することもある。 イントロダクション 現実的（substantive）トピックの文献レビュー 理論的（theoretical）基盤の文献レビュー 仮説の提示 調査分析手法 分析結果 まとめと議論 イントロダクションの役割は論文としての「問い」と、その問いにどのように回答するかを簡潔かつ明示的に説明することである。そのうえで、その問いについて答えることがなぜ重要なのか、どのような価値があるのかについて、論文の読み手に理解させる必要がある。そのためには、（1） 論文として着目する問題（および問題の背景）の明示、（2） 研究課題の提示、（3） どのような研究群にどんな貢献を与えるのかについての説明、（4） 論文の概要（実際にどんな結果を得たかなど）の説明、が必要になる。 第二に、現実的（substantive）なトピックの文献レビューについてだが、ここでは着目する現象や実務的課題に対応した先行研究のまとめを記述する。マーケティング研究における文献レビューは主に、（1） substantive なトピックベースで先行研究の潮流をまとめ、課題を見出すレビューと、（2） theoretical （理論的）な議論を整理したり、理論的課題を見出すレビューとに分けることができる。例えばあなたが、「ソーシャルメディアでの炎上における消費者行動を、Fluency theory （Schwarz, 2004; Schwarz et al., 2021） を用いて研究をする」という研究目的を持っていたと考える。このとき、「ソーシャルメディアでの炎上」に関する研究群を網羅的にレビューし、批判的検討を行うことは、substantive な文献レビューであると考えられる。多くの場合、substantiveな文献レビューによって、「そのトピックにおいて今までなにがわかっていて、何がわかっていないのか」を明確にし、研究課題を導出することが多い。一方で、Fluency theory とはどういうものであり、これまでどのような研究蓄積があるのかを整理し議論するような文献レビューは、theoretical な文献レビューであるといえる。 マーケティング領域の研究では、substantiveな文献レビューによって研究課題の明確化、提示している論文が多いものの、理論の精緻化の必要性から課題を明確化することももちろん可能である。つまり、文献レビューと言ってもその役割は大きく分けて二つあり（substantive vs. theoretical domain）、自身がどのような目的を持ってレビューを行っているのかについてその都度自覚的になる必要がある。その上でこのパートでは、「今までなにがわかっていて、何がわかっていないのか」を明示的に伝える必要がある。マーケティングに関する学術的な研究（特に国際的な査読ジャーナルの世界）では、あるトピックや理論的問題に関して複数の論文を通じて知見を積み重ねていく。そのため、自身の着目する研究潮流を定め、その潮流に関する網羅的かつ批判的なレビューを行うことで、リサーチギャップ（先行研究で不足している情報）を見つけることが必要になる。研究潮流の整理やリサーチギャップと問いの提示については、イントロダクションにおいても簡潔に説明することが求められる。その上でこのパートではイントロダクションでは説明しきれないより詳細な既存研究の整理を行う。そのためには、著者自身が着目している研究領域や潮流を明確化し、提示した研究課題およびそれへの回答が、どのような貢献をその研究領域に与えるのかについて説明することが求められる。 第三の理論的基盤の文献レビューは先述の theoretical な文献レビューに相当する。このパートにおいては、著者の研究課題を解明するために依拠する理論やメカニズムを提示することが求められる。ここでは主に、研究の文脈に依存しない抽象度の高い理論的枠組みを定義、整理し、議論する。そのため、理論的基盤の参照のためにはマーケティングや経営領域に限らない分野の研究を参照することになる。例えば、先述のFluency theoryとは、人々の情報処理や思考過程を捉えたメタ認知理論であり、主観的な情報処理の容易さ（流暢性）により、態度形成や意思決定に影響を与えることが知られている（Schwarz, 2004）。Fluency theoryはソーシャルメディアにおける炎上固有の理論ではなく、様々な異なる文脈において議論、分析されてきた（Schwarz et al. 2021）。理論的基盤の議論ではこのような抽象度の高い議論について、説明的な記述をすることで、どのような見方で研究対象を論じるのかを明確にすることが求められる。また、理論の定義や潮流の整理に加えて、着目している理論の精緻化や拡張可能性などを見出した場合には、それも議論し、精緻化されたモデルを提示することが好ましい。 第四の仮説の提示においては、先述の理論的枠組みに基づき具体的な変数7に対応する仮説を提示する。なお、「理論」と「仮説」は3.5節で説明するように、根本的には同じものだと考えられる。しかしここでは理解のしやすさを優先し、便宜上研究文脈に依存しない抽象的な議論を理論、それよりも具体的で文脈依存的な予想を仮説と仮定する。仮説は通常、「H1: 〇〇が高まると、〜〜が高まる。」というような説明として提示される事が多い。なお、この仮説で用いられる〇〇などは、分析で扱う具体的な変数と一致していることが求められる。 これに加えて、このパートでは仮説のみではなく、「なぜ、もしくはどのようにして仮説のような予想が可能なのか」という、仮説導出に関わる論拠や理屈を説明することが求められる。論文内で複数の仮説を提示する場合には、各仮説ごとにその論拠も含めて明示的に提示するこが求められる。また、例外や対抗仮説（別の有力な理論）がある場合にはそれも提示し、どのような結果になれば、対抗仮説ではなく著者が依拠する理論仮説が支持されることになるかについても論じるとよい。 第五の調査分析手法の説明において研究者は、自身の立てた仮説を検証するために使う変数をどのように測定、観察（データとして収集）するのかについて、具体的に説明する必要がある。例えば、データ収集方法として、仮説検証のための証拠となるデータを、誰が、どこで、どのような方法で集めた、どんなデータなのかを説明することが求められる。マーケティングにおけるデータ収集では、二次データ（企業のIR情報、政府統計など）、質問紙調査（郵送、オンライン）などを活用することが多い。データの種類についての詳細は「データの種類とアンケートデザイン」節で説明する。これに加えてこのパートでは、どのような分析手法を用いるのかについても説明する必要がある。統計的な分析を実施する場合には、その分析モデルや手法についても明示することが求められる。 第六の分析結果パートでは、分析の結果をありのまま提示し、それが論文内で主張している仮説と整合的な（仮説を支持する）結果なのかを評価し、説明する。統計的な分析を行う場合には、統計学的な原則と自身が提示する結果の解釈が整合的であるかどうかに注意が必要となる。また、ここでは必要な情報を網羅しながらも、表などを用いながら簡潔に読みやすく構成することが求められる。 最後に議論パートでは、この研究がどのような理論的貢献と、実務的含意を有しているのかについて説明する。 理論的貢献では、本研究が既存の知識体系にどのような新たな知見を提供したのかを明示的かつ簡潔に説明する。実務的含意では、研究結果に基づく解釈として、マーケティング実務者へ具体的にどのような行動指針を提示できるかを述べる必要がある。また、このパートでは、研究の限界（手法上の限界など）についても説明する必要がある。なお、限界を説明する場合にはその限界に基づき、どのような将来的な研究機会を見いだせるのかについても説明することが好ましい。 3.3.3 企業リサーチにおけるレポート 企業におけるリサーチでも、結果をまとめレポートを執筆することが重要になる。レポートは、リサーチに関する努力の有形の成果物となり、プロジェクトの記録的な資料としても機能する。また、マネージャーによる実務的な意思決定はレポートを読むことでリサーチからの影響を受ける。そして、リサーチそのものをプロジェクトとして捉える場合、そのプロジェクトについての質は評価されるべきである。その場合、多くのマネージャーはプロジェクトそのもの質をレポートの質から評価する（Malhotra, 2019）。そのため、マネージャーが知覚したレポートの有用性はその後再びリサーチを行うかに影響する。 企業リサーチにおけるレポート執筆でも、先述の論文執筆に関する考え方や構成が役に立つ。一方で企業向けレポートでは、学術論文と異なるいくつかの工夫も必要になる。第一に、レポートの読者層を特定しその読者に向けて書く必要がある。例えば、企業におけるリサーチレポートを書くのであれば、主な読者はマーケティング意思決定者（管理職従事者）である。彼/彼女らは基本的に忙しく、かつ必ずしも統計的分析手法の専門家とは限らないため、レポートは簡潔かつ平易な言葉で書く必要がある。そのため、一般的に不必要な専門用語は避け、直感的かつ説明的に書くことが好ましい。どうしても技術的説明が必要になる場合には、脚注に記す等の工夫が必要になる。また、図表のような視覚的コンテンツで内容を補強することも有効である。第二に、企業におけるリサーチレポートでは、学術論文とは重視する点が異なる。特に、企業におけるレポートでは、既存の文献レビューや理論的議論については多くの場合不要であり、最低限に留める必要がある。このようなレポートでは、文献レビューや理論的議論のかわりに、実務的課題、研究課題、知見、実務的含意（助言）が簡潔かつ明示的に提示されることが好ましい。第三に、レポートの構成も学術論文とは異なることが多い。最もフォーマルな形式のリサーチレポートは、（1）序文、（2）本文、（3）付録という構成で作成される （Malhotra, 2019）。序文パートでは、タイトルページ、カバーレター、目次、エグゼクティブサマリー（要約や概要）、主な発見物や含意（助言等）、などを記載する。本文パートでは、問題背景と研究課題から始まり、調査・分析手法、結果、限界と留意点、結論と含意を説明する。最後に付録では、調査で用いたアンケート項目や詳細な分析結果表など、読み飛ばすことも可能だが、研究内容を詳細に理解するためには不可欠な情報を追加する。企業でのマーケティングリサーチレポートの場合、これらの工夫が必要になるが、リサーチを実行する際に経るプロセスについては学術的研究と共通する部分も多い。そのため、実際にリサーチを実行する前に、本節で紹介したリサーチの全体像を把握することが必要になる。 上記で紹介した構成は、研究を行い、それを文章化する上で必要な情報を捉えている。そのため、研究者は、現実的なトピック、理論、手法の三つの側面について知識を蓄積し研究に望むことが求められる。 抽象度の高い既存の理論や枠組みに基づき、具体的な事象に対して論理的に結論を導く考え方である。↩︎ 変数の定義については色々とあるが、ここではデータにおける観測対象となっている情報とする（倉田・星野、2011）。↩︎ "],["question.html", "3.4 研究課題（問い）の設定", " 3.4 研究課題（問い）の設定 研究は、「問いを立てる」ことから始まる。「問いを立てる」というと、自身の疑問を提示することであり「そんなのは簡単だ」と感じる人もいるかもしれない。しかし、本資料は研究課題（研究上の問い）として、特に実証的に検証可能な問いに着目し、「問いを立てるのはなかなか難しい」という立場を取る。研究者は問いをただ闇雲に思いつくままに述べればいいわけではない。なぜならば、研究課題はその後のリサーチデザイン設計にも深く関わることになるためである。言い換えると、リサーチデザインは、自身の立てた研究課題にきちんと回答できるように設計すべきである。自身の立てた研究課題とその後のリサーチデザインや議論との一貫性を保つことは存外難しく、多くの人にとっては何度か失敗を繰り返しながら学ぶものになる。また、本節で説明する「実務的課題」と「研究課題」との関係は、（意思決定者のサポートとしての）企業におけるマーケティングリサーチにおいても重要になる。クライアントが抱える実務的課題に対して効果的な助言（含意）を提供するためには、直面する実務的課題に則した研究課題の設定が必要になる。そのため、本節の内容は企業におけるマーケティングリサーチ課題の設計方法としても役立つと考えられる。 マーケティング研究における問いを適切に立てるためには、「実務的課題」と「研究課題」という二つの異なる課題のタイプが存在することを理解すべきである。この2つの課題弁別は、前章で説明したマーケティングに関わる二つの視点に準拠するものである。実務的課題とは、マーケティングに関する意思決定についての課題であり、主に意思決定者が何をすべきなのかを捉えている。例えば、ある企業における製品の市場シェアが減少していたとする。ここで、「どうればよいのか？」という問いは典型的な実務的課題だと考える。また、たとえ具体的な方策に着目した問いを立てたとしても、例えば、「モバイル広告を実施すべきか？」や「どうすればオムニチャネル化を推進できるか？」といった問いも実務的課題だといえる。マーケティングに興味を持つ学生の場合、このような実務的課題に関連する、問題を解決するための手段やアイデアを扱うことに慣れているかもしれない。しかし、これら問いに直接的に答えることが必ずしも研究にはならないということを理解する必要がある。 一方で研究課題（ここでは特に実証的な研究課題）とは、実証的に検証可能なものであり、現実社会で何が起きているのかについて、定量/定性的調査を通じて得た情報を用いて結論を提示できる問いを指す。例えば、「ファストリテイリングは何年から有明倉庫を稼働させたか？」という問いは、事実を調べることで回答できる。また、「新しいパッケージデザインは以前のものよりも消費者の購買意図を高めるか？」という問いも研究課題の例であり、消費者を対象とした実験調査によって回答可能である。多くの場合前者のような問いは単純すぎて研究課題として利用されない。田村（2006）では、研究課題の価値は個人的な関心だけで判断されるのではなく、学術的重要性と実務・社会的重要性の両面から評価されると強調されている。そのため、本格的な学術研究を行うためには上記の実務的課題加え、架空術的な貢献や価値についても考えなければならない。学術的な価値を考えるうえで基礎的な考え方は ?? 節で、既存の研究への貢献に関するより実践的な考え方は3.7 節で紹介する。 その上で本書が強調したいのは、研究の実行においては研究課題の提示が絶対に必要なのだが、我々が経営学という応用学問領域に属している以上、実務的課題も重要だという点である。マーケティングにおいては多くの場合、社会や実務で起こっている問題に対し何らかの示唆を与える研究を行うことが求められる。そのため、研究のための研究ではなく、社会や実務への含意を見いだせるような研究が重視される傾向にある。つまり我々にとっては実務的課題も重要になるものの、先述の通り実務的課題のままでは研究課題として機能しない。そこで、実務的課題を実証的研究課題に変換することが求められる。問いの変換方法として、ここでは浅野・矢内（2018）で提示されている二つの方法について取り上げる8。 第一の方法が、「参照枠組みを変える」という方法である。具体的には、実務的課題から議論の対象となる主体を特定し、彼/彼女らの評価について情報を収集する形に問いを変換するような方法だと言える。例えば、「企業は環境負荷に配慮されたチョコ製品を販売すべきか？」という実務的課題を考える。このとき例えば、企業が販売したチョコ製品を購入する主体である既存顧客を、問いの中心となる主体として設定することで、「既存顧客のチョコ製品選択に対し、企業による環境対応の有無は影響を与えるか？」という問いに変換することができる。この変換後の問いであれば、既存顧客へのアンケート等でデータを集め実証可能であるため、研究課題として機能すると考えられる。同様の問いを特定の企業活動に限定しない形で提示するならば、例えば「企業の環境、社会・ガバナンス（ESG）活動はチョコレート菓子市場における消費者の購買意図を向上させるのか？」という問いも設定可能である。 第二の方法は、「背後に想定されている暗黙の前提を問う」というものである。これは、実務的問いの背後に暗黙的に仮定されている理屈やメカニズムに自覚的になり、それ自体を問うものである。例えば、「どうすればオムニチャネル化を推進できるか（or すべきか）？」という実務的課題があったと考える。この問いの背後には、「オムニチャネル化が企業成果に好ましい影響を与えるはずだ」という暗黙の前提が置かれているかもしれない。オムニチャネルに限らず、多くのビジネス書などで話題になる戦略では、このような成果に対する暗黙の前提が置かれ、その用語だけが独り歩きして流行ることも散見される。では「そもそもオムニチャネル化は本当に企業成果に影響があるのかだろうか」、もしあるのだとすれば「どのような成果に対して影響があるのか」という問いは、暗黙の前提を問うものであり、非常に素朴だが重要な研究課題である。これに関連する研究として、「オムニチャネル化（チャネル間統合）は小売企業の売上成長率に影響するのか？」（Cao and Li, 2015）や「オムニチャネル化（チャネル間統合）は小売企業の費用効率性に影響するのか？」（Tagashira and Minami, 2019）といった研究課題が実際に扱われ、論文化されている。 暗黙の前提を捉えた別の具体的な研究例として、Lim et al. （2020） による、顧客満足度が成果へ与える影響について捉えた論文が挙げられる。顧客満足度はマーケティングにおいて非常に重要視される概念である。顧客満足度の企業にとっての重要性は、顧客満足度が高まることによって顧客による再購買が増え、企業のマーケティング費用が効率化される、というロジックによって説明されてきた。しかしながらそれは本当だろうか、というのが Lim et al. （2020） の研究課題である。Lim et al.（2020）の結果が気になる場合はぜひ論文を読んでみて欲しい。このように暗黙の前提を問うような研究課題の設定は、非常に素朴な問いになるがそれだけに、もしそれが既存研究で未解決である場合には大きな理論的貢献につながる可能性を持つ。 本節では、研究課題の設定について説明した。本講義で扱う研究課題は、検証可能であり実務的課題とは異なるものであるという点を理解してほしい。また、次章（4.1節）ではリサーチデザインと研究課題との関連について説明しているため、そちらも合わせて研究課題設計について理解してほしい。次節では自ら立てた問いと整合的な議論を提示するための理論や仮説構築について説明する。 浅野・矢内（2018）では、規範的議論と実証的議論との対比で以下の内容を提示している。規範的議論に関心がある場合は、浅野・矢内（2018）を参照してほしい。↩︎ "],["問題と分析をつなげる仮説の提示.html", "3.5 問題と分析をつなげる仮説の提示", " 3.5 問題と分析をつなげる仮説の提示 リサーチクエスチョン（RQ）は通常未回答であり、未知の問題である。実証的な研究は、このような未知のRQに対して、「おそらくこうなっているだろう」と予測を立て、それが尤もらしいかどうかを確認する。ここで用いられる予測のことを「仮説」と呼ぶ。仮説はあくまで予測であるのだが、思いつきや当てずっぽうで提示することは好ましくない。仮説の提示においては、既存の知見を参照しながら論理的に、根拠を持って提示することが求められる。 マーケティングに関する実証分析では、抽象度の理論に基づく仮説を提示し、データを用いた分析によって仮説を検証するという形式が取る事が多い。言い換えると、事象を観察し、論理的な説明としての仮説を提示ししたうえで、それを客観的・科学的と考えられる手順で検証するというプロセスを通じて証明を行う。そのため、マーケティング研究における仮説は、検証可能かつ研究課題や研究が依拠する理論と整合的である必要がある。 本書では、理論を事象の原因と結果に関する一般的（抽象度の高い）理屈であると考える（浅野・矢内, 2018）。詳しくは後述するが、理論について詳細に考察すると、理論は現在広く受け入れられている仮説だと言い換えることができる。そういった意味で、「理論」と「仮説」の間に本質的な違いはないといえる。しかしながら、マーケティング領域では、様々な具体的変数に応用できる「抽象度の高い構成概念同士の関係」を理論と呼び、「より具体的な変数間の関係」を仮説と呼ぶことが多い。 構成概念とは直接観測することはできないがその存在を仮定することで、測定や観測を可能にするために構成された抽象的概念のことである。例えばマーケティング分野では、「顧客満足」や「顧客エンゲージメント」といった構成概念が用いられる。そのため、本書では説明の容易さのために、理論と仮説という言葉を区別して用いる。具体的には、理論とは抽象度の高い概念同士の関係を表し、仮説（作業仮説）は、理論を検証するために引き出された特定の変数間の関係に関する記述を指す（浅野・矢内, 2018）。そのうえで仮説は、データに基づく検証のベースとなる記述であるため、その内容は入手可能な変数間の関係として記述することが大切になる。 ここで、「お金がある人ほど衝動買いをする」という理論があったと仮定して、作業仮説化について考えてみる。作業仮説化においては、この理論と整合的かつ測定（検証）可能な変数を捉えた記述であることが重要だと述べた。それを踏まえ、以下の二つの作業仮説例を考える。 「年収」の高い人ほど「衝動買い性向の程度」が高い 「買い物時の予算」が多い人ほどその買い物における「非計画購買購入額」が高い （1）の例は、年収という個人属性と衝動買い性向という心理尺度を捉えており、個人の特性を表す二変数間の関係を示した仮説である。一方で、（2）はある購買客の入店時予算とその買い物時に発生した非計画購買額を捉えている。この二つの例は、作業仮説化において重要な要素である「分析単位の一貫性」を満たしている。基本的に作業仮説化で捉える変数は同一の分析単位である必要がある。どちらの例も、特定の消費者に関する（1）属性と心理尺度と、（2）買い物時の予算と購買額、という形で測定単位が一致している。これがもし、消費者個人の特徴と店舗での売上との関係を記述した仮説である場合、分析単位が異なるため、データによる分析と仮説検証が困難になる。したがって、特別な場合を除き分析単位の一貫性を守ることは重要となる。 先述の二つの仮説は分析単位の一貫性は守っていた一方で、「理論を正確に参照する」という点においては注意が必要である。「お金がある人ほど衝動買いをする」という理論では、「お金」と「衝動買い」という二つの概念間の正の関係が示唆されている。それに対して一つめの仮説では、「年収」という個人属性を示す変数で「お金」という概念を捉えており、個人の心理的傾向としての「衝動買いのしやすさ」を「衝動買い」の変数として扱っている。他方で二つめの仮説では、「特定の購買時点での予算」と、その買い物での「非計画購買の額」を捉えている。「お金がある人ほど衝動買いをする」という理論から導出された仮説としては、どちらもある程度の一貫性がありそうだが、両者は全く違う変数を捉えている。この場合、どちらか一方もしくはどちらも不正確に理論を参照している可能性がある。もしかしたらその理論を提唱している最初の論文を正確に参照すれば、消費者の所得と心理的性向を捉えたものであることがわかり、一つめの仮説化が適切であることが判明するかもしれない。しかしながら、「お金」や「衝動買い」という曖昧で自分にとって理解しやすい別の言葉に置き換えて理論を参照している状態では、その判断もつかない。そのため、抽象度の高い理論に関する論文が難解であったとしても、できる限り正確に、論文が述べていることをそのまま理解する必要がある。また、無意識的にしろ意識的にしろ、研究者にとって都合の良い（測定しやすい）文脈に理論を読み違えて仮説化してしまう場合も散見される。このような問題を避けるためにも、その理論が具体的にどのような視点に基づき議論を展開しているのかについてできるだけ正確に内容を理解することが必要になる。 多くのマーケティング研究では、抽象度の高い理論に基づき仮説を提示し、データを用いて仮説を検証する。仮説の提示においては、着目する研究課題、理論、分析単位の一貫性を保つことが重要になる。本節の後半では特に、理論を正確に参照することの重要性について強調した。本章の以降の節では、学術的研究を実行するために必要となる、マーケティング研究に関する知識や考え方を紹介する。具体的には、学術的研究を実施するうえで特に重要になる科学哲学や、理論的貢献の提示について説明する。 "],["philosphy.html", "3.6 学術的研究のための留意点", " 3.6 学術的研究のための留意点 3.6.1 マーケティング研究と科学哲学 本書では、主にマーケティングに関する定量的な調査・分析手法を紹介している。それでは、これらの手法によって明らかにされた知見はどのような意味を持つのだろうか。人によっては、データに基づき定量的に示された結果は、揺るぎないこの世の真実であると考えるかもしれない。しかしながら、そのような捉え方は科学的とは言い難い。そのために本節では、研究を行うことそのものの目的について学術的視点から考えることで、社会科学における科学的姿勢とはどのようなものかを概観する。 「科学的であるということはどういうことか。」この問題について考える領域は科学哲学と呼ばれており、本節では社会科学における科学哲学（吉田, 2021）とマーケティング研究におけるパラダイム議論（宇野ほか, 2022）の議論を整理した内容を紹介する。そのため、この議論に関心のある場合には、これらの文献およびそこで提示されている関連図書を参照してほしい。なお科学哲学に関する議論は、学術的研究を実施する際に特に重要な内容である。そのため、本節は大学院修士・博士課程に在籍、または今後進学を考えている読者に特に理解してほしい。科学哲学の潮流においては、論理実証主義、反証主義、解釈主義、などいくつかの主要な考え方が存在する。宇野ほか（2022）によると、マーケティング領域においては実証主義と解釈主義という二つの主義が主流のパラダイムとして存在する。実証主義では研究者は現象から独立した存在という立場のもと、普遍的な法則見出すことで知識を形成すること目指す。一方で解釈主義では、研究者が現実から独立し事象を観察することは困難であるという立場をとる。 その上で本資料では、より広範な社会科学における科学哲学との関連で議論が可能になるように、吉田（2021）の議論に従い、主に自然主義と解釈主義という二つの考え方に大別し議論を整理する。第一に、自然主義は現在の社会科学研究における有力な立場であり、社会現象は自然現象と同じように研究できると考える。これは、特に数学や物理学を中心に自然科学として蓄積されてきた方法論や考え方を採用することで社会現象に関する客観的かつ信頼性の高い知識の蓄積を試みる考え方である。上述の実証主義はこの自然主義的潮流に含まれる、19世紀に論じられた哲学である。その後様々な批判や社会変化を受け修正されることで、20世紀前半には論理実証主義という形で論じられるようになった。論理実証主義では、論理的に推測されたものが客観的な経験的事実に基づいて証明されたとき、それを科学的に正しいと捉える。そのうえで、このパラダイムでは再現可能性が重視された。しかし現実的には、科学的に正しいとされた検証結果が後に覆ることもある。その際、元々正しいとされた結論をどのように受け止めるべきなのかという問題が残る。また、論理実証主義的立場からは、観察・検証できないものは科学でないと切り捨てざるを得ない。 これらの問題を克服すべく議論されたのが、ポパーによる反証主義である（ポパー, 1980; 吉田, 2021）。反証主義においては、科学的であることの条件として「反証可能性」を重視する。これは、ある主張が科学的であるためにはその主張が間違っていることを証明される可能性を有している必要があるとする考え方である。反証可能性の詳しい説明と具体例については後述するが、このように自然科学的科学観に基づき社会現象を研究しようとする考え方を総じて自然主義と呼び、現代の社会科学研究の主流となっている（吉田,2021）。なお、宇野ほか（2022）によると、2016年から2021年に出版されたマーケティング領域のトップジャーナル8誌9に掲載された論文のうち、96%以上が実証主義的アプローチに基づくものであったとされている。 第二の解釈主義は、社会現象の研究には独自の手法が必要であるとする立場である。本資料では解釈主義に関する詳細な説明は避けるが、この考え方では特に社会現象においては因果的説明だけでなく、社会現象を構成する人々への内的な理解も必要であると考えられている。また、解釈主義的立場は反実証主義的立場を取り、社会科学研究においては研究者や研究対象者の意図を排除することはできないので、自然科学のように客観的かつ中立的な研究を実施することはできないと主張する。社会科学は自然科学とは異なり、社会現象を研究対象としている。この社会現象の特殊性が、解釈主義者が自然主義を批判する根拠となる。特に社会現象においては、「意図せざる結果」の影響が自然科学に比べて強いということが言われている。この意図せざる結果は主に「自己成就的予言」と「自己破壊的予言」に分けて説明することができる。自己成就的予言とは、たとえ事実に基づかない予言であっても、何らかの発言（予言）を行うことでそれが実現してしまうことである。例えば社会的不安を煽るデマ情報が流布され、その情報を目にした民衆がその不安を信じ込んでしまうことで、本来は根拠ないデマであった予言が実現してしまうことである。一方で自己破壊的予言は、予言されたことによってそれが実現されなくなることである。例えば、根拠があるものの楽観的な予想が提示されたことによって、当事者が油断しまいその予言が実現しなくなることが典型的な例として挙げられる。 解釈主義者が主張するように社会現象と自然現象には違いがある。違いがあること自体は事実であるため、社会科学がただ暗黙的に自然科学と全く同じ手法を踏襲するということが好ましくない場合もあるかもしれない。しかしながら、だからといって自然科学で蓄積されてきた客観的かつ信頼性の高い手法を放棄すべきだということはあまりに極端な主張であるとも考えられる。これらの主義については唯一絶対の正解が存在するわけではなく、継続的な議論や批判による発展が必要になる。しかしながら吉田（2021）は、現時点において社会科学分野で科学的研究を行うためには、研究プロセスは最低限、推測と反駁の方法として「反証可能性」を有している必要があると主張する。この反証可能性は、先述の通り反証主義的考え方であり、「漸進的な発展を想定した科学観」を反映している。反証可能性はテスト可能性などとも呼ばれ、科学的理論それ自体が正しいのか誤っているのかを確認することができる可能性を表す。反証可能性はポパーによる反証主義において科学的基本として捉えられており、科学的な理論や主張はそれを反証する余地を有する必要があると議論されている。つまり、科学とは何らかの真理に至るための方法論であり、何らかの絶対的な真理を前提とすることは科学的ではないと考える。そしてこの反証可能性が科学的か非科学的かを分ける決定的な違いであると捉えられている。 反証可能性を有していない議論の例としては、フロイトによる精神分析が有名である。精神分析は常に正しい理論であるため、科学的ではないと言われている。この点について伊勢田（2003）は、以下のように説明している。精神分析の枠組みでは、人間の心は自我（意識的欲求）、超自我（社会的・道徳的行動統制）と、イド（無意識の欲求）によって構成されている。この理論において、研究者が潜在的な無意識の欲求が本当に存在するのかという問いに関心をもったと想定する。研究者による調査・分析によって、その潜在的な欲求を示唆する行動が観察されれば潜在的欲求仮説は支持される。しかし、そのような行動が観察できない場合、どのように結論付けられるのか？フロイトの理論では、「無意識の欲求は存在するが、超自我によって統制されて顕在化しない」と説明される。すなわち、無意識の欲求の存在についてこの理論は反証可能性を有していないことになる。このように反証されるリスクを背負っていない主張は、反証可能性を軸とした科学哲学に基づくと科学的ではないということになる。 例えば、「AはBを高める」という予想があったとしよう。この予想が科学的である場合、特定の手続きを経て調査・分析を行い、ある結果が出ればこの予想を受けいれ、それ以外の結果であればこの予想が誤っていると結論づけることができる。このように、ある理論が科学的であるためには、絶えず反証による理論の修正や、新たな理論の提案を行うことが可能であることが重要となる（ポパー, 1980; 吉田, 2021）。その上で本資料では自然主義的な立場を取りつつも、社会現象が持つ独自性をもつことも認める。そのうえで吉田（2021）の議論の通り反証可能性をもつことが科学的知識の条件として考える。そのため、調査・分析の結果も唯一絶対の真理ではなく常に反証される可能性を有している、という理解のもと本資料の内容が構成されていることを理解して欲しい。本節では、我々が調査によって明らかにする知見・理論が持つ特性や目的について、主に反証可能性という概念の重要性を論じてきた。次節では、次節では、学術的研究において重要になる理論および理論的貢献について説明・紹介する。 Journal of Consumer Psychology，Journal of Consumer Research，Journal of Marketing，Journal of Marketing Research，Journal of the Academy of Marketing Science，Marketing Science，International Journal of Research in Marketing，Journal of Retailing↩︎ "],["elaboration.html", "3.7 研究における理論の利用と精緻化", " 3.7 研究における理論の利用と精緻化 実証的なマーケティング研究では、結果や議論との間の論理的整合性に寄与する「理論」が重要になる。また、研究課題の価値は学術的貢献という側面からも評価されると述べた（田村, 2006）。本節では、既存の研究群に対して貢献を与えるうえでのより実践的な考え方を紹介する。 本書では理論を、何らかの客観的真理を探求するための主張だとする「実在論」的な立場に基づき捉える。しかしながらここで強調したいのは、科学的な理論が「絶対的かつ不変の真理を意味しない」ということである。言い換えると、理論はあくまで暫定的な仮説に過ぎず、経験的な整合性などの観点から絶えず批判にさらされながら改善していくことが求められる。なお、実在論と相反する立場として「道具主義」という考え方も存在する（吉田, 2021）。道具主義では、科学理論を予想や説明のための便利な道具として捉えるため、用いている理論の経験的正しさは問題視しない。これらの異なる立場に対する優劣を決めるのは困難である。しかしいずれの立場においても、可謬性という、「知識についての主張は原理的に誤りうる」という性質を受け入れ、自己批判と相互批判に基づきより良い理論を目指すことが重要ではないかと言われている（吉田, 2021）。 本書では、先述の通り、理論を事象の原因と結果に関する一般的（抽象度の高い）理屈であると考える（浅野・矢内, 2018）。マーケティング領域では様々な分野から理論を応用することになるのだが、学術的研究では理論的貢献として、着目する理論的議論や理解を前進させることが求められる。広義の経営学領域における理論的貢献に関して Fisher and Aguinis （2017） は、theory generation（理論生成）、theory testing（理論検証）、theory elaboration（理論精緻化）という3種類のアプローチを紹介している。Theory generationは、ゼロから新たに検証可能な理論や概念を提示することを目的とする。これは、まだ既存の理論がなく説明されていない現象について、新たな構成概念の導出や概念間の関係について論理的かつ十分に納得の行く議論を展開することで新たな知見を提示するアプローチである。一方で theory testing は、既存の理論を検証することを目的とするアプローチである。既存の理論から具体的な仮説を導出し、データを収集し分析することで、提示した仮説的関係が支持されるかを検証するアプローチである。 Fisher and Aguinis （2017） では、theory generation と theory testing よりも特に、既存の理論を洗練させ、拡張することに重点を置いたtheory elaborationの重要性が強調された。Theory elaboration は、theory generation や theory testing よりも、漸進的な理論のアップデートについて捉えた考え方であり、既存の理論からの積み重ねによって理論を改善していくプロセスとして期待される。Theory elaboration の主な実施アプローチとして Fisher and Aguinis （2017） は、「対比（Contrasting）」「構成要素の特定（Construct specification）」「構造化（Structuring）」の3つを提示した。 「対比」アプローチでは、異なる理論や構成要素を比較し、類似点や相違点を明らかにする。このアプローチに関連する具体的なプロセスとして、水平的対比と垂直的対比がある。水平的対比は、既存の理論を異なる文脈で検討することで、その理論がどのように適合するかを調べるプロセスである。これにより、異なる文脈での理論の有用性や限界が明らかになる。垂直的対比は、同じ文脈内で異なるレベルの抽象度を持つ理論や構成要素を比較することで、新しい洞察を得るプロセスである。これにより、より具体的または抽象的なレベルで理論を分析し、新しい仮説や洞察を導き出すことが期待される。 「構成要素の特定」アプローチでは、構成要素定義の洗練化や明確化によって構成要素の妥当性を向上させることを目的とする。このアプローチに関連する具体的なプロセスとして、新しい構成要素の指定や構成要素の分割が考えられる。新しい構成要素の指定は、既存の理論で考慮されていなかった新しい概念を特定し、それを定義することである。これにより、既存の理論がカバーしていなかった現実世界の側面を捉えることができる。一方で構成要素の分割は、既存の理論内の概念をより明確に区別し分けて捉えることである。これにより、類似した概念間の区別が明確化され、それらが異なる現象を説明するために使用できることを示すことで、理論の妥当性や範囲を向上させる。 「構造化」アプローチでは、構成要素間の関係をより体系的に整理することを目指す。このアプローチに関連する具体的なプロセスとして、「特定の関係の構造化」、「連続的関係の構造化」、「循環的関係の構造化」などがある。「特定の関係の構造化」では、二つの概念間の特定の関係を明確に定義する。これにより、異なる概念間の関係性がより明確になり、理論が現実世界にどのように適合するかが明らかになる。「連続的関係の構造化」は、概念やイベントに関する一連や順序的な共起関係を説明することである。これにより、現象やイベントがどのように発生し、その結果として次にどのように進行していくかを説明することができる。 最後に、「循環的関係の構造化」は、二つ（以上）の概念間での相互作用（双方向の因果）を説明する。これにより、理論が現実世界でどのように適合し、相互作用がどのように進行するかを説明することができる。 本節では、理論を何らかの真理を探求するためのものだと捉えた。しかしながら、理論は唯一絶対の真理を意味せず、経験的な整合性などの観点から絶えず批判にさらされながら改善していくものである。その意味において理論と仮説は本質的には同質的であるものの、マーケティング領域では、抽象的で様々な分野へ応用可能な概念間の関係を理論、より具体的な変数間の関係を仮説と言う事が多い。また本節では、先行研究から理論をただ援用するだけではなく、既存研究へ理論的貢献を与えるための theory elaboration について説明した。Theory generation や theory testing だけでなく、より漸進的 な理論のアップデートについてその具体的なアプローチも重ねて理解することで、より発展的な論文執筆に役立つことを期待している。 "],["研究課題と仮説の設定例.html", "3.8 研究課題と仮説の設定例", " 3.8 研究課題と仮説の設定例 本節では、ある小売企業が「オムニチャネル化10を進めるべきか」という実務的問題を考えている状況を想定して、実証的な研究課題と仮説の設定について説明する。なおここではうまくいっていない例とその改善案も含めて提示することで、各要素の一貫性を保つことの難しいさや注意点について説明することを目指す。 ここでは、以下のような例を考える。上記の実務的問題に対して、「消費者のオムニチャネル小売への評価において重要な要因は何か？」という研究課題を設定したうえで、「有名人による推奨があると、オムニチャネル小売企業の売上が上がる」という仮説が提示されたとする（簡略化とその議論のために論拠は省略し、仮説のみを記述している）。 まず、研究課題について検討する。ここで提示されている研究課題は、参照枠組みを変える方法を応用し議論の焦点を消費者に合わせていることが伺える。しかしながら「重要な要因は何か」という問いは素朴かつ重要であるものの、探索的でありEFアプローチに適した問いである（10 節参照）。実証的な分析を実施するためには、検証可能な問いに洗練化させる必要がある。例えば、関連する既存研究や産業的知識から重要だと考えられる要素を特定し、なぜその要素が重要なのかを議論することが求められる。例えば、その後の仮説で有名人を用いた広告に着目しているのであれば、この要素を捉えることの重要性と妥当性を説明するように務めることが懸命だろう。 続いて、仮説に関して検討する。第一に、研究課題と分析単位が異なることに注意が必要である。研究課題では消費者を主体として特定している一方で、仮説では企業の売上を捉えている。もちろん売上は消費者行動と無関係ではない。しかしながら、この仮説が先述の研究課題と整合的である理由について説得的に論じる必要がある。 そしてこの分析単位の違いは仮説を導出するための議論においても注意が必要になる。仮説は、研究者の思いつきではなく、理論的な根拠を持って提示する必要がある。有名人の推奨（celebrity endorsement）に関する議論には、心理学的理論に依拠したものも多く、推奨が対象（製品）へのポジティブな感情や知覚につながるということが、伝統的な有効性の議論である（Erdogan, 1999）。このような先行研究での議論を論拠に仮説を提示する場合、この論拠はあくまで製品への感情や知覚に対しての影響をサポートするものであるため、製品売上についての議論直結しない可能性があることに注意が必要である。その場合、自身が依拠している理論と提示した仮説との間に一貫性がなく、やはり仮説としても消費者個人レベルの議論に着目したほうが良いかもしれない。 第二に、仮説で想定している議論の構造が不明確である点にも注意が必要である。まず、「有名人による推奨」と何を比べているのかが不明確である。例えば比較対象は「有名人以外の誰かによる推奨」であるのか、「推奨がない場合」なのかを明確にし、その上でなぜこれらと比較して有名人による推奨が高い売上（もしくは消費者による評価）につながるのかを議論する必要がある。 これに加えて、仮説では「オムニチャネル小売企業の売上が上がる」と述べている。これはオムニチャネル小売を行っている企業に限定した状況で、有名人推奨あり vs. なしを比較することを意味しているのだろうか。それとも、（1）オムニチャネルかつ有名人推奨あり、（2）非オムニチャネルかつ有名人推奨あり、（3）オムニチャネルかつ有名人推奨なし、（4）非オムニチャネルかつ有名人推奨なし、という4つの戦略タイプのうち、（1）の売上が高いということを言っているのだろうか。「有名人による推奨があると、オムニチャネル小売企業の売上が上がる」という文言からは、オムニチャネル小売に限定した議論の方が近いような印象を受ける。しかしその場合、「小売企業がオムニチャネル化を進めるべきか」という、非オムニチャネルからオムニチャネルへの変化を検討している実務的課題とは整合的でなくなる。 このように文章化をすると、ここで指摘している注意点が極めて当たり前のことだと感じる読者もいるかも知れない。しかしながら、初めて本格的な研究を行い、論文を執筆する場合、「オムニチャネル化を進めるすべきか」といった関心に対して「消費者のオムニチャネル小売への評価において重要な要因は何か？」という研究課題や「有名人による推奨があると、オムニチャネル小売企業の売上が上がる」といったような仮説を提示するようなケースは決して珍しくない。そのような研究課題や仮説を提示することから始まり、少しずつ上達するように訓練していくことが肝要になる。すぐに上手く議論を展開できない場合にも焦らずに、上記で述べたような注意点を参照し試行錯誤を繰り返すことで一貫性を保った議論を行うことに慣れてくるだろう。 オフライン（実店舗）とオンライン（Eコマース、モバイル）といった販売・コミュニケーションチャネルを結合し、様々なチャネルをシームレスに行き来できるような小売環境の設計↩︎ "],["練習問題.html", "3.9 練習問題", " 3.9 練習問題 3.9.1 研究課題の導出 ある企業が「フェアトレード製品を開発すべきか？」という実務的問題を考えている状況を考える。このとき、参照の対象となる主体を特定したうえで、「参照枠組みを変える」方法を用いてマーケティングに関する実証的リサーチクエスチョンを提示してみよう。 ある企業が「新しくモバイル広告を導入すべきだ」という実務的主張を持っている状況を考える。このとき、想定されている暗黙の前提を特定したうえで、「暗黙の前提を問う」方法を用いてマーケティングに関する実証的リサーチクエスチョンを提示してみよう。 3.9.2 仮説の導出 あなたが、とある食品スーパーの店舗運営を担当する職務についている状況を考えてほしい。あなたが担当しているとある店舗は、日々の売上は悪くないものの、購買客が固定化されており、客単価および新規顧客数のどちらも伸び悩んでいる。そんな中マーケティングに関する勉強をしていると、以下の消費者行動に関する理論を知ったとする： 消費者の店舗内移動距離が長いほど、非計画購買をしやすい。 製品・サービスへのこだわりが少ない消費者ほど、有名人を使った広告アピールの影響を受けやすい（購入・サービス利用しやすい）。 上記の内容を踏まえ、以下の各問いに答えなさい。 実務的問いを提示しなさい。 実証的リサーチクエスチョンを提示しなさい。 上記の二つの理論のうち、どちらか一方を選び、その理論に基づく作業仮説を提示しなさい。その際、説明変数、被説明変数、分析単位を特定すること。 "],["参考文献-1.html", "3.10 参考文献", " 3.10 参考文献 浅野雅彦・矢内勇生 (2018) 「Rによる計量政治学」, オーム社. 伊勢田哲治 (2003) 「疑似科学と科学の哲学」, 名古屋大学出版会. 宇野舞・グェン フォン バオ チャウ・趙雅欣・六嶋俊太・福川恭子（2022）『マーケティング研究領域におけるパラダイムの現状と課題 : より活発な理論発展へ向けて』,「一橋商学論叢」, 17(2), 14-30. 倉田博史・星野崇宏（2011）「入門統計解析」，新世社. 田村正紀（2006）「リサーチデザイン」,白桃書房. ポパーカール (1980) 「推測と反駁-科学的知識の発展」, 藤本隆志・石垣壽郎・森博訳, 法政大学出版局. ベルクラッセル・フィッシャーアイリーン・コジネッツロバート（2016）「消費者理解のための定性的マーケティング・リサーチ」，松井剛訳，碩学社. 吉田敬 (2021) 「社会科学の哲学入門」, 勁草書房. Cao, L., &amp; Li, L. (2015). The Impact of Cross-Channel Integration on Retailers’ Sales Growth. Journal of Retailing, 91(2), 198-216. Erdogan, B. Z. (1999). Celebrity Endorsement: A Literature Review. Journal of Marketing Management, 15(4), 291-314. doi:10.1362/026725799784870379 Enns, P. K., Lagodny, J., &amp; Schuldt, J. P. (2017). Understanding the 2016 US presidential polls: The importance of hidden Trump supporters. Statistics, Politics and Policy, 8(1), 41–63. Fisher, G., &amp; Aguinis, H. (2017). Using Theory Elaboration to Make Theoretical Advancements. Organizational Research Methods, 20(3), 438-464. Golder, P. N., Dekimpe, M. G., An, J. T., van Heerde, H. J., Kim, D. S. U., &amp; Alba, J. W. (2023). Learning from Data: An Empirics-First Approach to Relevant Knowledge Generation. Journal of Marketing, 87(3), 319-336. Granbois, D. H. (1968), Improving the Study of Customer In-Store Behavior, Journal of Marketing, 32 (October), 28–33. Hui, S. K., Inman, J. J., Huang, Y., &amp; Suher, J. (2013). The Effect of In-Store Travel Distance on Unplanned Spending: Applications to Mobile Promotion Strategies. Journal of Marketing, 77(2), 1-16. Lim, L. G., Tuli, K. R., &amp; Grewal, R. (2020). Customer Satisfaction and Its Impact on the Future Costs of Selling. Journal of Marketing, 84(4), 23-44. Malhotra, N. (2019) Marketing Research: An Applied Orientation, Pearson Education Limited. Schwarz, N. (2004). Metacognitive Experiences in Consumer Judgment and Decision Making. Journal of Consumer Psychology, 14(4), 332-348. Schwarz, N., Jalbert, M., Noah, T., &amp; Zhang, L. (2021). Metacognitive experiences as information: Processing fluency in consumer judgment and decision making. Consumer Psychology Review, 4(1), 4-25. Tagashira, T., &amp; Minami, C. (2019). The Effect of Cross-Channel Integration on Cost Efficiency. Journal of Interactive Marketing, 47, 68-83. "],["survey.html", "Chapter 4 尺度と質問紙調査設計 ", " Chapter 4 尺度と質問紙調査設計 "],["本章の概要-1.html", "4.1 本章の概要", " 4.1 本章の概要 コンピュータや情報通信技術が発展した現代においては、様々な手段で研究に用いるデータを集めることがきる。例えば、研究者の関心や研究課題のために収集されたデータを用いることもあれば、別の目的で収集されたデータを用いることもある。前者のようなデータを「一次データ」、後者を「二次データ」と呼ぶ。また、採用するデータの集計レベル（分析単位）にも注意が必要である。集計レベルの違いとして、個人の行動や回答を捉えた「非集計データ」と、非集計データをある単位でまとめ、計算や整理を施した「集計データ」とを捉えることができる。 マーケティングでは、消費者個人の状態や反応について、研究に適したデータを取得したいと考えることが多い。つまり、非集計の一次データを用いて消費者についての洞察を得ることが行われる。しかしながら、ただ闇雲にデータを集めれば良いというわけではなく、自身の研究に必要な情報を抽出するためにデータ収集方法を設計する必要がある。そこで本章では、一次データかつ非集計データを得る方法としてマーケティングや消費者行動領域で広く利用されている質問紙調査に焦点を合わせ、以下の点について説明する。 質問紙調査の得意分野とマーケティングリサーチとの関係 調査対象者の確保 心理尺度と質問紙設計 質問紙作成における実践的注意点 質問紙調査に伴う研究倫理 第一に、マーケティングリサーチにおける質問紙調査の役割について説明する。近年、情報システムの発展により様々なデータ取得の機会が増えた。オンライン上でのページ遷移や閲覧状況などの行動を追跡できることや、店舗での購買行動についてのデータも盛んに活用されている。このような状況でも、質問紙調査は多くの企業や研究者によって活用されているデータ収集方法である。その背景には、現行の行動データでは「どのようにもしくはなぜその行動に至ったのか」という消費者の内的な心理的プロセスについては観察できないという問題がある。観察できない内的プロセスを理論的モデルによって代替し行動データのみに着目する研究アプローチもあるが、心理学的アプローチに基づく研究では伝統的に、心理尺度を用いて質問紙を通じた調査を行ってきた。マーケティング領域では、質問紙調査を通じて消費者のブランド認知や再生、態度などについて知るためのノウハウが蓄積されてきている。 第二に、質問紙調査において必要な回答者（調査対象者）の確保について説明する。質問紙調査を実施するためには、いつ、どのような人に、どれぐらいの人数に対して調査を行うのかを決める必要がある。このような調査概要を明確化するために、6つのWs（Who, What, When, Where, Why, Way）に答えることが役立つ。これらの点に答えることで調査方法の概要が定まると期待できる（Malhotra, 2010）。また研究者がアクセス可能な個人に対して調査を行うためのサンプリング手法についても、代表的な方法をいくつか紹介する。特に、統計的に好ましい性質を持っているランダムサンプリングや、ランダムサンプリングが実施できない場合の代替的手段としての非確率的サンプリングについて紹介する。 第三に、質問紙調査で用いる尺度について、心理尺度の議論を中心に説明する。心理学においては直接的かつ量的に測定できない概念に焦点を合わせることが多く、パーソナリティ特性や感情、態度などに対し心理尺度を用いて測定を試みる。マーケティングや消費者行動研究でも、心理尺度を応用することで観察できない概念について調査することを試みる。本章では、リッカート尺度や意味微分（Semantic differential: SD）法を中心に、心理尺度の構成方法について説明する。また、作成した尺度の信頼性と妥当性について検討することも重要である。信頼性とは採用した尺度が同じ結果を繰り返し得ることに繋がるかを捉えており、妥当性は採用した尺度が着目する概念を測定できているかを捉えている。 第四に、実践的な注意点として、質問におけるワーディング（言い回し）と質問票の構成について説明する。質問においては、不明瞭で回答者の混乱に繋がるワーディングは避けるべきである。典型的な誤りとして、（1）曖昧な質問、（2）ダブルバレル質問、（3）誘導的な表現、が含まれる。これらを回避することで調査対象者の回答の心理的なコストを避けることが求められる。質問紙調査内で用いる項目やワーディングが定まったら、それらをどのように構成するかを決める必要がある。基本的には答えやすく興味を引く質問を最初に聞き、難しい問題やセンシティブな質問は終売いや最後に尋ねるという方法が捉えられる。また、近年では回答の質を広報させるための工夫についても議論されている。これらの代表的な取り組みとして instructional manipulation check（IMC）や directed question scale（DQS）を紹介する。 第五に、研究倫理について述べる。質問紙調査によるデータ収集は、社会科学における「人を対象にした研究」に該当する。そのため、調査協力者が質問紙に回答することで不利になったり、個人の幸福を損ねる結果にならないよう配慮する必要がある。特に、調査を行う主体、目的、調査により得た情報の管理方法、公開有無や方法、倫理的配慮の内容などについて十分な情報を提供し、この条件について回答者から許諾（インフォームド・コンセント）を得ることが重要になる。 本節の最後に、質問紙調査に関する概要を説明する。質問紙調査とは、データ収集のための構造化された手法で、回答者が回答する一連の質問（書面または口頭）から構成される。質問紙調査は、研究者と回答者との間の言語的情報のみを用いるコミュニケーションとして捉えられる。その意味で質問紙調査は、調査者が求める情報を、回答者が回答できる一連の具体的な質問に変換することを目的とし設計される。調査の実施において研究者は、回答者が相互コミュニケーションに参加し、協力し、面接を完了するように意欲を高め、動機を与え、奨励すると同時に、回答誤差を最小化する必要がある。研究者はこの双方向的なコミュニケーションにおいて、下の図のようなノイズが生じることを理解し、それを削減する努力を講じることが求められる。 コミュニケーションノイズ なお、質問紙調査は思いつきのまま実行することは難しく、調査実施前に綿密な調査設計を計画する必要がある。調査設計は主に以下のプロセスを経ることで決定される（鈴木, 2016; Malhotra, 2010）。 研究上必要な情報を特定する 研究課題、理論、仮説の決定（3章） 調査概要（スケジュール、予算、対象者、サンプリング方法）の決定 いつ、誰に対して、どのようにして調査を行うかを検討する（4.3節）。 分析手法の検討 どのような分析手法を用いるのかを事前に定める（7章以降） 項目の作成 捉える概念と尺度の検討（4.4.1節） 先行研究レビュー ワーディングを決める 教示文や質問の言葉遣いを調整、決定する（4.5.1節） 質問票の構造を決める（4.5.2節） 質問の順番を決める 調査票のレイアウトや装丁を決める プレ調査の実施と準備 （相対的に）少人数への調査を実施、フィードバックを得る 修正と項目の確定 倫理的検討と審査（4.6節） 本調査の実施 本章では、上記のステップを踏まえ、質問紙調査設計について説明する。なお研究上必要な情報を特定には自身が設定した研究課題に深く関わるため、3 章を参照してほしい。 "],["psychscale.html", "4.2 マーケティングと質問紙調査", " 4.2 マーケティングと質問紙調査 4.2.1 マーケティングリサーチにおける質問紙調査の活用 研究で利用するデータには、データの目的に応じて一次データと二次データという異なるタイプが存在する。一次データは研究上の問いに回答するために実施された調査、実験や観察に基づき収集形成されたデータある。一方で二次データは、業務上蓄積されたデータ、民間リサーチ会社の統計データや、政府統計などに代表される他の目的で収集された、ないし継続して収集されているデータを指す。 現在は、様々な二次データがアクセス可能であり、二次データを利用することで研究上の問いに回答できる可能性も十分にある。例えば、企業の視点にたてば、組織内部の二次データ（業務活動で得たデータ: Point of sales (POS) データ、webサイトへのアクセス記録など）と外部の二次データ（民間リサーチ会社の統計データや政府統計など）が存在する。その他にも、オープンソース化されているデータも様々存在する。自身の研究において用いているデータが二次データである場合、その出典や取得方法について論文やレポート内で明記する必要がある。一方で一次データを取得した場合、データの取得方法やデータの詳細について説明する必要がある。 そのため、本書で一次時データを収集する前に、関連する二次データとしてどのようなものが存在しアクセス可能なのか検討することを勧める。しかしながら、二次データではすでに集計や加工をされたデータしか入手できず、raw データ（収集されたまま加工されていないデータ）にアクセスできない場合もある。そのため、入手可能な二次データが本当に自身の研究課題や仮説で議論されている内容および集計レベルと整合的なのか、という点については慎重に検討する必要がある。 また、データの集計レベルにおいても非集計データと集計データという分類が可能になる。非集計データの例としては、ID-POSデータ（ロイヤルティカード（アプリ）などによる顧客の個人IDと、購買製品、価格、数量などの情報が含まれたPOSデータを結合したもの）や、消費者個人を対象としたアンケートデータなどが挙げられる。一方で企業成果・業績などの財務データは、企業レベルで集計されたデータだと言える。自身の研究課題や仮説がどのような集計レベルのデータに対応するものなのかを考え、研究内容と一貫したデータを用いることが必要になる。 マーケティングにおいて一次データかつ非集計データとして広く用いられれているのが、質問紙調査である。質問紙調査に関する詳細は ?? にて説明するが、マーケティングにおける調査や研究では行動データ等では捉えきれない消費者の心理的特性や状態（パーソナリティ、態度や知覚など）について知ることを目的に使用されてきた。なお、質問「紙」調査と呼ばれるものの、ウェブサイト画面上で回答を行う調査についても便宜的に質問紙調査と呼ぶことにする。 オンラインでの通販サイト（例、Amazon）やプラットフォームサービス（例、Google）が大規模化しデータ収集・分析能力が向上したことによって、現代のマーケティングでは（購買、クリック、検索などの）行動を捉えたビッグデータ分析が盛んになっている。一方でGoogle社が運営するYoutubeでの動画視聴中に質問紙調査への回答を求められた経験のある方も多いのではないだろうか。現在も質問紙調査が用いられる理由について理解するために、本節ではまずマーケティング施策とその成果のプロセスについて、以下の図のように簡易的に示す。 マーケティングと成果との関係、その過程 企業は自社の戦略・目標に基づいて具体的なマーケティング・ミックスを策定し顧客へ価値を提供する。客観的データとしてはその提供物（品質・価格、広告など）に対する反応（購買、買い物かごへの選択など）として観察できる。しかしながら、どのように/なぜその行動に至ったのかという消費者の内的な心理的プロセスについては観察できない。たとえばYoutubeにおいて提示される広告動画をどのアカウントが何秒視聴し、その後リンクをクリックしたか否かは行動として確認できる。しかし、その広告によってどの程度の視聴者がその企業を認知または記憶したのか、もしくは好意的な態度を形成したのかという心理的過程における行動への先行要因については観察できない。その意味で質問紙調査は上図内顧客の過程における点線部分に関する調査を得意とするのだと考えられる。 一方で、「企業による施策が消費者の行動的反応や企業成果へ与える直接的な効果を判別したい」というような目的においては質問紙調査よりも好ましい手法があるかもしれない。自身の着目する研究課題に適したリサーチ方法を採用する必要がある。なお、観察可能な行動に基づく調査・分析方法も点線で示されているプロセスを無視しているわけではない。詳細についてはコラム 14 や、消費者選択モデル（9 節）を参照してほしい。 "],["sampling.html", "4.3 調査対象者の確保", " 4.3 調査対象者の確保 調査において捉える情報が特定化されたら、研究者は次に実際に行う調査の概要を特定化する。いつ、誰に対して、どのように調査を行うのかを特定化する必要があるが、その際には以下の6つのWs への質問へ答えることで調査概要が明確になる（Malhotra, 2010）。 Who 調査対象者はだれか？ 例、特定の店舗から最低でも月一回買い物をしている顧客 What 回答者からどんな情報を得るべきか? 例、特定の仮説を検証するための変数（尺度） When いつ情報を得るべきか？ 例、買い物経験を評価するために時間を与えるために、来店後の任意のタイミングで回答してもらう。 Where どこで回答者と接触するべきか？ 例、（自宅や移動中などの）回答者の都合の良い場所 Why なぜ回答者から情報を得るのか？ 例、小売マーケティング・ミックスの修正や調整 Way どのような方法で回答者から情報を得るのか? 例、携帯電話やパソコンによるオンライン調査 調査概要を決めるうえで特に重要になるのが、調査対象者の特定である。調査対象者は研究課題との整合性を保つ形で検討される必要があることに加え、誰が対象となるかによってその後決断する質問項目や言葉遣いにも影響を与える。どのような人を対象に調査を行うべきかは、その研究が捉えている研究課題に依存する。しかし調査概要を決めるうえで大切なことは、調査対象者と整合的な条件（調査タイミングや方法）を採用することである。例えば、東京で働く会社員や公務員といった社会人が対象となっている場合、その人達にとっては通勤や帰宅時間帯に電車内で携帯電話からアンケートに回答することが容易かもしれない。それでは、調査対象とする人々の属性が確定したとして、どれぐらいの調査対象者を集めるべきなのだろうか。本書では母集団と標本（サンプル）という考えから、回答者数を特定化するうえでの基本的な考え方を提示する。なお、母集団とサンプルに関する統計学的な議論については 6 章を参照してほしい。 4.3.1 サンプリング手法 母集団とサンプルの関係を理解するために、まず母集団を定め、次に、サンプルについて決定するというプロセスを考える。母集団は、研究者が求める情報を持つ要素や物の集合体と考えられ、研究課題に応じて研究者によって決定される。ここでは、研究者が求める情報を有しているのはどのような人たちかという母集団の要素（日本の一般消費者か、東京都内の国立大学の学部生か、等）や、適切な母集団の単位（個人、家計など）はなにかについて定義する必要がある。 母集団という集合体全体を捉えることは通常困難であるため、研究者は母集団に対応するアクセス可能な標本（サンプル）の情報を得る。サンプルに対するデータ収集プロセスでは、サンプリングフレーム、サンプリングテクニック、サンプルサイズを決定する必要がある。サンプリングフレームとは、対象となる母集団の要素を表現したものであり、対象となる母集団を特定するためのリストによって構成される。例えば、電話帳や調査会社から購入した個人や組織のリストがサンプリングフレームの例である。次に、サンプリングテクニックは、サンプルフレームから特定のサンプルをピックアップする方法である。 サンプリングテクニックは非確率的サンプリングと確率的サンプリングに大別できる。確率的サンプリングの代表例はランダムサンプルである。ランダムサンプルでは、サンプリングフレームからランダムな手順でサンプルを抽出する方法であり、すべての回答者は他の回答者とは独立して選択される。これにより、サンプルは、互いに独立で同一の確率分布に従うと考えられる。また、別の確率的サンプリング手法としてシステマティックサンプリングと呼ばれる手法も存在する。これは、サンプリングフレームからの開始点をランダムで決定し、そこから任意の i 番目の要素がピックアップされるという方法である。これにより、サンプリングフレームから誰が調査対象として抽出されるかは確率的に決まると考えられる。 しかし、近年ランダムサンプリングを採用することが困難になっている。その理由としては、プライバシー保護の観点から調査協力を拒否する傾向が強まっていることや、法改正に伴い住民基本台帳や選挙人名簿といった以前有力であったサンプリングフレームの閲覧が難しくなったことが挙げられる（鈴木, 2014）。そのため、近年では非確率的サンプリングが現実的な手段として活用される。 非確率的サンプリングは、サンプルをランダムな選択により抽出しない方法である。ここでは代表的なサンプリング手法として、コンビニエンス法、有意抽出法（judgemental sampling）、スノーボール法、割り付け法（Quota sampling）を紹介する。コンビニエンスサンプリングは、研究者にとって便利な要素のサンプルを集める方法であり、適切なタイミングで適切な場所にいたという理由で回答者が選ばれる。たとえば、学生やある組織の構成員を使った調査や、ショッピングモールでのインターセプトインタビューはその典型的な例である。この方法は、質問への反応に偏りが生じるリスクが大きいと考えられるが、質問票や質問項目についての可読性や反応傾向を確認するためのプレテストには有力な手法となりうる。 スノーボールサンプリングは、最初の回答者をランダムで選んだ後、その後の回答者は、最初に選ばれた人による紹介や情報提供によって選ぶ方法である。これらの方法は最もお金も時間もかからないという利点を持つが、やはり質問への反応に偏りが生じうる点に注意することが必要である。 有意抽出法は、研究者が何らかの基準に基づき、特定の属性を持っている対象者を母集団を代表するサンプルであると判断し選んで抽出する方法である（鈴木, 2014）。研究者の主観的判断によって抽出されるため、標本の代表性に問題が残るが、少数の調査を行う場合には、属性を注意深く選ぶことで好ましい（信頼性や代表性が高い）サンプルを抽出できるという意見もある。 割り付け法は、着目する属性について、母集団の構成比率に等しく（近く）なるようにサンプルを抽出する方法である（鈴木, 2014）。例えば、東京都の一般消費者を母集団とする場合、東京都人口の年齢-性別構成比と等しくなるようにサンプルの比率を割り付けて抽出することが考えられる。割り付け法はランダムサンプリングが使えない状況において代表性を確保することを目的に多く用いられる。しかしながら、回答者に関する全ての属性や特徴について捉えた形で構成比率を決定することは不可能であるため、完全に母集団を代表しているとは言えない点に注意が必要である。 4.3.2 サンプルサイズの決定 サンプルサイズは、抽出する標本の数を表す。サンプルサイズは多いほうが統計的には望ましい（照井・佐藤, 2022）。しかしながら実践的にはリサーチコストや母集団の特徴にも配慮しながらサンプルサイズを決定することになる。コスト面に関しては、調査における予算制約とサンプルサイズを増やすことによる便益とを比較しながらできるだけリサーチの価値を高めるように判断することになる。母集団の特徴としては、例えば母集団内での異質性あ高い場合には、網羅的に情報を集めるためにサンプルサイズを増やすことが好ましい。一方で異質性が小さい場合にはサンプルサイズをは少なくて済むかもしれない。 また、研究者が採用する分析手法によっても必要なサンプルサイズは代わる。例えば、統計的検定が主眼ではない分析（例えば因子分析）を行う場合に求められるサンプルサイズの相場が300件程度であるため、経験的に300以上のサンプルサイズを確保することが期待される（小塩, 2024）。 一方で、統計的検定を伴う分析を採用する場合にもサンプルサイズが多いほうが統計的な誤差が小さくなり好ましいものの、その多さが実務的含意の弊害になりうるという議論も存在する。例えば、2つの異なるグループ間でのある変数の平均値に違いがあるかを検証したい状況を考える。この時、サンプルサイズが著しく多いと、実務的にはあまり意味を持たない極めて小さな差であっても、「統計的に有意な差」として判断される事がある。このような弊害を避けるために、検定力分析と呼ばれる分析枠組みからサンプルサイズを計算する方法が用いられる（Cohen, 1988）。ここでは、実務的な判断に耐えうる違いの値（効果量）や、検定力や有意水準といった統計的な値を定めることで、その条件に対応するサンプルサイズを求めることができる。 しかしながら、この手法の理解には基礎的な統計学知識を要するため、 6.11 章で改めて紹介する。 "],["尺度の種類と質問項目.html", "4.4 尺度の種類と質問項目", " 4.4 尺度の種類と質問項目 マーケティングリサーチにおける分析では、変数間の関係について分析、記述することが多い。仮説を提示する際の注意点においても、分析に用いる変数と整合的であることが重要であると述べた。そこで重要なのは、自身が用いる変数がどのような特徴を理解することである。ここではまず、尺度に基づくデータの分類を紹介する。マーケティング領域では様々な事柄が測定の対象となるため、それに対応した様々な尺度が利用される。主な尺度のタイプとその特徴は、以下の表のようにまとめられる。 Table 4.1: 尺度表 尺度 特徴 例 名義 対象の識別と分類 性別・職種 順序 対象の相対的ポジション 好み順位・ランキング 間隔 対象間の大小関係比較（原点は定まっていない） 態度・指数・気温 比率 連続的関係（原点が定まっており、比率計算も可能） 所得・売上 上表における名義尺度と順序尺度は主に質的尺度としてとらえられ、間隔尺度と比率尺度は一般的に量的尺度と分類される。マーケティング研究における「比率尺度」の最も典型的な例は売上高である。この尺度は大小にも間隔にも意味があり、かつ比率にも意味があるため、四則演算に対応する尺度である。一方で「間隔尺度」は正の整数で表される尺度であり、その値の大小関係と間隔にも意味があるものの、原点が定まっておらず、比率計算に耐えない尺度である。マーケティング研究においては、アンケート調査における質問項目や、質問項目の合計値（およびそれを項目数で割ったもの）である合成変数が間隔尺度の典型的な例である。 一方でマーケティング研究においては、必ずしも量的ではない情報に着目して分析を行うことも多い。そのような場合には、質的尺度を用いて観察対象のカテゴリを分類することで分析可能にする。例えば、消費者の購買行動に関する東京、大阪、北海道という地域（都道府）間の差を分析する場合を考える。このとき、「地域の違い」に数量的な違いは存在しないものの、地域の違いを表すために東京 = 1, 大阪 = 2, 北海道 = 3 のような地域コードを用いる事が多い。しかしながら、この変数がとる数値そのものに本来的な意味はなく、東京が大阪と北海道よりも低い値を取っているという解釈は適切でない。この変数はあくまで異なる地域に分類されることを示しているのみである。このような属性の分類や有無を表すための尺度を「名義尺度」と呼ぶ。また、観測対象が特定の属性に対応する場合（例、男性）には 1 を、そうでない場合には 0 を取るような、1 と 0 で分類された名義尺度のことを特に「ダミー変数」と呼ぶ。ダミー変数は分析結果の解釈が容易になる利点もあるが、詳しくは後述する。質的尺度のもうひとつの例が「順序尺度」である。順序尺度は、観察対象の序列や大小関係を表す尺度であるが、その数値の間隔に意味はない。例えば 1 が最低であり、4 が最高となるような金融商品の等級において、商品 A は ランク 4、商品 B はランク 1 だとする。このとき、 B は A よりも高い評価を受けているということは言えるが、A は B の 4倍優れているという議論は不適切である。このように、各対象間の推移性を表現するときに用いるのが、順序尺度である。 4.4.1 心理尺度 4.2 節において、質問紙調査は回答者の心理的過程を捉えることに向いていると述べた。この背景には、質問紙調査を通じて回答者の心理的特性や状態を知るための尺度が開発、利用されてきたことが貢献している。尺度とはある事柄を数値的に測定するための道具と考えられる。そして心理尺度とは、心理的な事柄を連続的な一連の数値として測定する道具である（小塩, 2024）。また多くの場合、心理尺度は質問項目と解答欄の組として作成される。 心理尺度は心理学分野にて開発、発展が進んでいるものであり、マーケティングや消費者行動領域でも広く応用されている。心理学においては直接的かつ量的に測定できない概念に焦点を合わせることが多く、パーソナリティ特性や感情、態度などに対し心理尺度を用いて測定を試みる。 概念を測定するためには主に「概念定義」と「操作定義」という二種類の定義を行うことが必要になる。概念定義とは、捉える概念の内容について言語的に定義することであり、操作定義とは概念について操作と呼ばれる概念を知るための方法から定義することである。例えば、企業への態度という概念を捉えるために心理尺度（質問項目と解答欄の組）は、態度に対する操作定義と理解できる。そして、操作定義についてはその信頼性と妥当性が検討される必要がある（小塩, 2024）。 心理学的アプローチに基づくマーケティング研究では、心理尺度を用いて調査、分析されることが多い。そのため、質問紙調査で採用される質問項目においても、心理尺度利用した質問項目を作成もしくは採用できると、調査や分析に関して蓄積されてきた知見を応用することが可能になる。 例えば、消費者のセグメントを把握したい場合においても、（潜在的に）連続的な構造が想定されている心理尺度によって消費者の心理的特性や状態を測定したあとに、その値に基づき類型化を行うことが好ましい（小塩, 2024）。類型化の方法としては、（1）ある心理尺度単体の観測値に対して閾値（例えば中央値）を基準に高水準グループと低水準グループに分ける方法、（2）（1）の方法を複数（例えば2つ）の心理尺度に対して適応する方法、（3）クラスター分析（11節参照）を用いる方法が挙げられる。本節では心理尺度に関する議論も踏まえ、質問項目の種類について説明する。 4.4.2 質問項目と解答欄 質問紙調査では、主に自由回答式質問、選択肢型質問、尺度型質問、といった方式の質問が用いられる。自由回答式質問は、回答者が自身の言葉で自由に回答できる質問であり、回答に関する事前の選択肢は設定されていないタイプの質問である。回答者は、このような質問に対して一言で簡潔に回答するか、詳細に長く回答するかなど、回答方法に裁量を持つことになる。例えば、「出身はどちらですか？」という問いに対して回答者は、「東京都」と都道府県レベルで答えることも、「東京都町田市」と市区町村レベルで答えることも可能である。また、典型的な自由回答式質問の例として、理由や動機を問う設問がある。例えば「なぜ、この航空会社を選びましたか？」といった質問に対して回答者は「機内食」のように一言で回答することも、長い文章を用いて回答することも可能である。選択肢型質問は、回答者に質問に対する回答のための2つ以上の選択肢を与え、その選択肢から択一させるものである。例えば、回答者の特定のサービスの利用経験について「1. はい、2. いいえ」からいずれかを選ばせたり、学歴について複数の選択肢から一つを選ばせるというものがこのタイプの質問に該当する。 一方で尺度型質問は、消費者による企業への態度や満足などを捉えるためにマーケティング分野で広く用いられている質問方式である。ここでは、ある質問に対する点数を回答者に選ばせることで、その質問で捉えようとしている性向の程度を測る。尺度は質問項目と解答欄の組として捉えられ様々な形成方法がが存在するが、ここではマーケティング領域で広く用いられている項目別評価尺度について説明する。項目別評価尺度においてよく用いられる尺度はリッカート尺度と意味微分法（Semantic differential: SD）法と呼ばれる尺度である。リッカート尺度では、調査者は回答者に文章を提示し、回答者がその文章の内容にどの程度同意するかについて情報を得る。解答欄では、「全くそう思わない,…,とてもそう思う」や「全くあてはまらない,…,とても良くあてはまる」という選択肢について5点尺度や7点尺度で回答させることが多い。以下は、7点のリッカート尺度による質問の例である。 リッカート尺度例 一方でSD法は、両極の意味を持つ一連の尺度を用いて、回答者の評価を得る方法である。例えば、「冷たい-温かい」や、「弱い-強い」というような対極にある言葉を両極に設定して、5点や7点尺度によって回答を得る方法が一般的である。以下は、7点のSD法による質問の例である。 SD尺度例 尺度型質問の中でも、リッカート尺度は特によく用いられる形式である。この理由として、リッカート尺度によって得たデータに関しては、ある条件を満たせば間隔尺度としての活用が許容されることが挙げられる。尺度型質問によって得たデータは順序尺度として理解することが自然かもしれない。しかしながら心理尺度として作成された質問への回答データは量的データ（間隔尺度）として扱われ分析されることが多い。この背景として、測定されたデータは離散的だが、背後に想定されている概念は連続的なものであるという想定が影響している。リッカート尺度によって得られるデータ自体は順序尺度だが、尺度への反応（回答）度数が正規分布（中心が最も観測値が多く左右対称なベル型の分布）とみなせる場合、シグマ法という方法を用いた際に、間隔尺度としてみなせることが知られている。シグマ法は回答への相対度数や標準正規分布の確率密度を用いた数値を求め計算方法である（小塩, 2024）が、詳細については省略するため興味のある方は他の著書を参照してほしい。 また、リッカート尺度の項目数が少ない場合を除いて選択肢の数値をそのまま利用する簡便法と呼ばれる方法を用いてもシグマ法と同様の結果を得ることが知られている（小塩, 2024）。そのため、リッカート尺度によって得たデータが心理尺度として背後に存在する連続的な概念を捉えており、質問への反応が正規分布のような形状をしている場合には、間隔尺度としてみなして結果を利用する事が経験的に許容されている。そのため、質問項目への理論、概念的理解や、データに対する基礎統計量や図表を確認することが重要になる。リッカート尺度を間隔尺度としてみなすことが適切ではないと判断する場合には、順序尺度として扱うことや、中央値に基づく類型などによりダミー変数を作成するなど、質的変数として捉え直し分析に活用することが好ましい。 4.4.3 マーケティングと心理尺度 マーケティングにおける研究では、アンケートを通じて、顧客満足や企業態度などの抽象度の高い概念を捉えようと試みることも多い（南・小野, 2010）。このような方法は4.4.1 節で紹介した心理尺度の作成や引用を行うことで達成される。捉えたい概念の範囲が広く、多様な側面を含む場合、複数の質問項目と解答欄の組を用いることも多い。 例えば、2010年代以降よくマーケティング領域で用いられる概念に、消費者によるブランドエンゲージメントがある。ブランドエンゲージメントは「消費者が特定のブランドをどの程度自分自身の一部のように大事に捉えているか」を表す概念と定義される (Sprott et al., 2009, p. 92)。このような抽象的な概念は多面的な視点から包括的に捉えるが多く、例えば Sprott et al. (2009) では、以下の8つの質問項目とリッカート尺度による解答欄が、ブランドエンゲージメントの操作定義とした： I have a special bond with this brand. 私はこのブランドと特別なつながりを持っている。 I consider this brand to be a part of myself. 私はこのブランドを自分の一部として考える。 I often feel a personal connection between this brand and myself. 私はこのブランドと自分自身の間に個人的な関係を感じる。 Part of me is defined by this brand in my life. 私の人生の一部はこのブランドによって規定されている。 I feel as if I have a close personal connection with this brand I most prefer. 私が最も好きなこのブランドとは、まるで個人的な関係を持っているかのように感じる。 I can identify with this brand in my life. 私の人生において、このブランドに共感できる。 There are links between this brand and how I view myself. このブランドと私自身との間には繋がりがある。 This brand is an important indication of who I am. このブランドは、私がどういう人間かを示す重要なものである。 この例において研究者は、抽象的かつ多面的な概念を複数の質問項目を使って様々な側面から捉えようとしていることが伺える。しかしながら複数の質問項目を用いた操作定義を採用する場合、尺度の操作定義が適切か否かをチェックすることが特に重要になる。適切性は主に、尺度の妥当性と信頼性の観点から評価される。 4.4.4 信頼性と妥当性 信頼性は、採用した尺度が持つ一貫性や安定性の程度を捉えている（小塩, 2024）。言い換えると、その尺度が同じ結果を繰り返し提示することができるかを表す。尺度の信頼性の低い場合、偶然生じる誤差により測定のたびに異なる得点を返し、一貫性が低いといえる（Malhotra, 2010）。 信頼性を確認する代表的な手法として、再検査信頼性（test-retest reliability）と内的一貫性信頼性（internal consistency reliability）を紹介する。 再検査信頼性 調査のタイミングによって尺度の得点に誤差が生じるかを捉える。 着目する尺度への回答を2時点（2週間か4週間空けることが多い）で取得し、異なる時点で得た得点間の相関係数を分析する。 内的一貫性信頼性 尺度が着目する概念をきちんと測定しているか、概念と関係の弱い尺度が採用されていないかを捉える。 アルファ係数（クロンバックのアルファ）と呼ばれる指標を用いる。 詳細については省略するが、アルファ係数は尺度得点の分散のうち偶然生じる誤差に左右されない分散の比率を表している（小塩, 2024）。 0から1までの値を取り、この値が大きいほど概念に対して関連の強い項目が利用されていると解釈できる。 信頼性を評価するための数値的基準は慣習や経験に頼る必要があり、0.6を下回る場合には信頼性が不十分だと解釈されることが多い（Malhotra, 2010）。 妥当性とは採用した尺度が着目する概念を測定できているかを表す。採用した尺度がどの程度妥当であるかは主に経験的証拠と理論的説得に基づき判断される。妥当性の検討に用いる経験的証拠については様々な側面が存在している（小塩, 2024）。本書では、Malhotra（2010）を参考に内容妥当性（content validity）、基準関連妥当性（criterion related validity）、構成概念妥当性（construct）について以下のようにまとめ、紹介する。 内容妥当性 扱っている尺度の内容が着目する概念の範囲に含まれているか、そして概念を網羅的に偏りなく捉えているかを捉えている。 より精緻な妥当性の検討には、基準関連妥当性が用いられる。 基準関連妥当性 作成された尺度の得点と外部指標（何らかの基準や、別の有意義な変数）との関連のもとで、期待通りの結果につながるかを捉えている。 例として、顧客満足を用いてこの妥当性の考え方を紹介する。顧客満足は「満足とは、消費者が感じる充足感への反応である。これは、製品やサービスの特徴、または製品やサービスそのものが、消費に関連した充足感を心地よいレベルで提供した（もしくは提供している）かどうかを、充足感の不足や過剰も含む形で判断するものである。」と定義される（Oliver, 1997, p.13）。また、顧客満足度を測る理論的議論では、顧客が経験した製品やサービスに対する事前の期待と事後の成果（品質の知覚など）との差によって満足度が変化すると考えられている（南・小川, 2010）。そのため、採用した顧客満足尺度と、事前の期待や事後的な成果を表す変数との関係を分析することで、外部指標との関連から採用した尺度が妥当か否かを判断することができる。 構成概念妥当性 尺度が実際にどのような概念を捉えているかを評価する。 捉えている概念に関する理論に基づき、なぜ採用している尺度が機能するのか、そして他の概念とどのような関係性を推測できるのかについて論じることが求められる。 また妥当性の検証として、捉えた心理学的概念の理論的特徴や仮定と得た心理尺度得点との関連を経験的に確認する方法が取られている。その代表的なものが収束妥当性（convergent validity）と弁別妥当性（discriminant validity）である（Malhotra, 2010）。 収束妥当性 尺度が同じ概念を捉えている別の尺度とどの程度関連が強い（相関が強い）かを捉えている。 弁別妥当性 尺度が他の概念を捉えた尺度とどの程度関連が弱い（相関が弱い）かを捉えている。 それらの項目が共通の概念に寄与しているか（十分に似ているか）を表すものである。妥当性チェックにおいては、探索的因子分析や確認的因子分析が用いられる。 採用した心理尺度の構造を統計的に理解するためにマーケティング領域では、探索的因子分析と確認的因子分析と呼ばれる方法が広く用いられる。探索的因子分析については、12 章 で扱う。一方で、確認的因子分析は通常共分散構造分析の一種として扱われることが多い手法なので本書では扱わない。 また、ある概念を複数の尺度により捉える場合、その概念全体を捉えた得点を用いて分析をしたいと考えるだろう。そのような場合、概念を構成する項目全体を捉えた得点を用いてその後の統計的な分析を行ったり、共分散構造分析という手法を用いることがある。共分散構造分析については他の書籍（例えば、豊田, 2012）を参照してほしい。項目全体の得点を求める場合、その概念を構成する尺度得点の合計値や算術平均などを用いてひとつ合成変数を作成する場合や、因子分析を実施することで求めることができる因子得点（12章）を用いることが多い。 本節では、質問紙調査において活用する質問項目について、その種類と心理尺度との関係から採用する尺度の信頼性と妥当性を検討する必要性について説明した。次節では、より実践的な質問文の構成について説明する。 "],["実践的な注意点.html", "4.5 実践的な注意点", " 4.5 実践的な注意点 4.5.1 質問におけるワーディング 本節では、質問紙調査作成におけるより実践的な注意点を説明する。具体的には、ここでは質問紙調査内で使用する言葉やフレーズについての注意点を説明する。これは、データ収集前に吟味し、修正する必要がある。研究者が質問紙調査内で用いる言葉や表現の複雑さは、調査のトピックや対象となる回答者特性に合わせて調整すべきである。 また、質問を提示する前に、どのように質問に回答すべきかを指示する教示文を提示することも、調査対象者の回答コストを削減するための有力な方法である。 質問文の不明瞭さは、回答者の混乱や調査におけるコミュニケーションノイズにつながる。研究者がよく犯す質問紙調査における言い回し（ワーディング）誤りとして、以下のものが挙げられる。 曖昧な質問 ダブルバレル質問 誘導的な表現 曖昧な質問は、質問文の解釈が一意に定まらないような文章による質問を指す。このような曖昧かつ多義的な質問をしてしまうと、回答者と調査者が質問に対して異なる意味を見出してしまうことで、不適切かつ予想外の回答を得る可能性が高まる。例えば、「あなたはいつケーキを買いますか？」という質問を考える。この質問で問われている「いつ」が曖昧であるといえる。回答者はこの質問に対して、一年間のうち特定の月やタイミングを答えるべきなのか、次にいつ（例えば、2週間後等）ケーキを買うと答えるべきなのかが曖昧である。この場合、仮に研究者の意図が後者であったとしても、適切な回答を得ることができないかもしれない。 ダブルバレル質問は一つの質問の中に2つの論点が含まれているような質問を指す。このような質問の場合、回答の含意が一意に定まらず、回答に対する適切な解釈が提示できない。例えば、ホテルにおいてリピート客に対してサービスを評価してもらうようなアンケートを考える。そこで、ホテルの調査担当者以下のような質問と回答選択肢を 「当ホテルの食事や接客サービスにおいて、以前利用した際と比べて何か改善は見られましたか？」 「(1) はい (2) いいえ (3) わからない」 この質問の問題点を明確にするために、ある回答者が「 (2) いいえ」と回答した場合を考える。この回答の含意には、以下の3つの可能性を見出すことができる。 食事とサービスどちらにも改善がない。 食事に改善があってもサービスにはない。 サービスに改善があっても食事にはない。 しかしながら、(2) いいえ、という回答が上記のどの理由によって提示されたのかは識別できない。また、このような質問は、回答者を混乱されることにもつながるため、回答にかかる精神的労力を高めるという点からも好ましくない。この場合、「食事に関する改善」と「接客サービスに関する改善」とを別々の質問として問うことが好ましい。 誘導的な質問は、回答者を特定の答えに誘導する傾向のある質問を指す。例えば、以下のような消費者による小売業態への評価や利用に関するアンケートを実施することを考える。 「ドン・キホーテのような安価なディスカウントストアをどの程度利用しますか？」 上記の質問は、質問文による誘導のリスクを含む。「安価」や「ディスカウント」という言葉を強調しており、回答者が頻繁に行くと答えにくいと感じてしまう可能性がある。また、特定の企業を想起させることで、特定の企業に対する評価を誘導してしまう可能性がある点にも注意が必要である。 誘導的な質問は質問文と回答選択肢の組み合わせによっても生じる可能性があるため注意が必要である。例えば、ある政策に対する評価を確認するため、以下のような質問と回答選択肢を考える。 あなたの〇〇政策を支持していますか？ 「(1) はい (2) いいえ (3) わからない」 この質問は、選択肢による誘導のリスクを含む。その問題点を理解するために、まず「(2) いいえ」という回答を得た場合を考える。「いいえ」という回答では、その回答者がこの政策に対して積極的に不支持なのか、それとも積極的に支持しているわけではないのかがわからない。つまり、「(1) はい」には、積極している支持している層しか観察されず、いいえに回答が集まりやすい設計になっていることがうかがえる。このような場合、例えば「賛成-反対」を両極とするSD法によって回答を得ることで、回答者が当該政策に対してどのような立場に立っているのかがわかりやすくなる。 暗黙の前提を含む質問は、研究者と回答者とで異なることを想定し、適切な回答を得ることができない可能性を高める。例えば、研究者が一般消費者の固定電話の利用頻度を知りたいと考えている状況を仮定し、そのために「あなたの電話の利用頻度について教えてください」と質問したとする。この質問における「電話」とは何を想定しているのだろうか。質問において聞かれているのは自宅等に置かれている固定電話、もしくは携帯電話の利用頻度なのかが不明確である。これは、「曖昧な質問」にも通じる問題も含まれるが、研究者が電話という言葉に対して暗黙的に固定電話を仮定していることが原因で生じた問題だと理解できる。このような問題を避けるために、例えば質問の前に固定電話についての説明やフィルター質問（事前質問）を提示することで、回答者が同じ対象（固定電話）を想定できるように調査プロセスを設計することも有効である。 本書は、実際にアンケートを実施する際にはまず先行研究を調べ参照することを強く勧める。マーケティングに関する多くの尺度はすでに対応する質問文が開発されている。それらのほとんどは信頼性や妥当性の分析をクリアした質問内容なので、それらを引用するのが一番確実である。平たく言えば、とにかくまずは先行研究を探すべきだと言える。先行研究については主に、国際的な査読誌（海外ジャーナル）と、国内の文献とに分けることができる。海外ジャーナルに掲載された論文は、Google scholarや図書館システムから検索しアクセスすることが可能である。こちらの方がそもそもの論文量も多く、また、競争率および査読の水準の高いジャーナルの審査を乗り越えたという点から質の高い論文も多い。ただし、これらで参照できる論文及び尺度は英語で書かれているため、日本語で尺度を引用する場合には、日本語への翻訳と、バックトランスレーションによる日本語訳の適切性チェックが必要になる。一方で国内文献はCiniiや図書館システムから検索及びアクセスすることが可能である。こちらの方が相対的に量は少ないが、すでに適切な翻訳プロセスや、信頼性・妥当性チェックを経た尺度を提示しているものもあり、そのような尺度の場合、追加的努力を節約する形で既存の尺度を引用できる。 4.5.2 質問票の構成 前節では個別の質問項目設計に関する注意点を紹介したが、ここでは複数の質問やアンケート全体の構成についての説明を行う。質問紙調査で用いる各質問項目を確定したら、それをどのような順番で構成するのかについて考えることが必要になる。質問の掲載順においては、できるだけ回答者の回答への心理的負担を下げるような工夫を考慮する必要がある。そのため、基本的には答えやすく単純かつ興味を引く質問を最初に尋ね、広範な質問から特定的な質問へ移行し、難しい質問やセンシティブないしプライベートな情報を問う質問は最後に聞く、というような工夫が求められる。特に、質問紙調査の最初の質問には興味深く簡潔で威圧的でないものを選ぶと良い。例えば、回答者の素朴な意見や感想を聞く質問が有効であり、仮に調査には不必要であってもこの手の質問を最初に聞くことが有効になる場合もある。また、特定のトピックや製品・ブランドに関する項目はまとめて質問したり、時系列に関する質問は時系列順に問うなどの論理的な順番を守る構成も回答者の認知・心理的負担を減らすことに貢献すると考えられる。 質問紙の構成の他にも、レイアウトや回答の回収方法についても決定する必要がある。質問紙全体のフォーマット、文字スペースや質問文の配置などの装丁は、回答者にとっての可読性および回答率に影響を与えると考えられる。基本的にはシンプルかつ読みやすいレイアウトを心がけてデザインすることが必要になる。また、アンケート自体をいくつかのパートに分けつつ、各質問に番号を割り振るなどの工夫をすることで、回答者がアンケート回答の進捗を把握でき、途中離脱を防ぐことができるかもしれない。 次に回収方法は、大きく分けてオフラインとオンラインによる回収に区別することができる。オフラインでの回答回収には、対面質問（家庭訪問や商業施設での接触設問）、電話、郵送といった方法が存在する。対面法では、回答者と調査員とのやり取りが可能になるため、回答回収率が高かったり、質問に関する理解を促すような即時的なコミュニケーションが可能になるという利点もあるが、一回答あたりの費用が高いことや調査員の介在によるバイアスといった欠点もある。電話による調査は広範囲への調査と比較的高い回収率につながる方法として利用されたが、近年では用いられなくなっている。郵送法は、回答者に印刷された質問票と返信用封筒（切手添付または料金受取人払手続き済み）を送付し、回答後に調査者の住所に回答結果を返送してもらうという方法である。この方法であれば、回答者が好きなタイミングで回答でき、かつ広範囲への調査が可能になる。しかしこの方法には、回答状況を統制できない、回収率が相対的に低いといった欠点も存在する。 一方でオンラインでの回答回収は、eメールによってアンケートを送付する方法と、回答者にアンケートサイトにアクセスしてもらい、回答させる方法とに大別できるが、近年ではアンケートサイトにアクセスを促し回答を回収する方法が主流である。オンラインでの調査の場合、回答者はインターネット環境が整っていれば、いつでもどこでも回答することができる。また、調査者はページの見出し、セクション紹介と進捗バーを組み合わせることで回答者が回答をやめないように工夫する事ができる。加えて、調査者はオンラインであれば、画像、音声、映像やアニメーションなどの要素を含めることが可能である。一方で、調査者はアンケートに関わるプライバシーポリシーについて回答者に説明することが必要になる。ただし、オンラインでのアンケートでは、オンライン調査に参加するようなタイプの回答者からの回答しか収集できないという、回答者の傾向についてのバイアスについても理解する必要がある。 4.5.3 調査回答の質向上にむけた工夫 オンライン調査では、調査への参加者に偏りが生じるという問題に加えて、回答者による努力の最小限化に伴う不良回答が問題視されている（三浦・小林, 2018）。不良回答として例えば、教示文や質問文をきちんと読んでいない回答や、全質問で同じ回答選択肢を選ぶ（例えば全て4を選ぶ）ような回答（ストレートライニング）が挙げられる（鈴木, 2016）。また、極端な場合、ボットと呼ばれる自動タスク処理プログラムを用いて自動で各質問の選択肢を選んでいく場合もある。 このような問題への対応として、instructional manipulation check（IMC）や directed question scale（DQS）を用いて努力の最小限化を行っている可能性の高い回答を検出する方法が用いられている（三浦・小林, 2018; 増田ほか, 2019）。IMCは教示文をきちんと呼んでいないと回答できないような設問であり。例えば、増田ほか（2019, p.472）では、IMCとして以下のような質問と選択肢の組を採用している。 「インターネットを用いた調査においては，うそをついたり，質問を読まないで，いい加減な回答をしたりする方がいることが問題となっています。つきましては大変失礼なお願いですが，あなたがこの文章をきちんと読んでいるかどうかを確認させてください。あなたがこの文章をお読みになったら，以下の質問には回答せずに（つまり，どの選択肢もクリックせずに），次のページに進んでください。（選択肢：1. そう思う, …,5.そう思わない ）」 他方でDQSは、尺度を精読努力の最小限化を検出するための方法で、リッカートスケールにおいて特定の選択肢を選ぶように指示する設問である（小塩, 2024）。具体的には、リッカートスケール形式の尺度内に「**この項目は「1. 全くあてはまらない」を選択してください」という項目を含めるといった方法が採用される（三浦・小林, 2018, p.4）。また、尺度への精読努力の検出や不良回答の軽減に対しては、逆転項目の採用も期待されている（小塩, 2024）。逆転項目は測定したい概念と反対方向（負の相関をもつような）の項目である。例えば、社交性を測定するための項目に「内気なところがある」という項目が含まれている（小塩, 2024）。逆転項目を含めることにより、回答者が尺度を吟味することにつながる可能性がある。また、もし他の正項目（概念と正の相関を持つと考えられる項目）について「あてはまる」と答えながら逆転項目にも「あてはまる」と答えている個人がいた場合、この回答は論理的に不自然であり不良回答の可能性がある回答者として検出が可能になる。 4.5.4 変数間の関係を捉えるための質問紙調査設計 ここでは、質問紙調査を通じたデータ収集とそれを用いた分析においてよく観察される失敗とその対策について説明する。ここでは、研究者が質問紙調査により得た情報に基づき、 ある二つの変数間の関係を捉える場合を考える。例えば、あなたが「顧客の知覚サービス品質と満足度との間には正の関係ある。」という理論に関心があったとする。ここで着目されている変数（概念）はサービス品質と満足度であり、財務データでは観察不可であるため、一般的に研究者はアンケートを通じた調査が用いることで情報を得る。この2変数間の関係を捉えるために研究者は質問紙を設計する必要があるのだが、「一つの質問でまとめてこの関係を捉えようとする」、という誤ったアプローチを採用することがよく観察される。例えば、初めて質問紙調査を実施する学生は「あなたは、品質が高いサービスを経験すると満足しますか？」のような質問項目を設定しがちである11。 このような質問項目は、この項目に対する回答を基にどのように二変数間の関係を検証するのかが明確ではない（検証できない）という点で問題がある。複数の変数間の関係を捉えたい場合、各変数ごとの質問を別々に作成し、それぞれの質問への回答データを用いて二変数間の関係を統計的に分析するというプロセスを経る。この点については、財務データに基づく二変数間関係の分析と対比させるとわかりやすい。例えば、研究者が小売企業の店舗数と売上高の関係に関心があるとする。このとき研究者は企業レベルの店舗数と売上高についてそれぞれ別の変数として情報を集め、これらの変数間の関係を回帰分析などの手法で分析しようと試みるだろう。おそらく、多くの人が、店舗数と売上高の関係を分析するために、これら両方の情報を内包した一つの変数に関する情報を収集しようとは考えないはずである。アンケートによる調査・分析においても原則としては同様であり、特定の変数を排他的に捉えるような質問項目を作成し、それらに対して得た回答を基に、分析を行っていく必要がある。 変数間の関係を捉えるための調査設計について、もう少し具体的な説明を例とともに提示する。ここではあなたが「具体的な小売店舗の属性（価格）とその店舗へのロイヤルティとの関係」に関心があると仮定する。このとき、先述の悪い質問に該当する質問例は「あなたは、価格の低いお店を利用しますか？」や「あなたは、価格の低いお店をどの程度利用しますか？」である。これらの質問は、価格とロイヤルティの関係を検証できるデザインになっていない。では、具体的な店舗の特徴とその店舗への評価や利用状況をアンケートによって捉えるためにはどのようなデザインが考えられるのだろうか。本書では、以下の2つの対応例を紹介する。 回答者が利用している店舗を特定し、その店舗について回答してもらう。 研究者が準備したシナリオや実験刺激としての店舗情報などを提示して、その店舗について回答してもらう。 1の方法の場合、アンケートにおいて回答者が想定する企業は回答者ごとに異質である。この方法では、回答者が頻繁に利用する店舗について特定化するような質問をしたあとに、その店舗についての評価や利用状況を尋ねるという階層的な質問構造を形成する。例えば、以下のような質問を構成することが考えられる。 「Q1 あなたは過去3か月間に次のどのスーパーの店舗に、最も良く食料品を買いに行きましたか。（回答は1つ）食料品を買うスーパーについて伺います。:リストを提示」 「Q2 あなたが食料品を買う際に最もよく利用する店舗【Q1スーパー名引用】でのお買い物の状況について、お答えください。週間の間にあなたがその店舗で食料品の買い物をする頻度をお答えください：1. １回未満, 2. 1回, 3. 2回,…」 「Q3 最もよく利用する店舗【Q1スーパー名引用】での食料品購入額が、ご家庭の食料品購入額全体の何％を占めているか、それぞれお答えください。: 選択肢を提示」 「Q4 最もよく利用する【Q1スーパー名引用】の店舗は従業員のサービスが手厚い：1. 全く当てはまらない,…, 7. とてもよくあてはまる」 「Q5 最もよく利用する【Q1スーパー名引用】は取扱製品の品質が高い：1. 全く当てはまらない,…, 7. とてもよくあてはまる」 一方で、2. のシナリオを提示する方法は、シナリオ実験アプローチと言われ、基本的な調査の構造は投薬の実験などと同様であり、ある刺激を受ける群と受けない群とでその後の結果に差があるかを捉える。このアプローチの場合、回答者が想定する店舗やその他の状況は特定化され、コントロールされている。この方法では、とある購買状況を想定してもらうための、全ての回答者に共通したシナリオを想定しつつ、検証したい施策のみ変化させた（施策あり vs. 施策なし）２種類のシナリオを準備する。そして研究者は、回答者を検証したい施策ありのシナリオを読むグループ（トリートメント群）と施策なしのシナリオを読むグループ（コントロール群）とにランダムでわけ、それぞれのグループ間で、回答者が異なる情報に直面するように調査を設計する。回答者はシナリオ読了後、成果変数に相当する質問に回答する。そして、成果変数に関するトリートメント群とコントロール群間での差を統計的に分析する、というアプローチを取る。ここでは、例として企業の活動に関するシナリオを読ませるという方法を紹介したが、この方法はシナリオに限らず、何かを実際に体験させたり、回答者にタスクを課すなど、様々な調査設計に応用する事が可能になる。 テキストにこのように書かれていると、「自分はこんなことをしない」と思うだろうが、実際にアンケート設計をさせるとこのような誤りを犯す学生は存外多いので注意されたい↩︎ "],["ethicality.html", "4.6 研究倫理とインフォームド・コンセント", " 4.6 研究倫理とインフォームド・コンセント 質問紙調査によるデータ収集は、社会科学における「人を対象にした研究」に該当するため、人を対象にした研究としての研究倫理ガイドラインを守る必要がある。調査協力者が質問紙に回答することで不利になったり、個人の幸福を損ねる結果にならないよう配慮することが、倫理的ガイドラインの基本的な原則である（鈴木, 2016）。例えば、社会科学領域におけるこの規則の例として、一橋大学における研究倫理規則は以下のリンクから確認できる（https://www.hit-u.ac.jp/academic/research_ethics/index.html）。ここでは、主に調査協力者の身体的・精神的負担を最小化し、彼らに不利益が生じないよう留意することが求められている。 倫理的ガイドラインの遵守をした調査設計を行ったうえで、データを収集する際には、研究対象者から同意（インフォームド・コンセント）を得る必要がある。インフォームド・コンセントにおいては、調査を行う主体、目的、調査により得た情報の管理方法、公開有無や方法、倫理的配慮の内容などについて十分な情報を提供し、この条件について回答者から許諾を得る。調査対象者に提供すべき主な情報以下の通りである（鈴木, 2016）。 調査者に関する情報 調査内容の概要 調査対象者として選ばれた理由 匿名性、プライバシー、個人情報、人権を保護するための方法 回答者の権利 調査協力への有無が利益あるいは不利益をもたらさないこと 調査回答に伴うリスクと便益 調査結果の報告機会の提示 調査結果の使用・公表方法 調査への助成 謝礼の有無 ただし、大規模な無記名アンケート調査など、研究者が、回答者から個人名を記載した形の同意書を得ることが困難な場合がある。その場合は慣習として、調査開始前に上記の点について説明し、調査回答へ進むことで同意を得たことと代替する事が多い。ただし、このような対応方法に関する適切性の議論も、時代の経過とともに変化することが考えられるため、研究倫理については研究者自身が継続的に学習し、適切な手法を採用していくことが求められる。 4.6.1 研究倫理と事前登録 調査対象者への配慮としての倫理に加え、仮説検証型研究を行う上での疑わしい研究実践（questionable research practices: QRPs）を避けることも必要である。QRPsには、p-hacking、Hypothesizing After the Results are Known（HARKing）や、チェリーピッキングが含まれる。マーケティングや消費者行動領域におけるQRPsについて、ここでは元木ほか（2021）参考に説明する。 P-hacking とは、p値に着目し統計的に有意な結果を得るために恣意的な分析を行うことや、そのような分析結果を報告することである12。具体的には、多くの条件や分析モデルを測定・分析したうえで研究者にとって都合の良い一部のみを報告することや、統計的に有意な結果がでるまでデータを追加したり調査を繰り返したりすること、特定のデータを除外して統計的に有意な結果を得ることが含まれる（元木ほか, 2021）。 HARKingとは、分析を実行し、その結果を踏まえて、あたかも事前にその結果を想定していたかのように仮説を立案し報告する行為である。このような方法は、3 章で確認した、科学哲学の考え方と反するものである。論理実証主義でも反証主義でも、理論的な予測や主張を科学的手順で実証もしくは反証できるかどうかが科学的であることの論点であった。このような科学観にもとづく研究を行っている（もしくはそのように論文上暗黙的にアピールしている）にもかかわらず、HARKingを行うことは不誠実な対応であると考えられる。結果を得た後の解釈を事前に想定した理論的議論や仮説導出かのように報告するのは不適切であると認識されている。 チェリーピッキングは、選択的報告とも呼ばれ、研究者にとって都合の良い結果のみを選んで報告することである（元木ほか, 2021）。例えば、同じテーマや仮説に基づく実験を何度も繰り返したうえで、統計的に有意な結果のみを報告し、有意でない結果については公表しないことがチェリーピッキングの代表的な行為であり、出版バイアスという問題にもつながる。 これらのQRPsは研究に対する誠実さという側面に加えて、研究の再現性の低下という実態的な問題にもつながっている（元木ほか, 2021）。このようなQRPsを防止するために、研究の仮説、調査方法、分析手続きなどを前もって明記しておく事前登録（pre-registration）の活用が盛んになっている。事前登録においては、第三者機関、例えば Open Science Framework にこれから行う予定の実験、調査、分析に関する情報を登録する手続きが必要になる。 ただし、事前登録が研究の自由度を損なう（少なくともそのような意図のもと運営されている）ものではない。仮に事前登録の内容とは異なる検討や手続きを取りたい場合には、事前登録の内容から変更したという事実とその理由を明示すべきである。また、仮説を支持しない結果を得たとしても査読付き論文へ刊行できる可能性はある。近年では、事前登録の内容に対して査読を行うシステム（査読付き事前登録）を採用する国際的な査読付き学術誌も存在しており、このシステムでは事後的に得る結果（統計的有意性や仮説の支持不支持）に関わらず論文が採択される（元木ほか, 2021）。 このような取り組みは近年マーケティング領域において議論が盛んになってきている内容である。そのため、仮説検証型の研究における事前登録との付き合い方については数年のうちに状況が変化する可能性もあり、今後検討を続けるべき課題と言える。 統計的有意についての詳細は6章を参照してほしい。↩︎ "],["練習問題-1.html", "4.7 練習問題", " 4.7 練習問題 以下の仮説に答えるための質問紙調査票（インフォームドコンセント、教示文、質問文、回答欄）を考えよう。ただし、必要に応じて先行研究を検索し参照すること。 （小売店舗における）品揃え製品に対する知覚品質は、その店舗への好意的な態度へつながる。 価格志向の高い消費者ほど、月あたり携帯電話利用料金が低い。 "],["参考文献-2.html", "4.8 参考文献", " 4.8 参考文献 池尾恭一・青木幸弘・南知惠子・井上哲浩（2010）「マーケティング」, 有斐閣. 小塩真司編（2024）「心理尺度構成の方法-基礎から実践まで」, 誠信書房. 鈴木敦子（2016）「質問紙デザインの技法[第2版]」,ナカニシヤ出版. 田村正紀 (2006) 「リサーチ・デザイン: 経営知識創造の基本技術」, 白桃書房. 南知惠子・小野孔輔 (2010) 『日本版顧客満足度指数（JCSI）のモデル開発とその理論的な基礎』, 「マーケティングジャーナル」,30(1), 4-19. 元木康介・米満文哉・有賀敦紀（2021）『消費者行動研究における再現性問題と研究実践』,「消費者行動研究」, 27(1,2), 1-22. Malhotra, N. (2019) Marketing Research: An Applied Orientation, Pearson Education Limited. Oliver, L. R. (1997) Satisfaction: A Behavioral Perspective on the Consumer, McGraw-Hill Education. Sprott, D., Czellar, S., &amp; Spangenberg, E. (2009). The importance of a general measure of brand engagement on market behavior: Development and validation of a scale. Journal of Marketing Research, 46(1), 92–104. "],["handling.html", "Chapter 5 データ処理と記述分析 ", " Chapter 5 データ処理と記述分析 "],["本章の概要-2.html", "5.1 本章の概要", " 5.1 本章の概要 本章では、Rを用いたデータの処理と記述的な分析について紹介する。マーケティング領域では、様々なタイプのデータを扱うが、どのようなデータであってもデータを取り込み、分析可能な形に処理した後、データの特徴について確認することが必要になる。とくに、最終的に高度な統計分析を行うことを想定していたとしても、自身の獲得したデータの特徴を確認することは非常に重要である。そのため、本章ではデータの読み込みやデータ処理といった、分析の前に必要な技術的過程を紹介する。 これまでの本書の内容は、データを収集するまでの注意点や方法を説明した。しかしながら、収集したデータをただ眺めているだけでは、定量的な知見を得ることはできない。そのため、以降の節では主にデータ処理や分析手法について説明する。まず我々は、データセットの構築から学ぶ。例えばあなたがアンケートを実施したならば、そのアンケートからデータセットを構築する努力が必要になる。アンケート結果に基づくデータセット構築において研究者はコーディング、トランスクライビング、データクリーニングのプロセスを経る。 コーディングは、回答を分析可能なフォーマットへ変換する作業であり、通常回答に対して数値を当てはめる作業を伴う。例えば、回答者が男性ならば 1 を、女性ならば 0 をとるようなダミー変数を作成する作業がこれに当てはまる。コーディングは、不必要な情報を減らすことでデータ化プロセスを担う。トランスクライビングは、質問紙に記載された回答をデータ入力していく作業である。入力に関するヒューマンエラーは起こるものとして考える必要があるため、通常このプロセスは二人一組でダブルチェックをしながら行う。なお、オンラインアンケートの際はこのプロセスは自動で行われるため、不要になる。データクリーニングでは、研究者は不適切な回答のチェックを行う。例えば、回答可能範囲から外れた回答（例、7点尺度における8点回答）や、論理的に非整合的な回答（例、回答者が利用したことないと答えているサービスについて評価している場合） がないかをチェックする。また、欠損値という回答がない観測についてもチェックする必要がある。マーケティング分野においては欠損値のあるサンプルを削除するという方法も用いられるが、欠損値の扱いは奥が深く、いくつかの対応法がある。本書ではその詳細については扱わないが、欠損値に対応するためのデータ処理についての専門書も存在するため、関心のある読者はそれを参照してほしい（高橋・渡辺, 2017）。 データセットの構築が完了したあとは、本書では基本的にRを通じて様々な作業を行う。Rには、様々な計算を実行するための関数が用意されており（例、mean, median, sqrt 等）、これらを使えば、実際に我々分析者が各コマンドもシンプルになる。関数の利用においては f(argument) のように関数名 f のあとにカッコをつけて表記する。なお、argument は日本では引数とよばれ、計算に必要な情報の指定である。関数の利用において作業者は具体的な関数名とそれに対応する引数を指定する必要がある。また、我々は通常、パッケージをインストール・起動することで他者が作った関数を利用することが多い。関数とパッケージについての説明や実行例は「Rの基本操作」節で紹介しているのでそちらを参照してほしい。 ここで学ぶRでの作業は主に以下の通りである。 データの読み込み（csv, excel, etc.） dplyrの利用とデータ整形 パイプ演算子を用いた複数処理の実行 Wide vs. Long data format (おまけ) なお、これらの作業は、統計的な分析を実行する前のデータ前処理にも使われるものなので、データ分析をしたいと考える人達にとってはとても重要なスキルになる。 分析可能な形にデータを処理した後は、データの特徴を確認することが必要になる。具体的には、記述統計や図示化を用いて、特定の変数の分布や変数間の関係について確認を行うことが重要である。この過程により、調査の背景にある実情を把握できるとともに、入手したデータ（のコーディングなど）にエラーがないかを確認することにもつながる。 "],["データの読み込み.html", "5.2 データの読み込み", " 5.2 データの読み込み 本節で用いるパッケージをまだインストールしていない読者は、以下のコマンドを用いてインストールしてほしい。また、インストールを完了したら、library()関数によって各パッケージを起動すること。 install.packages(c(&quot;tidyverse&quot;,&quot;readr&quot;,&quot;readxl&quot;)) library(tidyverse) library(readr) library(readxl) ここからは、データセットを用いた分析を行う。基本的な操作においては、R外部で作成されたデータを取り込み利用するのだが、あるソフトウェアで作成・保存されたデータセットが他の環境で利用できるとは限らないという点に注意が必要である。具体的には、エクセルファイルが誰にでも開けるとは思ってはいけない。そのため、ソフト特性に依存しない汎用的な形式を使うことが好ましい。汎用性の高いファイル形式の代表的な例がCSV (comma separated values) である。以下は、mktData.csvという架空のファイルをdfというオブジェクト名で取り込むための、見本コードである。ここで用いる関数は、readrというパッケージのread_csv() という関数である。なお、以下のコードは、実在しない ’mktData.csv’というデータセットを引数に利用した見本コードであるため、このコードをそのまま実行してもエラーを返すだけであることに注意をしてほしい。実際には、自身が利用するファイル名を指定してファイルを読み込むことになる。なお、以下のコードの2行目は、データの1行目に変数名（列名）が含まれていない場合の引数の指定方法である。 df1 &lt;- readr::read_csv(&quot;mktData.csv&quot;) df2 &lt;- readr::read_csv(&quot;mktData.csv&quot;, col_name = FALSE) なお、デスクトップ版を利用している場合には、ファイルが格納されているディレクトリ名も指定する必要がある。Rにおいては様々なファイルを入力・出力することになるため、利用するディレクトリが一貫していないとそれだけで作業が煩雑になる。そのため、「（補足）デスクトップ版の利用とプロジェクト機能」節で紹介している「プロジェクト機能」必ずを活用するようにほしい。 本講義では、大学の学務ポータル（manaba）を通じて教員が配布したデータを学生各自のコンピュータにダウンロードし、それを post.cloudに各自がアップロードするという手順によって分析用データを利用する。Manabaからのデータのダウンロードは各自で済ませてほしい。Posit.cloudはR studio 画面を表示する段階でプロジェクトが作成される。ここではまず、分析に利用するデータを格納するディレクトリを作成するコードを紹介する&lt;デスクトップ版を使用している場合も、Project を指定していれば、以下のコードで全く同じ結果を得ることができる。&gt; 具体的は、以下の通りdir.create() を使って新たに data というディレクトリ（フォルダ）を作成する。 dir.create(&quot;data&quot;) 新たなディレクトリを作成したら、そこに、ポータルよりダウンロードしたデータを入れてほしい。ここではまず “2022idpos.csv”というデータを用いる。データが無事 data ディレクトリに含まれたら、以下のコマンドによってそのデータファイルをR の作業スペースに読み込み、それに “idpos” というオブジェクト名を定義する。なお、ここで分析実行社はディレクトリを指定することも必要になる。また、コード内の na は、欠損値がどのように保存されているかを指定するための引数であり、もし欠損値が空欄であればnaによる指定は必要ない。 idpos &lt;- readr::read_csv(&quot;data/2022idpos.csv&quot;, na = &quot;.&quot;) 問題なくデータを読み込むことができたら、そのデータの冒頭数行を head() 関数によって表示する。head() 関数の結果によると、このidposデータは、3000行、5列のデータセットであることがわかる。なお、同様の情報は R studio 画面内の Environment タブから確認することできる。 head(idpos) ## # A tibble: 6 × 4 ## id date spent coupon ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019/9/25 14326 1 ## 2 32 2019/9/10 10232 1 ## 3 30 2019/9/9 6881 1 ## 4 29 2019/9/4 6365 0 ## 5 46 2019/9/10 7595 1 ## 6 44 2019/9/14 7858 0 また、読み込んだデータ特徴の確認は他の関数でも実行できる。例えば、name() 関数を使えば、データ内の変数名 (列名) を確認できるし、tidyverseに含まれる glimpse() 関数によってもデータの冒頭数行を含むいくつかの情報を返してくれる。 names(idpos) ## [1] &quot;id&quot; &quot;date&quot; &quot;spent&quot; &quot;coupon&quot; glimpse(idpos) ## Rows: 3,000 ## Columns: 4 ## $ id &lt;dbl&gt; 12, 32, 30, 29, 46, 44, 44, 32, 3, 34, 36, 3, 42, 18, 38, 4, 19… ## $ date &lt;chr&gt; &quot;2019/9/25&quot;, &quot;2019/9/10&quot;, &quot;2019/9/9&quot;, &quot;2019/9/4&quot;, &quot;2019/9/10&quot;, … ## $ spent &lt;dbl&gt; 14326, 10232, 6881, 6365, 7595, 7858, 9405, 1821, 8375, 1828, 6… ## $ coupon &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, … なお、このidposデータは、POS (Point of sales) という小売店レジでの取引データとロイヤルティプログラムなどの会員IDを含むID-POSと呼ばれるデータを想定し作成した、簡易的な人工データである。データには、小売店舗での取引日（date）、金額（spent）、クーポン利用の有無 (coupon)、性別 (gender) が含まれている。本来のPOSデータは、より詳細な日時や具体的な製品単品レベルの取引品目など、より詳細な情報が含まれているはずだが、ここでは簡単化のためにこのようなデータにしている。また、もしスプレッドシート形式で表示したい場合には View() 関数をconsoleに直接入力することでそれが可能になる。例えば、idposデータを用いて以下のようなコードを入力することで、Sourceウィンドウに新しいタブができ、そこにデータセットが表示される。 "],["データの整形.html", "5.3 データの整形", " 5.3 データの整形 データの整形には、tidyverseパッケージ群に含まれるdplyrというパッケージを用いる。しかしながら、tidyverseのインストール・起動しておけばdplyrも利用できるため、特に心配する必要はない。dplyr には、いくつもの便利な関数がふくまれているが、本節では主に以下の関数および機能を紹介する。 summarize() mutate() filter() select() arrange() パイプ演算子 %&gt;% summarize は、ある変数の平均値や標準偏差などの記述統計量を計算することができる関数である。例えば、dataというデータセットに含まれる var_name という変数の平均値を計算し、それを M という変数名として定義する場合、以下のコマンドを用いる（以下のコマンドは見本コードである）。 summarize(data, M = mean(var_name)) mutate は、データセットに引数内で指定した定義の変数（列）を追加する関数である。例えば、dataというデータセットに対し、definition で定義した変数をnew_varとして追加するには、以下のコマンドを用いる（以下は見本コードである）。実際にdefinitionを定義する場合には、様々な関数や論理式を利用する必要がある。例えば、“new_var = var1/100” という定義を用いれば、var1を1/100倍した値をnew_varとして定義することになる。また、“new_var = var1 – mean(var1)”という定義を用いれば、var1の観測値からvar1の平均値を引いた値をnew_varとしている。なお、このような操作化を一般的に「中心化」と呼ぶ。 mutate(data, new_var = definition) mutate関数の利用においては、条件分岐を用いた変数の作成を行うこともある。そのように、研究者がある変数の値に応じて異なる値を変数を作成するときには、mutate内で、ifelse()関数を用いるのが良い。ifelse() 内の第一引数は条件、第二引数は条件が満たされたときの処理、第三引数は条件が満たされないときの処理をそれぞれ表す。なお、特定の条件の指定には “==” （同値）, “&gt;=”（以上）, “&lt;=”（以下） を使う。具体的には、var1 が2ならば1をとり、それ以外であれば０をとるという条件でnew_varを作成するという指示は、以下のようになる（以下は見本コードである）。 mutate(data, new_var = ifelse(var1 == 2, 1, 0)) filter関数は、データから特定の条件に合致する行だけ取り出す場合に用いる関数である。例えば、男性（gender == “male”）のサンプル情報のみ抽出したい場合には以下のような指示になる。 filter(data, gender == &quot;male&quot;) なお、特定の条件以外のものを指定したいときは、 という論理式 “!=” (not equal) を使う。男性以外の行を選ぶための指示は、以下の通りになる。 filter(data, gender != &quot;male&quot;) select関数は、特定の変数（列）を選んで新たなデータフレームを作成することができる関数である。例えば、dataというデータセットから、var1、var2、var3 という変数（列）を抽出して、data2というdataframeとして定義するには、以下のような指示になる。 data2&lt;- select(data, var1, var2, var3) 反対に、取り除きたい変数を指定するときには、以下のように “-” を使う。 data2&lt;- select(data, -var1) 列の指定方法には、いくつかのやり方が存在する。並んでいる列をまとめて指定するときは:（コロン）を使う。例えば、var1からvar5までの列をまとめて抽出し、それをdata2として定義するのは以下のようにできる。 data2&lt;- select(data, var1:var5) また、tidyverseのstarts_with()（ends_with()）を使うことで、変数名の冒頭（末尾）が特定の文字列から始まる変数を指定するようなことも可能である。例えば、“v” という文字から始まる変数を取り出すための指示は、いかのようになる。 data3&lt;- select(data, starts_with(&quot;v&quot;)) arrangeは、データの並べかえを可能にする関数である。例えば、以下ではvar1の値が小さい順（昇順）に並べ替えるような指示を示す。一方で、降順にする場合は、desc(var1)と引数を指定する必要がある。 data2 &lt;- arrange(data, var1) data2 &lt;- arrange(data, desc(var1)) また、tidyverse環境において、変数名を変更することも、rename() 関数で可能になる。 data3 &lt;- rename(data2, var1 = sales) Tidyverse 内の dplyr を使うことでパイプ演算子（%&gt;%）が使える（ショートカット: Command (control) + Shift + m）。パイプ演算子は、左側の処理結果を演算子右側の関数の第一引数として利用するための指示である。たとえば、以下のコマンドではまず \\(\\small 10-6\\) が計算され、その結果である “4” が sqrt() の引数として利用される（sqrt(4) は 2）。 (10-6) %&gt;% sqrt() ## [1] 2 パイプ演算子は、複数のデータ操作処理を連続して行う際に便利である。例えば、顧客の情報を含むデータセット(data)から、男性に該当する情報のみを抽出し、var1(例、購買額)についてのランキングを作成したうえでいくつかの変数を含んだデータセット（new_data）を作成する場合を考える。その際に実行すべき作業とそれらに対応する関数は以下のように示すことができる。 男性の情報だけ抜き出す(filter) Var1の値について降順に並べ替える(arrange) 第一位から最下位までの順位を割り当てた ranking 変数を作る(mutate) var1 , var2, var3, var4, orderだけ残し(select) new_dataとして定義する 上記の作業を一気に行うためのコードをパイプ演算子を使わずに書くと以下の様になる（以下は見本コード）。 new_data &lt;- select(  mutate(   arrange(    filter(data, gender == &quot;male&quot;),    desc(var1)),    ranking = 1:n()),   var1, var2, var3, var4, order) パイプ演算子を使わない場合、先に実行する処理が内側に来ており、一見して何を行っているのか理解するのが難しい。一方でパイプ演算子を使い、左側の処理結果を演算子右側の関数の第一引数として利用すると、以下のように書き換えることができる。 new_data &lt;- data %&gt;% filter(gender == &quot;male&quot;)%&gt;% arrange(desc(var1)) %&gt;% mutate(ranking = 1:n()) %&gt;% select(var1, var2, var3, var4, order) パイプ演算子の利用により、各関数の処理を一つの行で示せる。また、処理の順番通りに関数を記載することが可能なので、コードの記述容易性と可読性の両方が高まる。また、パイプ演算子による操作は次の関数の第一引数以外に反映されることも可能である。第一引数以外の引数に左側の処理結果を反映させる際には、該当する箇所に “.” （ドット）を使う。たとえば、\\(\\small 10-2\\)の計算結果を用いて2から8の偶数で構成されるベクトルを返すためのコードは以下のように書くことができる。 (10-2) %&gt;% seq(from = 2, to = ., by = 2) ## [1] 2 4 6 8 データの整形・処理作業が終わったら、そのデータを自身のコンピュータ内のストレージに保存したいと考えるかもしれない。Rでは、外部への書き出しという形でデータを保存することが可能である。例えば、df という名前のデータフレームをnew_dataというファイル名で、dataというディレクトリにcsv形式を用いて保存するためには、以下のようなコードを用いる（以下は見本コード）。また、csv以外にもファイル形式は選択可能であり、例えばRのデータ形式(.Rds)で保存する場合には、“#Rds” 以降のコードを用いる。 readr::write_csv(df, path = &quot;data/new_data.csv&quot;) #Rds readr::write_rds(df, path = &quot;data/new_data.Rds&quot;) 5.3.1 企業データの処理 ここで学んだデータ処理の手法を実行するために、 本節では、先ほど読み込んだ MktRes_firmdata.xlsxデータを用いた分析を行う。このデータをＷｅｂサイトより data ディレクトリにダウンロードし、以下の要領で読み込んでほしい。 firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) このデータは、小売・サービス分野の企業約160社（企業数は年によって異なる）に関する2010年から2019年までの財務データである（計1440件）。このデータは、日本生産性本部における顧客満足度調査の対象になっている企業リストを作成し、その企業の中から金融領域の企業や、データを入手できなかった一部の企業を教育的意図から排除したものである。したがって、日本の小売・サービス分野において全国的に知名度のある代表的な企業の財務データ（の一部）だと考えられる。 なお、本データには以下の変数が含まれており、データ内の単位は従業員数（人）を除き百万円である。 fyear: 決算年 legalname: 企業名 ind_en: 日経業種名（英文） parent:親会社名（もしあれば） fiscal_month: 決算月 current_liability: 流動負債 ltloans: 長期借入金 total_liability: 負債合計 current_assets: 流動資産 ppent: 有形固定資産 total_assets: 資産合計 net_assets_per_capital: 純資産合計／資本合計 sales: 売上高 sga: 販売費及び一般管理費 operating_profit: 営業利益 net_profit: 当期純利益 pnet_profit: 親会社株主に帰属する当期純利益（連結）／当期利益（単独） re: 利益剰余金 adv: 広告・宣伝費 labor_cost: 人件費 rd: 研究開発費 other_sg: その他販売費及び一般管理費 emp: 期末従業員数 temp: 平均臨時従業員数 tempratio: temp/(emp+temp) indgrowth: 産業成長率 adint: 広告集中率（adv/sales） rdint: 研究集中率（rd/sales） mkexp: (sga - rd) / sales op: operating_profit / sales roa: pnet_profit / total_assets 本データセットは、複数年にわたる複数サンプルからのデータであり、一般的にこのような構造のデータをパネルデータという。パネルデータの分析の概要は 18 節で紹介している。 ここではこのデータを用いて、以下の作業を行う。 2018年度のデータのみを抽出する。 企業名、年、売上高、人件費、期末従業員数、平均臨時従業員数のみの変数を含むデータセットにする。 労働単価（人件費/（期末従業員数+平均臨時従業員数））変数を作成する。 労働単価の高い順に並び替えてトップ10企業を出力する。 firm2018_check &lt;- firmdata %&gt;% filter(fyear == 2018) %&gt;% select(legalname, fyear, sales, labor_cost, emp, temp) %&gt;% mutate(wage = labor_cost/(temp+emp), na.rm=TRUE) %&gt;% arrange(desc(wage)) head(firm2018_check, n = 10) ## # A tibble: 10 × 8 ## legalname fyear sales labor_cost emp temp wage na.rm ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 株式会社リクルート 2018 2310756 388583 45856 2449 8.04 TRUE ## 2 株式会社 大丸松坂屋百貨店 2018 459840 62692 6695 3581 6.10 TRUE ## 3 株式会社 大丸松坂屋百貨店 2018 459840 62692 6695 3581 6.10 TRUE ## 4 株式会社 帝国ホテル 2018 58426 17307 1940 998 5.89 TRUE ## 5 株式会社 髙島屋 2018 912848 83779 7761 8849 5.04 TRUE ## 6 株式会社コメリ 2018 346862 43991 4646 4777 4.67 TRUE ## 7 株式会社オートバックスセブン 2018 213840 22139 4171 747 4.50 TRUE ## 8 株式会社ロイヤルホテル 2018 40884 13115 2049 894 4.46 TRUE ## 9 オルビス株式会社 2018 248574 28555 4181 2330 4.39 TRUE ## 10 株式会社ファンケル 2018 122496 15103 1381 2213 4.20 TRUE このように、データの中から研究課題と整合的な情報を抽出したり、変数を作成したりすることができる。ただし、研究者にとって都合の良い結果を得るために恣意的に用いるデータを制限、操作することは、研究不正となる。そのため、実際の研究では、どのようなデータ・情報を用いるかについて事前に計画しておく必要がある。 "],["おまけ-wide型とlong型データセット.html", "5.4 (おまけ) Wide型とLong型データセット", " 5.4 (おまけ) Wide型とLong型データセット インターネットを通じて、とても都合の良いかつ信頼できるデータセットが入手できたとしても、それが分析のために望ましい形で保存されているとは限らない。特に、横長(wide)と縦長(long)データが存在し、それらのデータの型の違いには注意が必要である。我々人間がデータを眺め、解釈を与える場合にはwide型データのほうが扱いやすいのだが、コンピュータやソフトウェアがデータを分析する際には、long型のほうが好ましい。例えば、下図は4店舗のある年の6月から10月までの売上情報（単位：千円）を示したデータセットである。これは、複数サンプル-複数時点という構造のデータだが、各時点の観測値が横に並んでおり、wide型データだといえる。 Wide型データ 手元にあるWide型データをLong型に変換したい場合の対応策として、ここでは tidyverseに含まれているtidyrのgather()関数を用いる方法を紹介する。この関数の利用方法を実演するためにManabaにアップされている sales_wide.csv をダウンロードし、プロジェクト内のdataディレクトリに移して欲しい。データを読み込むと、以下のようにデータ構造を確認することができる。 sales_wide &lt;- readr::read_csv(&quot;data/sales_wide.csv&quot;, na = &quot;.&quot;) ## Rows: 4 Columns: 6 ## ── Column specification ──────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): store, sales06, sales07, sales08, sales09, sales10 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. sales_wide ## # A tibble: 4 × 6 ## store sales06 sales07 sales08 sales09 sales10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1500 2000 2300 1700 1500 ## 2 2 900 1500 1600 1600 1000 ## 3 3 700 900 1000 800 650 ## 4 4 1000 1300 2000 1500 1400 ここで用いる gather()関数は、以下の引数を指定する： - data: 変換元のデータ - key: 変数を１列にまとめたあと、元の列を区別するための列につける名前 - value: 変数を１列にまとめたあと、値が入る列につける名前 - どの範囲を一列にまとめるかの範囲指定 その上で、さきほどのsalesデータをLong型に変換するために、以下のようなコマンドを利用する。 sales_long&lt;- gather(data = sales_wide, key = &quot;month&quot;, value = &quot;sales&quot;, starts_with(&quot;sales&quot;)) sales_long ## # A tibble: 20 × 3 ## store month sales ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 sales06 1500 ## 2 2 sales06 900 ## 3 3 sales06 700 ## 4 4 sales06 1000 ## 5 1 sales07 2000 ## 6 2 sales07 1500 ## 7 3 sales07 900 ## 8 4 sales07 1300 ## 9 1 sales08 2300 ## 10 2 sales08 1600 ## 11 3 sales08 1000 ## 12 4 sales08 2000 ## 13 1 sales09 1700 ## 14 2 sales09 1600 ## 15 3 sales09 800 ## 16 4 sales09 1500 ## 17 1 sales10 1500 ## 18 2 sales10 1000 ## 19 3 sales10 650 ## 20 4 sales10 1400 編集された縦長データは上記の通り示されるが、その中身を見ると、monthの列にsales06などの情報が記載されており、好ましくない。この問題は、元のデータにおける変数名に該当する情報を新しいkey列の値に使うというgather関数の仕様に影響するものである。この問題に対しては、パイプ演算子を使ってgatherの実行前に、列名を年だけの形に変更することで対応可能である。以下が、修正版のコードであり、出力結果より、先述の問題点が解決されたことが確認できる。ただし、gather関数の実行において、下記コード内では範囲の引数の指定についても修正されていることに注意が必要である。 sales_long &lt;- sales_wide %&gt;% rename(`06` = sales06, `07` = sales07, `08` = sales08, `09` = sales09, `10` = sales10) %&gt;% gather(key = &quot;month&quot;,value = &quot;sales&quot;, `06`:`10`) %&gt;% arrange(store) sales_long ## # A tibble: 20 × 3 ## store month sales ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 06 1500 ## 2 1 07 2000 ## 3 1 08 2300 ## 4 1 09 1700 ## 5 1 10 1500 ## 6 2 06 900 ## 7 2 07 1500 ## 8 2 08 1600 ## 9 2 09 1600 ## 10 2 10 1000 ## 11 3 06 700 ## 12 3 07 900 ## 13 3 08 1000 ## 14 3 09 800 ## 15 3 10 650 ## 16 4 06 1000 ## 17 4 07 1300 ## 18 4 08 2000 ## 19 4 09 1500 ## 20 4 10 1400 一方で、Long型データからWide型データへ変換するためには、tidyrのspread()を用いることが多い。spread() では、主に以下の引数を用いる。 data: 変換元のデータ key: 変数を複数列にわけるとき、列を区別するための変数 value: 複数列に分ける値 例えば、以下のコードで先程の sales_long データをwide型に変換することができる。 wide_test &lt;- sales_long %&gt;% spread(key = &quot;month&quot;, value = &quot;sales&quot;) %&gt;% rename(sales06 = `06`, sales07 = `07`, sales08 = `08`, sales09 = `09`, sales10 = `10`) wide_test ## # A tibble: 4 × 6 ## store sales06 sales07 sales08 sales09 sales10 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1500 2000 2300 1700 1500 ## 2 2 900 1500 1600 1600 1000 ## 3 3 700 900 1000 800 650 ## 4 4 1000 1300 2000 1500 1400 "],["データの要約と可視化.html", "5.5 データの要約と可視化", " 5.5 データの要約と可視化 本章では、記述統計や可視化によってデータを要約する方法について説明する。記述統計では、統計量と呼ばれる指標を用いてデータの特徴を数値から把握する。一方で可視化においては、図表を作成することでデータの特徴を視覚的に理解することを目的とする。実証的なマーケティング研究においては、データを用いた仮説の検証という方法が主流かもしれないが、仮説検証に用いるデータはどのようなものなのかを要約し、それを（論文やレポートの）読者へ伝えるプロセスは必要である。記述統計やデータの可視化は、このプロセスにおいて機能する方法である。なお、本章の作業においても tidyverse を用いるので、以下のように tidyverse を起動してほしい。 library(tidyverse) "],["記述統計.html", "5.6 記述統計", " 5.6 記述統計 記述統計の利用においては、データのタイプ別に利用すべき統計量が異なることに注意が必要である。「データのタイプ」という節で確認したように、データには量的変数とカテゴリ（を示す質的）変数がある。量的変数は数値で測定できるものであり、その計算結果を解釈することも可能である。一方でカテゴリ変数は、各観測個体が属している状態やグループを表す指標であり、それを計算してもそこから含意を得るのが難しい。Rのような統計ソフトは非常に素直なので、たとえカテゴリ変数であってもそこに数値が入力されていれば、記述統計に必要な計算を実行し、結果を返してくれる。しかしながら研究においてはそれらの結果を適切に解釈する必要があり、自身が用いている変数のタイプに応じた分析を実行する必要がある。 その上で本節ではまずひとつの量的変数の情報を要約するための記述統計を紹介する。一つの数値によってデータ全体を代表させるような数値を代表値と呼ぶ。代表値はおもにデータの中心を示す指標と考えられる。本節ではデータの中心を表す指標として中央値 (median) と平均値 (mean) を紹介する。中央値は、データのすべての観測値において、その値より小さな観測値の数と大きな観測値の数が等しくなるような真ん中の値を表す。そのため、（1, 3, 2, 5, 4）というデータにおける中央値は3である。これは、このデータを、1, 2, 3, 4, 5 と並べ替えると、3よりより小さな観測値の数と大きな観測値の数が等しくなっていることから確認できる13。 d &lt;- c(1, 3, 2, 5, 4) median(d) ## [1] 3 d2 &lt;- c(1, 3, 2, 5, 4, 6) median(d2) ## [1] 3.5 平均値（算術平均と呼ばれる）は、最もよく使われる代表値の一つである。平均値は、n個のデータ、\\(\\small x_1,x_2,...,x_n\\) に対して以下のように定義される。 \\[\\bar{x} = \\frac{1}{n}\\sum_i^n x_i\\] 観測値と平均値の差（\\(x_i - \\bar{x}\\)）は偏差と呼ばれ、偏差の和はゼロである（\\(\\sum_ix_i - \\bar{x}=0\\)）という性質を持つ。つまり、平均値を中心として、データの正の方向へのばらつきと負の方向へのばらつきが釣り合いが取れているということが伺える。この点が、平均値がデータの中心を表す代表値として用いられるひとつの理由である。また、平均値にはいくつかの好ましい統計的性質があるのだが、それについては後述する。Rにおいては、mean() 関数を用いることで分析が可能である。例えば、9人の生徒に対して行われた数学(x)と国語(y)のテスト(10 点満点)の結果が、それぞれ以下の通りであったとしよう。 数学: (3,3,5,5,5,5,5,7,7) 国語: (2,3,3,5,5,5,7,7,8) このときの平均値は以下のように求まる。 math &lt;- c(3,3,5,5,5,5,5,7,7) jpn &lt;- c(2,3,3,5,5,5,7,7,8) mean(math) ## [1] 5 mean(jpn) ## [1] 5 計算の結果、どちらも平均値は5であった。データの中心を表す代表値の値が等しかったため、これら2科目のテスト結果は同じ分布を持つと判断して良いのだろうか。自明かもしれないが、そのような解釈は不適切である。具体的には、データの「ばらつき」についても確認する必要がある。分布のばらつきは、平均値からの離れ方(平均値からの偏差) によって判断される事が多く、これが大きなデータが多い場合は、よりデータは散らばっ て分布していると解釈される。一方でデータが平均の近くに集まって分布している場合、ばらつきが小さいと捉えられる。この分布のばらつきは主に、分散や標準偏差という指標で測られる。 分散 (Variance, \\(S^2\\)で定義する) は以下のように、平均からの偏差の二乗の和をデータ数で割ったものだと定義される。平均からの偏差の和を計算すると、正の方向へのズレとマイナス方向へのずれがあるので、互いに相殺しあって合計は 0 になる。そこで、偏差の二乗和を用いることでデータ全体がどの程度平均からばらついているかを把握する。 \\[S^2 = \\frac{1}{n}\\sum_i^n (x_i-\\bar{x})^2\\] しかしながら、分散は元の値を二乗しているのでもとのデータと単位が異なる。そのため、分散の正の平方根 (\\(\\sqrt{\\cdot}\\)) を取った値を標準偏差と呼び、この標準偏差を用いることも多い14。なお、Rでは var() と sd() によって分散と標準偏差をそれぞれ求める。ただし、Rの関数による計算では \\(s^2=\\frac{1}{n-1}\\sum_i^n (x_i-\\bar{x})^2\\) で定義される「不偏標本分散」および「不偏標準誤差」という指標を用いる。これは、これらの指標のほうが統計的に好ましい性質を持っているためであるが、Rを用いた分散の計算値が、nで割った際の手計算値と異なることがあるのでその点には注意が必要である。 var(math) ## [1] 2 var(jpn) ## [1] 4.25 先程の数学と国語のテスト結果データを用いて分散を計算すると、国語の方が分散が大きいことがわかる。つまり、両テストとも平均値は同じであるものの、国語のほうがそのスコアのばらつきが大きいことがわかる。このように、代表値とともにデータのばらつきに関する情報も踏まえてデータの特徴を把握することが好ましい。 観察されたデータと標準偏差を用いて、特定の観測結果がデータ内において「相対的に」どのような位置にいるのかを捉えることも可能になる。具体的には、任意の量的変数 \\(x_1,...,x_n\\) に対して、標準化されたスコア \\(z_1,..,z_n\\) は以下のように定義できる。 \\[ z_i=\\frac{(x_i-\\bar{x})}{\\sqrt{(S^2)}} \\] ただし、 \\(S^2\\) は変数 \\(x\\) の分散である（不偏標本分散を用いることもある）。上記定義の通り、標準化スコアは観測値の平均からの偏差を標準偏差で割っており、ある観測が平均値から標準偏差何個分ズレているかを示していると解釈できる。なお、標準化スコアは、平均が0、分散が1になることも知られている。 一方でデータの観測数（ \\(n\\) ）が偶数である場合、\\(\\small n/2\\) 番目と、\\(\\small (n/2)+1\\) 番目が中央となるため、n個のデータの観測値を、\\(x_1,x_2,...,x_n\\) とすると、これらふたつの値の平均値（ \\(\\small \\frac{x_{\\frac{n}{2}}+x_{ \\frac{n}{2}+1}}{2}\\) ）が中央値となる。Rにおいてはmedian() 関数によって以下のように計算することができる。↩︎ 偏差の二乗和のかわりに偏差の絶対値を用いた平均偏差という指標も存在する。しかしながら、分散や標準偏差のほうが好ましい統計的性質を持つことから、二乗和が用いられることが多い。↩︎ "],["カテゴリ変数の要約.html", "5.7 カテゴリ変数の要約", " 5.7 カテゴリ変数の要約 一方でカテゴリ変数は、代表値や分散によって含意を得るのではなく、頻度のカウント（集計）や、クロス集計を用いることが多い。これにより、各カテゴリにどれぐらいの観測数があるのかを確認することが可能になる。カテゴリ変数の内容（出現頻度）の確認には、table() 関数を用いる。また、with()関数を用いて同様の結果を得ることも可能である。ここでは、先ほど用いた firmdata から2018年度の情報を抽出し、日経業種に基づく産業の違いから、どのカテゴリの企業がどれだけデータ内にいるのかを確認する。なお、tidyverseを起動していない場合には、必要に応じて library(tidyverse) を事前に指示してほしい。 firm2018 &lt;- firmdata %&gt;% filter(fyear == 2018) table(firm2018$ind_en) ## ## Air Transportation Amusement Services Bakery Products ## 8 4 1 ## Communication Services Cosmetics &amp; Toilet Goods Department Stores ## 2 3 8 ## Foods, NEC Home &amp; Pre-Fabs Hotels ## 1 2 5 ## Miscellaneous Services Miscellaneous Wholesales Motor Vehicles ## 27 2 4 ## Musical Instrument Railroad (Major) Railroad (Minor) ## 1 27 2 ## Real Estate - Sales Retail Stores, NEC Supermarket Chains ## 1 35 14 ## Trucking ## 1 with(firm2018, table(ind_en)) ## ind_en ## Air Transportation Amusement Services Bakery Products ## 8 4 1 ## Communication Services Cosmetics &amp; Toilet Goods Department Stores ## 2 3 8 ## Foods, NEC Home &amp; Pre-Fabs Hotels ## 1 2 5 ## Miscellaneous Services Miscellaneous Wholesales Motor Vehicles ## 27 2 4 ## Musical Instrument Railroad (Major) Railroad (Minor) ## 1 27 2 ## Real Estate - Sales Retail Stores, NEC Supermarket Chains ## 1 35 14 ## Trucking ## 1 また、table関数にて2つのカテゴリ変数を指定することで、両変数に対応するカテゴリの出現頻度を返してくれる。このような表のことをクロス集計表とよぶ。例えば、同データにおける広告集中的な企業を把握するため、広告集中度が中央値よりも高ければ1、それ以外であれば0を取るダミー変数（8.2節参照）を作成し、各産業カテゴリとの関係を確認する。 firm2018 &lt;- firm2018 %&gt;% mutate(ad_dummy = ifelse(adint &gt; median(adint),1, 0)) with(firm2018, table(ind_en,ad_dummy)) ## ad_dummy ## ind_en 0 1 ## Air Transportation 4 4 ## Amusement Services 4 0 ## Bakery Products 0 1 ## Communication Services 1 1 ## Cosmetics &amp; Toilet Goods 0 3 ## Department Stores 0 8 ## Foods, NEC 0 1 ## Home &amp; Pre-Fabs 0 2 ## Hotels 5 0 ## Miscellaneous Services 17 10 ## Miscellaneous Wholesales 1 1 ## Motor Vehicles 0 4 ## Musical Instrument 0 1 ## Railroad (Major) 27 0 ## Railroad (Minor) 2 0 ## Real Estate - Sales 0 1 ## Retail Stores, NEC 11 24 ## Supermarket Chains 2 12 ## Trucking 1 0 これらのデータを確認すると、広告集中度が高い企業は鉄道会社やアミューズメント、ホテル、トラック運送業において少ないことがわかる。それ以外の産業では産業内でも広告集中度の高い企業と低い企業とが比較的バラけている。 特定のカテゴリ（例、デシルランク）に着目して、カテゴリ変数（例、性別）についての集計を行うことも可能である。例えば、広告集中度が高い企業における産業のばらつきを調べたいときには、filter() 関数を用いれば良い。 firm2018 %&gt;% filter(ad_dummy == 1) %&gt;% with(table(ind_en)) ## ind_en ## Air Transportation Bakery Products Communication Services ## 4 1 1 ## Cosmetics &amp; Toilet Goods Department Stores Foods, NEC ## 3 8 1 ## Home &amp; Pre-Fabs Miscellaneous Services Miscellaneous Wholesales ## 2 10 1 ## Motor Vehicles Musical Instrument Real Estate - Sales ## 4 1 1 ## Retail Stores, NEC Supermarket Chains ## 24 12 カテゴリ変数と量的変数の関係を調べることも、グループ別に量的変数の要約を行う形で可能である。また、そのための手法はすでに我々は学習済みである。具体的には、前節で利用した group_by() 関数を用いる。例えば、売上高と広告集中度の平均と標準偏差を産業ごとに確認することは、以下のような指示で可能になる。 firm2018 %&gt;% group_by(ind_en) %&gt;% summarize(obs = n(), sales_m = mean(sales), sales_sd = sd(sales), adint_m = mean(adint), adint_sd = sd(adint)) ## # A tibble: 19 × 6 ## ind_en obs sales_m sales_sd adint_m adint_sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Air Transportation 8 1772786. 305240. 0.00311 0.00333 ## 2 Amusement Services 4 298138. 263017. 0 0 ## 3 Bakery Products 1 1059442 NA 0.0122 NA ## 4 Communication Services 2 547088. 172736. 0.0231 0.0327 ## 5 Cosmetics &amp; Toilet Goods 3 140669. 100063. 0.108 0.0498 ## 6 Department Stores 8 795322. 350240. 0.0199 0.00507 ## 7 Foods, NEC 1 504153 NA 0.0229 NA ## 8 Home &amp; Pre-Fabs 2 4143505 0 0.00857 0 ## 9 Hotels 5 62135. 58060. 0 0 ## 10 Miscellaneous Services 27 311867. 456037. 0.0114 0.0204 ## 11 Miscellaneous Wholesales 2 176520 52778. 0.0203 0.0287 ## 12 Motor Vehicles 4 5279122. 4233188. 0.0254 0.00404 ## 13 Musical Instrument 1 434373 NA 0.0443 NA ## 14 Railroad (Major) 27 1302921. 1037834. 0 0 ## 15 Railroad (Minor) 2 260502 0 0 0 ## 16 Real Estate - Sales 1 1861195 NA 0.0114 NA ## 17 Retail Stores, NEC 35 571019. 547247. 0.0243 0.0272 ## 18 Supermarket Chains 14 4335164. 3511347. 0.0147 0.00782 ## 19 Trucking 1 1118094 NA 0 NA このように、カテゴリごとの量的変数の要約も実行可能である。なお、標準偏差が NA となっている箇所は、観測数が 1 であり、標準偏差を計算できない状況を表している。 "],["データの可視化.html", "5.8 データの可視化", " 5.8 データの可視化 本書でのデータの可視化では、主にtidyverse内に含まれる ggplot2 というパッケージを用いる。データは一般的に、円グラフ、折れ線グラフ、帯グラフなどの様々なグラフを用いて視覚化される。しかしなが本節では、主にヒストグラム、箱ひげ図、バイオリンプロットをRでの実行例とともに紹介する。これらの図は、量的変数の分布を視覚的に示すことについて優れた可視化の方法だと言える。ここでは、ggplot2に内包されている diamonds データを用いて可視化を学ぶ（tidyverseを起動することで自動的に ggplot2も起動されるため、このタイミングでtidyverseを起動していない場合には、必要に応じて library(tidyverse) によってパッケージを起動してほしい）。diamonds データについては以下のように確認できる。 head(diamonds) ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 なお、Macのデスクトップ版でggplot2等を使うと日本語が文字化けするので、Macユーザーは別途以下のコマンドを実行する必要がある。 ##For mac users theme_set(theme_gray(base_size = 10, base_family = &quot;HiraMinProN-W3&quot;)) 本書の可視化では、まず、ggplot2の ggplot() 関数を用いて図示化のためのオブジェクトを作成する。この関数では、以下の引数を指定する。 data: 可視化に用いるデータフレームの指定 mapping: データから抽出する変数と画面に表示される図との関係の指定 mapping内で、aes() 関数（aesthetics）で視覚化に用いる変数とプロット要素間の接続を図ることも多い。 ggplot関数で作成された図示化オブジェクトには、着目するデータと変数が特定されている。続いて、ggplot()で作られたオブジェクトに対して、geom (geometry) 用いてグラフィックの層(layer)を加えることで図を作成する。このプロセスでは、geom_point() による散布図や、geom_histogram() によるヒストグラムなど、具体的な図表のタイプに対応する関数を利用することで、図を作成できる。また、geomに関する関数以降に labs() というラベルに関する関数を追加することで、図に必要な情報を加筆することが可能になる。 ggplot2を用いたデータ可視化の例として、まず本書はヒストグラムを描画する。ヒストグラムはデータの分布を離散的に示すものであり、連続変数を階級で分けて各階級の頻度を図示化する。一つの変数を扱った図なので、mapping引数ではひとつの変数を指定する。その上で作成した図示化オブジェクトに geom_histogram() を追加することでヒストグラムを描画する。以下では、ダイアモンドの価格の観測頻度についての可視化例である。価格の程度を離散的に区切り、その区切られた各範囲の価格を取る観測がデータ内にどれだけ存在するかを示している。 p1 &lt;- ggplot(diamonds, mapping = aes(x = price)) p1 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム1: ダイアモンド価格&quot;) なお、縦軸を確率密度(density)に変えるときは、geom_density()を用いる。その際、fillという引数を設定すると、密度を範囲に色を塗ることができる (なお、“p1” というオブジェクトは再利用できるので、再びggplot()によって指定する必要はない)。 p1 + geom_density(fill = &quot;black&quot;, alpha = 0.5) + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム2: ダイアモンド価格（geom_density）&quot;) 箱ひげ図は、四分位数と四分位範囲等を図示化したもの。四分位数はデータを4等分する区切りの値であり、第一四分位はQ1、第二四分位はQ2、第三四分位はQ3、最大値はQ4で示される。四分位範囲はQ3-Q1の範囲で示されるものである。ここでは、Cutの質（Fair, Good, Very Good, Premium, Ideal）ごとに価格の分布を比べるため、複数の箱ひげ図を並べる例を提示する。 p2 &lt;- ggplot(diamonds, mapping = aes(x = cut, y = price)) p2 + geom_boxplot() + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;箱ひげ図1: ダイアモンド価格&quot;) 箱ひげ図を作成すると、ひげの上下に点が表示されることがある（上図では上部が太線のように見えている）。これは、外れ値の候補として全体の分布から離れて存在する観測値が示されている。ここで示される外れ値の候補は、Q1よりも四分位範囲\\(\\times 1.5\\times 1.5\\) 以上小さい、ないしは、Q3よりも四分位範囲\\(\\times 1.5\\times 1.5\\) 以上大きいかで特定される。外れ値がある場合、入力ミスなどのエラーではないか、異質な観測値でないか、を検討、確認することが必要になる。 バイオリンプロットは、箱ひげ図よりももう少し詳しくデータの分布を確認できる図である。ggplot2では、geom_violin() を用いる。例えば、先程の箱ひげ図をバイオリンプロットで示すと、以下のようになる。以下の図は、バイオリンプロット内に箱ひげ図を示すことでよりわかりやすい図を作成するように工夫している。 p2 + geom_violin() + geom_boxplot(fill = &quot;gray&quot;, width = 0.1) + labs(x = &quot;Cutの質&quot;, y = &quot;価格&quot;, title = &quot;バイオリンプロット: ダイアモンド価格&quot;) バイオリンプロットで横に広がっているところは、ヒストグラムで言う山が高いところを意味しており、そこに多くのデータが集まっていることを示している。 "],["二変数間の関係の要約.html", "5.9 二変数間の関係の要約", " 5.9 二変数間の関係の要約 ここまでの内容は（カテゴリ変数に関する一部の説明を除き）、一つの変数に関する要約と可視化を扱っていた。しかし、データ分析では二つの異なる変数間の関係を捉えたいと考えることも多い。二変数間の関係を数量的に要約するための指標の代表例が共分散や相関係数である。データ数をnとする変数xとyの共分散（\\(S_{xy}\\)）は、以下のように定義される。なお、Rで共分散を求める際には cov() 関数を用いる。 \\(S_{xy}=\\frac{1}{n}\\sum_i^n (x_i-\\bar{x})(y_i-\\bar{y})\\) また、\\(S_x\\)と\\(S_y\\)をそれぞれxとyの分散とし、相関係数（\\(\\rho_{xy}\\)）は以下のように定義される。Rで相関係数を求める際には cor() 関数を用いる。 \\(\\rho_{xy}=\\frac{S_{xy}}{\\sqrt{S_x^2}\\cdot \\sqrt{S_y^2}}\\) 共分散は、二つのデータ間の共変動を示す指標であるものの、この数値を持って我々研究者が二変数の関係について（例えばその強弱などを）解釈するのは困難である。そこで、二変数間の関係を数値的に解釈する場合には、一般的に相関係数を用いる。相関係数は、-1 から 1 までの値を取り、正の値を取る場合は正の相関、負の値を取る場合は負の相関を、着目している二つの変数が持つことが知られている。また、相関係数が正（負）の値かつ1に近いほど強い正（負）の相関であることが知られている。ただし、相関係数で表される二変数間の関係は、線形関係の程度である。言い換えると、相関が高いとはデータがどれだけ直線上に集まって分布しているかを示しており、グラフ等で示される線形関係の傾きについては何も回答することができないという点に注意が必要である。 例えば、以下のようなデータセットを考える。 X &lt;- tibble(x1 = c(-3, -1, 0, 2, 5), y1 = c(16, 12, 10, 6, 0), y2 = c(8, 6, 5, 3, 0)) X ## # A tibble: 5 × 3 ## x1 y1 y2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -3 16 8 ## 2 -1 12 6 ## 3 0 10 5 ## 4 2 6 3 ## 5 5 0 0 このデータセットにおける x1 と y1 の相関係数は -1 であり、両者の関係を図で示すと、すべてのデータが直線上（\\(y=-2x+10\\)）に並ぶことがわかる。一方で、x1 と y2 との相関係数も -1 であるものの、両者の線形関係は \\(y=-x+5\\)である。このことからも、相関係数が線形関数の傾きや切片についての情報は何も持たないことがわかる。 cor(X$x1, X$y1) ## [1] -1 ggplot(data = X, mapping = aes(x = x1, y = y1)) + geom_point() + geom_smooth(method = lm) また我々は、二変数間の相関係数がゼロであることが、両者が無関係であることを意味しないことにも注意をしなければならない。例えば、以下のようなデータセットにおけるA と B の相関は 0 になる。 ## # A tibble: 5 × 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -2 4 ## 2 -1 1 ## 3 0 0 ## 4 1 1 ## 5 2 4 cor(AB$A, AB$B) ## [1] 0 しかしながら、両者の関係を描画すると、\\(y = x^2\\) という二次関数の関係にあることがわかる。つまり、相関係数がゼロだからといって、二つの変数間に関係がないと結論づける事はできず、相関ではなく異なる複数の分析アプローチによって関係を特定していくことが必要になる。 ggplot(data = AB, mapping = aes(x = A, y = B)) + geom_point() + geom_smooth(method = lm, formula = y ~ x + I(x^2), se = FALSE) 勘の良い読者であればすでに気づいているかも知れないが、二変数間の関係についての可視化もggplot2にて対応できる。具体的には、geom_point()という関数を用いるのだが、それだけではなく、mappingに対する引数として、x と y 二つの変数を指定することが必要になる。ダイアモンドの価格は、カラット数に大きく依存すると考えられる。そこで、以下のようにカラット数と価格との間の共分散と相関を計算する。 cov(diamonds$carat,diamonds$price) ## [1] 1742.765 cor(diamonds$carat,diamonds$price) ## [1] 0.9215913 これらの変数間の相関係数は約0.92であり、高い正の相関関係であることが確認された。続いて、これらの変数の関係を可視化する。二変数間の関係を端的に可視化する方法が散布図である。散布図は、一方の変数を横軸に、もう一方の変数を縦軸に取り、各データのそれぞれの値の組み合わせをプロットしたものである。 p3 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price)) p3 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図1: カラット：価格&quot;) 研究目的次第では、二つの変数間の関係をカテゴリごとに比較したい場合もあるだろう。例えば、我々はカラットと価格の関係は、カットの質によって変わるのか、という問いに関心があるとしよう。その場合には、(1) 同一図内にてカテゴリごとに色分けする方法と、(2) カテゴリごとに分割して図示化する方法がある。それぞれのggplot2での実行方法は、以下のとおりである。 Mapping = aes() 内に、 color = categ_varと指定することで、categ_var変数のカテゴリに基づき色分けする。 facet_grid() や facet_wrap() を用いる。 まず、(1) の図内での色分け方法は、以下のようなコマンドで実行できる。 p4 &lt;- ggplot(diamonds, mapping = aes(x = carat, y = price, color = cut)) p4 + geom_point() + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, color = &quot;カット&quot;, title = &quot;散布図 2: カット別、カラット：価格&quot;) このように、mapping = aes() 内にて色付けに関する引数を設定することで散布図内の観測値を色分けできる。ただし、ここで重要なのは、color =という引数では、カテゴリ変数を指定すべきであり、色そのもの（例えば、redやblue）を指定するものではないということである。しかしながら、散布図 2のように多くのカテゴリが含まれる場合には、この可視化の方法だと逆に見にくいかもしれない。そこで、以下の方法を紹介する。facet_wrap() を用いた図の作成では、散布図 2のように color 引数を指定する必要はなく、p3 を再利用できる。geom_point() で散布図作成の指示を与えたあとに、facet_wrap() のレイヤーを重ねる指示を与えれば、散布図 3が作成される。 p3 + geom_point() + facet_wrap(~cut) + labs(x = &quot;カラット&quot;, y = &quot;価格&quot;, title = &quot;散布図 3: カット別、カラット：価格&quot;) 散布図 3をみると、基本的にはカラット数と価格には正の相関があるものの、カットの質が低い（例、Fair）場合にはばらつきが大きいことがうかがえる。 これまでに学んだdplyrによるデータ処理方法をパイプ演算子でつなげることで、特定の群のみを対象にした図示化も容易になる。ここでは例として、1.00カラット以上と未満とで分けて、それぞれのヒストグラムを作成してみる。 p5 &lt;- diamonds %&gt;% filter(carat &gt;= 1.0) %&gt;% ggplot(mapping = aes(x = price)) p5 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット以上&quot;) p6 &lt;- diamonds %&gt;% filter(carat &lt; 1.0) %&gt;% ggplot(mapping = aes(x = price)) p6 + geom_histogram() + labs(x = &quot;価格&quot;, y = &quot;頻度&quot;, title = &quot;ヒストグラム:1.00カラット未満&quot;) Rで図を作成したら保存（出力）したいと考えることも多いだろう。日本語を使っていない図はggsaveを使い簡単に保存できる。具体的には、まず、作成した図そのもの（図示化のためのggplot() オブジェクトではない）をオブジェクトとして定義（例、plot1）する。ggsaveの使用例は以下の様になる (以下は見本コード)。 ggsave(filename = &quot;plot1.pdf&quot;, plot = plot1, width = 10, height = 5, units = &quot;cm&quot;) 日本語を含む頭の場合、quartz() を用いた以下の手順を経て図を保存する。 1. quartz()で作図デバイスを起動する。 2. 作図デバイスを開いたまま、Rstudio内で図を表示する。 3. dev.off()という指示で作図デバイスを閉じることで図が保存される。 また、Rstudio内のplotタブから、クリック-バイ-クリックで実行することも可能である（Export -&gt; Save as Image/ Save as PDF -&gt; Directory -&gt; File name）。 "],["練習問題-2.html", "5.10 練習問題", " 5.10 練習問題 “reshape2” パッケージに含まれる“tips” データを使い以下の分析を実行しよう。 支出額とチップのヒストグラムを作成 支出額とチップの平均値を計算 支出額とチップの分散を計算 支出額とチップの共分散と相関を計算し、散布図を作成 "],["参考文献-3.html", "5.11 参考文献", " 5.11 参考文献 倉田博史・星野崇宏（2011）「入門統計解析」、新世社. 高橋将宜・渡辺美智子 (2017). 「欠測データ処理」, 共立出版. 松村優哉・湯谷啓明・紀ノ定保礼・前田和寛（2021）「改訂2版 RユーザのためのRStudio[実践]入門〜tidyverseによるモダンな分析フローの世界」，技術評論社. Healy, Kieran (2018) Data Visualization: A Practical Introduction, Princeton University Press. "],["test.html", "Chapter 6 基礎統計学復習 ", " Chapter 6 基礎統計学復習 "],["本章の概要-3.html", "6.1 本章の概要", " 6.1 本章の概要 データを集めて分析を行うことで、ある変数の平均値などの統計量を計算することが可能であることは前章で学んだ。しかし、その計算された数値にどのような意味があるのかを解釈するのが難しい場合もある。例えば、ある変数の平均値を異なるグループ（例えば、性別）それぞれで計算したとする。このとき、ほとんどの場合においてグループ間で同じ値を取ることはないと考えられる。しかしながら、この違う値が誤差の範囲なのか、意味のある（誤差を考慮しても無視できないほど大きな）差なのかについて検討することは重要である。このような目的を達成するために利用されるのが、、統計的な分析（区間推定や検定）である。 Rを用いて、統計的な分析（区間推定や検定）を実行すること自体はさほど難しくない。基本的な分析に必要な関数は基本パッケージに搭載されており、コードの書き方（引数の設定など）もネット上で検索すれば容易に知ることができる。しかしながら、自身もしくは他者が実行した分析をきちんと理解するためには、基礎的な統計学の内容を理解している必要がある。本章では、基礎統計学に関する説明を提示する。なお、本書で紹介する内容はあくまで簡易的な内容であるため、統計学を未習の場合は基礎統計学の図書を用いて学習することを強く推奨する。また、章末に統計学や計量経済学の学習に役立つ参考文献を提示しているので、各自の学習に役立ててほしい。 本章で扱う内容は主に、標本から得た情報に基づき母集団の性質について推測するアプローチを想定したものである。具体的には、以下の内容を含める： 確率モデルと期待値・分散 統計的推測と点推定・区間推定 統計的仮説検定 平均値に関する検定・分散分析 検定力分析とサンプルサイズ 本章の後半では様々な検定方法を紹介するが、本書ではこれらの分析手法について記憶するよりも、マーケティング領域の学生において特に誤解の多い統計的分析に関する基本的な性質について理解してほしいと考えている。第一に、確率変数によって定義された統計量もまた確率変数であるという点である。データを収集し、分析を実行し統計量を計算すると、自身が観察した値が唯一の値であると考えてしまうかもしれない。しかし、母集団とは確率分布であり、標本はその確率分布に従う確率変数、データはその確率変数の実現値だと解釈できる（倉田・星野、2011）。そのため、自身が観察した推定値もまた確率変数の実現値であるということを理解することが大切である。 第二に、信頼区間についてである。統計的な分析では、信頼区間の計算を行うことも多い。しかしながら、信頼区間は誤解のもと解釈される場合も多いため、注意が必要である。信頼区間は、未知パラメータ（母平均等）を一定の確率（信頼水準）で含む区間を示す。信頼区間には確率を割り当て、例えば95%信頼区間のようなものを計算するのだが、よくある誤った解釈として「母集団の期待値（未知パラメータ）は95%の確率でxx以上、yy以下という区間に含まれる」というものである。基礎的な統計学の枠組みでは、未知パラメータは確率変数ではない。そのため、未知パラメータに確率を割り当てるような表現は誤りである。確率変数であるのは、推定された区間である。 第三に、統計的仮説検定についてである。統計的仮説検定ではまず、母集団の統計的特徴に関する記述である「帰無仮説」と「対立仮説」を定める必要がある。特に帰無仮説は検定における分析や考察の基準となり、帰無仮説を棄却するか否かによって仮説検定の判断を行う。通常、「棄却する」という言葉は帰無仮説に対してのみ用いるため、論文の中で利用している理論仮説や作業仮説に対しては使わない。また、統計的仮説検定では、帰無仮説が真であるにも関わらず帰無仮説を棄却してしまう第一種の誤りと、帰無仮説が真ではないのにも関わらず、帰無仮説を採択してしまう第二種の誤りとが存在する。第一種の誤りを起こす確率は有意水準という。統計的仮説検定では、事前に設定した有意水準の分だけ（慣習として5%や1%といった基準が採用される）第一種の誤りを起こす確率を起用したうえで帰無仮説が正しいか否かを判断する。一方で、「帰無仮説を棄却できない」という結果を得た際には注意が必要である。具体的には、帰無仮説を棄却できない（統計的に有意でない）からと言って、帰無仮説が正しいと結論づけることはできない。そのため例えば、統計的に有意でない結果をもとに「〇〇は××に影響がない（もしくは、差がない）ことが明らかになった」という解釈を行うことは適切ではない。 第四にp値についての解釈である。Rや他のソフトウェアで統計的検定を実行すると “p-value”（p値）という値を得る。p値は「値が小さければ仮説が真であることを示す指標」や「小さいほど結果の重要性を示す指標」といった解釈を行うべきではない（Baker, 2016）。p値は、帰無仮説が正しいと仮定したときに，手元のデータから計算した検定統計量以上に極端な値をとる確率だと定義できる（豊田, 2017）。そしてp値が有意水準よりも小さい場合には、帰無仮説が間違っていたという判断を下すというのが、p値に基づく帰無仮説棄却の判断である。 本章では、上記の注意点について理解するための統計的な原理原則と、それに対応するRを用いた統計的分析手法を提示する。 "],["確率モデル期待値と分散.html", "6.2 確率モデル、期待値と分散", " 6.2 確率モデル、期待値と分散 6.2.1 確率と離散確率変数 伝統的なデータ分析では、標本から得た情報に基づき母集団の性質について推測する。母集団とは確率分布であり、標本はその確率分布に従う確率変数、データはその確率変数の実現値だと解釈できる（倉田・星野、2011）。そのうえで確率とは、起こりうる事象の集合内において、各事象の起こりやすさの度合いを0以上1以下の実数で表したものである。より詳細な定義として、標本空間を \\(\\small \\Omega\\)、任意の事象 A に対して実数 P(A) が定まっていて、以下の三つを満たすとき、P(A)は事象 A の確率という： 確率は非負であり、以下を満たす： \\[0\\leq P(A)\\leq 1\\] 全事象を \\(\\Omega\\)、空事象を \\(\\emptyset\\) とするとこれらの確率は以下の様に示される： \\[P(\\Omega)=1,~~ P(\\emptyset)=0\\] 事象 \\(A_1,A_2,...\\) が互いに排反ならば、これらのうち少なくとも１つが起こる事象 \\(A_1\\cup A_2\\cup ...\\) の確率は以下となる： \\[ P(A_1\\cup A_2\\cup ...)=P(A_1)+P(A_2)+... \\] 確率変数とは、ある標本空間上で定義される取りうる各事象に対してそれぞれ一定の確率と対応関係のあるような変数である。例えば、細工のないサイコロを投げるとき、出た目の値を \\(x\\) とすると、\\(x\\) は1から6までの整数を取りうる変数だと言い換えることができる。この場合標本空間は、取りうる出目に対応した6個の標本点からなる。またこれらの標本点には、それぞれ対応する確率が以下のように付与されている。 Table 6.1: サイコロの確率分布 x 1 2 3 4 5 6 確率 1/6 1/6 1/6 1/6 1/6 1/6 このように、確率変数の取る値に対応して確率が付与されるルール（ \\(x\\) の関数としての確率 \\(P(x)\\) ）を確率分布や確率分布関数という。確率変数は主に、離散確率変数と連続確率変数に分けることができる。離散確率変数は、サイコロのように、取りうる値が離散的な確率変数である。一方で、連続確率変数は、ある範囲の中で連続的にどんな値も取りうる確率変数である。離散確率変数では、サイコロの表で示されているように、取りうる特定の値に対応する確率を確率分布に基づき計算できる。 6.2.2 連続確率変数 一方で連続確率変数の場合、取りうる値の数が無限に存在する。例えば、-1 から 1 までの区間を取りうる値の範囲とする連続確率変数があったとする。この変数は例えば、0.90という値を取りうるが、同様に、0.91 や 0.900001 といった値も取りうる。このように、連続確率変数が取りうる値の数は無限に存在するため、取りうる特定の値に対応する確率は0になる。もし取りうる各値に確率が付与されていると、確率の合計が無限大になってしまうという問題に直面する。そのため、連続確率変数の場合、取りうる区間に対して確率が付与される。これを踏まえて連続確率変数を捉え直すと、連続確率変数は、その取りうる任意の区間に対して一定の確率が対応するような変数であるといえる。また、連続確率変数における取りうる区間の起こりやすさには「確率密度」が対応することで計算可能になる。言い換えると、確率変数 \\(x\\) の値に確率密度がどのように対応するのかという関係は、\\(f(x)\\) という確率密度関数（probability density function: PDF）として示される。PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) が区間 [a, b] を取る確率 \\(P(a\\leq x \\leq b)\\) は、以下の積分計算で求められる。 \\[ P(a\\leq x \\leq b)=\\int^b_a f(x) dx \\] 以下の図はPDFの例であり、図内の曲線はPDFを、灰色に塗られている面積はある区間の確率を示している。なお、上記の式で示されている関係から、PDFを特定（仮定）することで、ある確率に対応する区間 [a, b] を求めることも可能である。以降の節で紹介する統計的分析では、この関係を用いて分析することもあるが、詳しくは後述する。 連続確率変数例 連続確率変数を用いた具体的な確率計算例を紹介するために、ここでは一様分布（uniform distribution）を用いる。区間 [a, b] を持つ一様分布に従う確率変数 \\(x\\) のPDFは以下のように示される。 \\[ f(x) = \\begin{cases} \\frac{1}{b-a} &amp; a\\leq x \\leq b\\\\ 0 &amp; otherwise \\end{cases} \\] 具体的な計算を実行するために、ここで区間 [-1, 3] を持つ一様分布を考える。この一様分布のPDFは、\\(\\small f(x)=\\frac{1}{4}~for~-1\\leq x\\leq 3\\)（その他の区間の確率は0）となる。このとき、\\(x\\) が区間 [0, 2] を取る確率は、以下のように求められる。 \\[ P(0\\leq x \\leq 2) = \\int^2_0 \\frac{1}{4} dx=\\left[\\frac{x}{4}\\right]^2_0=\\frac{1}{2}-0=\\frac{1}{2} \\] 6.2.3 期待値 期待値を求める。期待値とは確率の考え方を含む理論的な平均値（\\(\\mu\\)）といえる。確率分布 \\(P(x)\\) を持つ離散確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\sum_x x \\cdot P(x)=\\mu \\] 一方、PDF \\(f(x)\\) を持つ連続確率変数 \\(x\\) の期待値 \\(E(x)\\) は一般的に以下のように定義することができる: \\[ E(x) = \\int_{-\\infty}^\\infty x \\cdot f(x) dx=\\mu \\] この定義に基づき、先程の区間 [-1, 3] を持つ一様分布の期待値を以下のように求める。 \\[ E(x)= \\int^3_{-1} \\frac{x}{4} dx=\\left[\\frac{x^2}{8}\\right]_{-1}^3=\\frac{9-1}{8}=1 \\] 期待値 \\(E(x)\\) は一般的に、\\(a\\) を定数、\\(g(x)\\) と \\(h(x)\\) を \\(x\\) の関数とするとき、以下が成り立つ： \\(E(a) = a\\) \\(E[a\\cdot g(x)]=a\\cdot E[g(x)]\\) \\(E[g(x)+h(x)]=E[g(x)]+E[h(x)]\\) これらの性質により、確率変数 \\(x\\) の分散（\\(\\sigma^2\\)）は以下のように求まる。つまり、分散は二乗の期待値から期待値の二乗を引くことで計算できる。 \\[ \\sigma^2=E\\left[\\bigl(x-E(x)\\bigr)^2\\right]=E\\left[(x-\\mu)^2\\right]=E(x^2)-E(x)^2 \\] "],["統計的推測.html", "6.3 統計的推測", " 6.3 統計的推測 前節で述べた通り、我々は研究の対象となる集団全体ではなく、その一部から情報を取得し分析を行う。このとき、その集団全体を母集団、母集団から抽出した一部を標本と呼ぶ。統計的な分析においては、確率分布を用いて母集団をモデル化し、標本をその確率分布に従う確率変数とみなすことで母集団と標本の関係を捉える。そのため、データ分析は標本を対象とするものの、分析者の関心は、母集団の特徴である母数（parameter）についてであることが多い。母集団の平均（\\(\\mu\\)）や分散（\\(\\sigma^2\\)）は母数の代表例である。しかし、母数は通常未知であり直接知ることはできないため、標本の情報を用いて母集団の特徴について推測する。このプロセスを統計的推測と呼ぶ。統計的推測を行うためには、原則として母集団からの無作為標本抽出（random sampling）が必要になる。統計的推測では、互いに独立に同一の分布に従う（Identically Independently Distributed: IID）ような標本が好ましく、無作為標本は、IIDを満たすことが知られている。 統計的推測においては、「推定」、「統計量」、「推定量」、「推定値」などの似たような言葉が利用されるが、これらはそれぞれ異なる意味を持つ。推定とは、標本の情報に基づき母数について把握しようとする作業そのものを示す。一方で、標本として得られるデータに基づき計算できる値（計算式）を一般的に統計量というのだが、その中でも特に推定に用いる統計量を推定量という。そして推定値は、推定量についてデータから求めた実際の計算値を表す。また、推定には「点推定」と「区間推定」がある。点推定とは、未知の母数について１つの数値に基づいて推定する方法である。例えば、標本平均は母平均（\\(\\mu\\)）を点推定するための推定量である。一方で区間推定は、未知の母数を一定の確率で含む区間を推定する方法である。これは、点推定では捉えきれない統計的誤差を考慮して区間を推定する方法であり、母平均の信頼区間の測定は区間推定の代表例である。 "],["点推定.html", "6.4 点推定", " 6.4 点推定 点推定は特定の推定量によって母数を捉えようとするが、どのような推定量を用いるべきなのだろうか。本節では、不偏性（unbiasedness）、一致性（consistency）、効率性（efficiency）という統計的に重要な推定量の性質について説明する。なお、以下の説明では、未知パラメータ \\(\\small \\theta\\)（シータ）に対する推定量 \\(\\small \\hat{\\theta}\\)（シータハット）を考える。不偏性とは、推定量の「期待値」が未知パラメータの真の値に等しいという性質であり、以下のように示すことができる。 \\[ E(\\hat{\\theta})=\\theta \\] つまり、実際の推定量の実現値がどうかは置いておいて、期待値の下では推定量が未知パラメータを示していることを表すものであり、サンプルサイズに関係のない推定量の性質である。そして、不偏性を満たす推定量のことを不偏推定量（unbiased estimator）という。なお、上記の定義から、統計的なバイアス（B）は、以下のように定義できる。 \\[ B=E(\\hat{\\theta})-\\theta \\] 第二に一致性とは、サンプルサイズが十分に大きいとき、推定量が未知パラメータの真の値と等しくなる確率が1に近づくという性質である。この性質について詳しく論じるには、漸近理論を学ぶ必要があるため、本書では詳細を省略するが、サンプルサイズを大きくすると未知パラメータの真の値に近づくような推定量を示した性質だと解釈できる。なお、任意の\\(\\small \\epsilon &gt;0\\)（\\(\\small \\epsilon\\): イプシロン）に対して以下のような性質を持つ推定量を一致推定量という。 \\[ \\lim_{n\\rightarrow \\infty} P\\left(|\\hat{\\theta}-\\theta|\\leq \\epsilon \\right)=1 \\] 第三に効率性は、推定量の分散の小ささを示している。分散の小さい推定量の方が、期待値から離れた値を取りにくく、好ましい推定量と考えられる。複数の不偏推定量や一致推定量がある場合、効率性を元に好ましい推定量を考える。 なお、代表的な推定量である標本平均は母集団期待値の推定量として好ましい性質（不偏性と一致性）も持っている。以下では、期待値 \\(\\mu\\)、分散 \\(\\sigma^2\\) の確率分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\)を考える（つまり、\\(\\small E(X)=\\mu\\), \\(\\small Var(X)=\\sigma^2\\)）。このとき、標本平均（\\(\\small \\bar{X}\\)）の普遍性は以下のように示すことができる。 \\[ E(\\bar{X})= \\left[\\frac{1}{n}(X_1+X_2+...+X_n)\\right] = \\frac{1}{n}~\\left[E(X_1)+E(X_2)+...+E(X_n)\\right] = \\frac{1}{n}\\cdot n\\mu=\\mu. \\] また標本平均の分散については、以下となることが知られている（計算は省略）。 \\[ Var(\\bar{X})=\\frac{\\sigma^2}{n} \\] 上記と同様の無作為標本による標本平均の一致性については、任意の \\(\\small \\epsilon&gt;0\\) に対していかが成り立つことが知られている。 \\[ \\lim_{n\\rightarrow \\infty}P(|\\bar{X}-\\mu|\\leq \\epsilon)=1 \\] 言い換えると、サンプルサイズが増えることで標本平均 \\(\\small\\bar{X}\\) は母集団の真の平均 \\(\\small \\mu\\) と等しくなる確率が1に近づく。なお、標本平均がもつこの特性は「大数の法則（Law of Large Number）」として知られている。 また、標本平均はその分布の収束に関しても重要な特性を持っている。ここで、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) を持つ確率分布に従う母集団からのn個の無作為標本 \\(\\small X_1,.., X_n\\) を考える。サンプルサイズが十分に大きい場合、 \\(\\small \\bar{X}\\sim N(\\mu,\\sigma^2/n)\\)（\\(\\small \\bar{X}\\) が平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布に従う）となることが知られている。この性質を「中心極限定理（Central Limit Theorem）」という（詳細な証明や定義は省略）。また、この定理を以下のような\\(\\small \\bar{X}\\) を標準化した確率変数に応用することも可能である。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] 中心極限定理より、サンプルサイズが十分に大きい場合、Z の分布関数は標準正規分布（N(0,1)）の分布関数に収束する。詳細については割愛するが、サンプルサイズが十分に大きい場合、「標本平均」や「標本平均を標準化した確率変数」の確率分布が正規分布や標準正規分布に近似できるという定理は、統計的な推定や検定において重要なものである。 また、母集団分散の推定量としては、不偏標本分散が使われる事が多い。上記と同じ無作為標本に対し、標本分散 \\(S^2\\) と、不偏標本分散 \\(s^2\\) は以下のように定義される。 \\[ S^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X}) \\] \\[ s^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X}) \\] そして、それぞれの推定量の期待値は以下のようになることが知られている（計算省略）。そのため、母集団分散の推定量として、不偏標本分散（\\(s^2\\)）が用いられる。 \\[ E(S^2)=\\frac{n-1}{n}\\sigma^2 \\] \\[ E(s^2)=\\sigma^2 \\] "],["推定量もまた確率変数.html", "6.5 推定量もまた確率変数", " 6.5 推定量もまた確率変数 次に、推定値と母数との関係を標本平均（\\(\\small \\bar{X}\\)）を使って考える（標本平均の定義は、「記述統計」の節を参照）。ある母集団からランダムサンプルを収集し、標本平均を計算することを考える。ここで計算された数値は真の母平均を捉えた唯一の値なのだろうか。 結論としては、点推定の推定値は母数そのものではなく、ひとつのある実現値でしかないことに注意が必要である。この理由は、「確率変数から計算される推定量もまた確率変数である」という事実から理解することができる。 例えば、我々が神戸大学経営学部生の一ヶ月あたりの平均収入（仕送りは除く）に関心があるとする。（実現可能性は置いておいて）経営学部全体を母集団とする無作為標本を100件収集し、標本平均を計算した結果 \\(\\small \\bar{X}=\\) 0 だったとする。もしこのような極端な結果を得た場合、多くの人が「標本平均の実現値は必ずしも真の母平均そのものではない」という説明に納得がいくだろう。同様の調査（100件のランダムサンプリング）をもう一度行い平均収入を計算し直すと、おそらく0とは異なる推定値を得る可能性が高い。仮に、\\(\\small\\bar{X}=\\) 50,000 だった場合、その結果をどのように解釈するだろうか。仕送りを除く大学生の月当たり収入の平均が5万円だという結果はなんとも尤もらしい。しかしながら、たとえ尤もらしい結果を得たとしても、それはひとつの分析結果であり、真の母平均を示す唯一の値ではない。 確率変数から計算される推定量もまた確率変数であるという点を直感的に経験するために、細工のない6面サイコロを（バーチャルに）振ってもらう。なお、コード内では、出力結果を少し見やすくするために、knitr というパッケージを利用しているため、以下のようにインストールしてほしい。 install.packages(&quot;knitr&quot;) 標本平均についての議論を行う前に、理論的な期待値を求める。6面サイコロの出目の期待値 \\(\\mu\\) は以下の通りである。 \\[ \\mu = 1\\cdot \\frac{1}{6}+2\\cdot \\frac{1}{6}+...+6\\cdot \\frac{1}{6}=3.5 \\] ここで、以下のようなコマンドを用いてR内でサイコロを振ってみてほしい（実際にサイコロを振ってもらっても構わないが、面倒くさい）。 set.seed(442)# 乱数の再現性確保のための指示。関数内の数字に特に意味はないため各自別の値を使っても良い。 die &lt;- 1:6 d &lt;- sample(die,size=1,replace = TRUE) d ## [1] 6 この講義ノート内では以上で示されている通り、6という出目を得た。上記のコマンドを実施した各自がそれぞれ異なる値を得ているだろう。ここで得た6という数字は、サイコロの出目という確率変数の実現値（\\(n=1\\)）である。そのため、本データの標本平均も6であり、真の期待値とは異なる。ただし、読者によっては1件の標本による標本平均という表現を直感的に理解しにくいかもしれない。そのため、以下のように サイコロを10回振る試行を3回実施し、各サンプリング結果に基づき標本平均を以下のように計算する。 set.seed(352) d1 &lt;- sample(die,size=10,replace = TRUE) d2 &lt;- sample(die,size=10,replace = TRUE) d3 &lt;- sample(die,size=10,replace = TRUE) d_mean &lt;- matrix(c(mean(d1),mean(d2),mean(d3)),nrow = 1) colnames(d_mean) &lt;- c(&quot;d1の平均&quot;, &quot;d2の平均&quot;, &quot;d3の平均&quot;) knitr::kable(d_mean, caption = &quot;サイコロの標本平均比較&quot;, align = &quot;ccc&quot;) Table 6.2: サイコロの標本平均比較 d1の平均 d2の平均 d3の平均 3.2 3 2.4 上記の通り、d1, d2, d3 いずれの標本平均も互いに異なるものであり、また3.5とも異なる。このことからも、確率変数（サイコロの出目）を用いて計算された推定値（標本平均）もまた確率変数であり、推定値と未知パラメータとの間にはズレ（誤差）が生じうることがわかる。なお、中には3.5と等しい標本平均を偶然得た読者もいると考えられるが、それもあくまで一つの実現値である。 では、標本平均の推定値がサンプルサイズによってどれだけ真の期待値に近づくのかについて、サイコロの試行回数を10回、100回、1,000回と増やして確認する。以下の結果を見ると、サンプルサイズ（試行回数）が増えるごとに真の期待値に近づいていることが伺える。ただし、これらの結果もあくまで確率的な試行結果の実現値である。そのため、読者によっては異なる傾向を示すような結果を得る可能性があることに注意が必要である。 set.seed(541) d10 &lt;- sample(die,size=10,replace = TRUE) d100 &lt;- sample(die,size=100,replace = TRUE) d1000 &lt;- sample(die,size=1000,replace = TRUE) d_lln &lt;- matrix(c(mean(d10),mean(d100),mean(d1000)),nrow = 1) colnames(d_lln) &lt;- c(&quot;10回試行の平均&quot;, &quot;100回試行の平均&quot;, &quot;1,000回試行の平均&quot;) knitr::kable(d_lln, caption = &quot;サイコロの標本平均比較２&quot;, align = &quot;ccc&quot;) Table 6.3: サイコロの標本平均比較２ 10回試行の平均 100回試行の平均 1,000回試行の平均 3 3.36 3.516 "],["補足いくつかの確率分布の関係性.html", "6.6 補足（いくつかの確率分布の関係性）", " 6.6 補足（いくつかの確率分布の関係性） 統計的な分析の際によく用いられる確率分布として、正規分布、カイ二乗分布、t分布、F分布間の関係性について簡単に紹介する。なお、各分布の確率密度関数などは記載しないため、関心のある読者は参考文献を参照してほしい。 6.6.1 正規分布 正規分布は、様々な分布の基準として用いられる重要な分布である。期待値を中心に左右対称であり「ベルカーブ」と言われる形状を持つ。また、平均0、分散1の正規分布は特に標準正規分布と言われ、正規母集団からの無作為標本の標本平均等の分布を特定する際などに用いられる。 6.6.2 カイ二乗分布 標準正規分布からの無作為標本の二乗和はカイ二乗分布に従う。カイ二乗分布は、正規母集団からの無作為標本の不偏標本分散の分布を特定する際などに用いられる。 6.6.3 t分布 標準正規分布とカイ二乗分布の比はt分布に従う。t分布は、正規母集団からの無作為標本による標本平均と不偏標本分散の比の分布を特定する際などに用いられる。 6.6.4 F分布 カイ二乗分布の比はF分布に従う。F分布は、異なる正規母集団からの無作為標本の不偏標本分散の比の分布を特定する際などに用いられる。 "],["区間推定.html", "6.7 区間推定", " 6.7 区間推定 点推定の節で示した通り、推定値と未知パラメータの間には、ずれ（誤差）がある。標本平均の様に好ましい性質（不偏性や一致性）を持つ推定量であっても、計算の結果示された一つの推定値がどの程度信頼できるものなのかはわからない。そこで区間推定という、未知パラメータ（母平均等）を一定の確率（信頼水準）で含む区間を計算する方法を用いて、統計的な誤差を加味した母数への検討を試みる。区間推定においては、「信頼水準zz%で、xx以上、yy以下という区間は真の母数を含む」という区間[xx, yy]を調べる。このような区間は信頼区間（confidence interval）と呼ばれ、多くの統計分析において用いられている。 では、ここで示された区間がどのように計算され、どのようなことを意味するのだろうか。以下では、一度電球の例から離れ、もう少し一般的な形で信頼区間の導出や解釈を説明する。 はじめに、標準正規分布に基づくある区間の確率の求め方を説明する。 \\(Z_1,Z_2,...,Z_n\\) は、N(0, 1) （標準正規分布）に従う母集団からの無作為標本とする。このとき、標準正規分布がある区間 [\\(-\\infty,~z_\\alpha\\)] をとる確率は、以下の式および図のように示すことができる15。なお、\\(\\small z_\\alpha\\) は、確率\\(\\small \\alpha\\) に対応する標準正規分布上の上側確率 \\(\\small \\alpha\\) 点とする。このとき、この分布における \\(\\small z_\\alpha\\) 以下（以上）の範囲を取る確率は 1 \\(\\small -\\alpha\\)（\\(\\small \\alpha\\)） である。 \\[ P(Z\\leq z_\\alpha) = \\int^{z_\\alpha}_{-\\infty}~f(z)~dz =\\int^{z_\\alpha}_{-\\infty}~\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right)~ dz = 1-\\alpha \\] 標準正規分布と確率計算 同様に、以下のような関係も捉えることができる。この場合、斜線部で示されている範囲の確率は両側合わせて \\(\\small \\alpha\\) であり、その内側の確率は 1 \\(\\small - \\alpha\\) である。 \\[ P(-z_{\\alpha/2} \\leq Z\\leq z_{\\alpha/2}) = \\int^{z_{\\alpha/2}}_{-z_{\\alpha/2}}~f(z)~dz = 1-\\alpha \\] 標準正規分布と両側確率 上述の関係を、区間推定に応用するために、あるデータの標本平均に関する議論を捉える。標準正規分布に従う確率変数は、正規分布に従う確率変数を標準化することで得ることができる。ここで、\\(\\small X_1,...,X_n\\) を、期待値 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2\\) の正規分布に従う母集団からの無作為標本とする。これまで学んだ標準化および標本平均の特性から、以下の通り、標本平均を標準化したものは標準正規分布に従うことがわかる。 \\[ \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] このことから、先述の標準正規分布における確率計算の関係を応用し、以下を得る。 \\[ P\\left(-z_{\\alpha/2}\\leq \\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\leq z_{\\alpha/2}\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+z_{\\alpha/2}\\cdot \\frac{\\sigma}{\\sqrt{n}}\\right)=1-\\alpha \\] したがって、区間 [\\(\\small \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n}\\)] は、確率 \\(\\small 1-\\alpha\\) で未知の母平均 \\(\\small \\mu\\) を含むと解釈できる。また、上記の関係から任意の確率 \\(\\alpha\\) を指定することで、区間の上限と下限（\\(\\pm z_{\\alpha/2}\\)）の具体的な値を（統計学テキスト巻末などに記載されている）標準正規分布表などから求めることができる。 そして、このような区間を「信頼区間」といい、信頼区間の計算にて仮定された確率 \\(\\small 1-\\alpha\\) を「信頼水準」もしくは「信頼係数」という。信頼係数は、信頼区間の計算のために研究者によって事前に選択される。慣習としては、90%, 95%や99% (\\(\\small \\alpha =\\) 0.10, 0.05, 0.01)を用いる事が多い。なお、信頼係数を大きくすると、信頼区間も広くなる。 上記の区間推定は母分散 \\(\\sigma^2\\) が既知である場合に計算可能であるが、多くの場合母分散は未知である。そのような場合には、自由度 n-1 の「t分布」を用いて、両端の確率 \\(\\small \\alpha\\) 点を \\(\\small t_{\\alpha/2}(n-1)\\) とする信頼区間を求める。\\(N(\\mu,\\sigma^2)\\) に従う母集団からの無作為標本を考えるが、今回は母分散が未知である場合を仮定する。このような場合は、母分散 \\(\\small \\sigma^2\\) のかわりに母分散の不偏推定量である不偏標本分散 \\(\\small s^2\\) を用いた以下の統計量 t をもとに信頼区間を計算する。このとき、統計量 t は自由度 n-1 の t 分布に従うことが知られている（t分布に関する詳細および証明は省略）。 \\[ t=\\frac{\\bar{X}-\\mu}{\\sqrt{s^2/n}}\\sim t(n-1) \\] ここで、先述の標準正規分布に基づくある区間の確率計算と同様の計算を、自由度 \\(\\small n-1\\) のt分布に基づき実行すると、以下のような確率と区間の関係に書き換えることができる。 \\[ P\\left(-t_{\\alpha/2}(n-1)\\leq \\frac{\\bar{X}-\\mu}{s^2/n}\\leq t_{\\alpha/2}(n-1)\\right)=1-\\alpha \\] 上記の式に基づき、未知の母平均 \\(\\small \\mu\\) についての区間として整理すると、以下の式を得る。 \\[ P\\left(\\bar{X}-t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\leq \\mu \\leq \\bar{X}+t_{\\alpha/2}(n-1)\\cdot \\frac{s}{\\sqrt{n}}\\right)=1-\\alpha \\] 信頼区間を求めるための手順は標準正規分布の場合もt分布の場合も同様だが、標準正規分布のかわりに t 分布を用いた場合、特定の確率に対応する閾値が変わることが知られている（\\(\\small z_{\\alpha/2}\\neq t_{\\alpha/2}(n-1)\\)）。 t分布は、左右対称であり標準正規分布よりもテールが厚いという特徴を持つが、自由度が大きくなると正規分布に近づくことが知られている。標準正規分布と、自由度の異なる t 分布との関係は以下のように図示化できる。自由度（df）3の t 分布よりも自由度20の t 分布のほうが標準正規分布に近い形状であることが伺える。 標準正規分布と t 分布によってある確率に対応する閾値が異なるということは、ある信頼水準に対応する信頼区間も仮定する分布によって異なるということである。 6.7.1 Rによる区間推定 Rで信頼区間を求める事自体は難しくない。最も手間のかからない方法としては、t.test() の分析結果を用いて、conf.int()によって信頼区間が計算できる。信頼区間の計算を実行するために、倉田・星野（2011, p.248）で提示されている以下の電球の製品寿命に関する例を考える。ある製品（電球）の寿命は平均1700（時間）である。企業は性能を改良するために新型の電球が開発したが、新型化に伴い製品寿命も変化したのかについては不明である。ただし、この製品の寿命は新型も旧型のものも正規分布に従い、その標準偏差は \\(\\sigma=\\) 180（時間）であると仮定する。 工場で生産された新型製品を16個無作為に選びその寿命を計測した所、以下の結果を得た。 1873 1685 2275 1760 1769 2176 1748 1760 1994 1473 1715 1771 1784 1684 2038 1850 このデータは、平均が \\(\\small \\mu\\)、分散が \\(\\small 180^2\\) である正規分布（\\(\\small N(\\mu, \\sigma^2=180^2)\\) と表記する）からの無作為標本 \\(X_1,..., X_{16}\\)の実現値とみなすことができる。なお、このデータの標本平均は1,835（時間）、不偏標本標準偏差は 200である。このデータに基づく、新型電球寿命の期待値に関する95% 信頼区間（95%の確率で真の母数を含む区間）はt.test() 関数を用いると以下の様に求まる（ただし後述するが、この方法はこの例に対しては適切ではない）。 bulb &lt;- c(1873, 1685, 2275, 1760, 1769, 2176, 1748, 1760, 1994, 1473, 1715, 1771, 1784, 1684, 2038, 1850) bulb_ci &lt;- t.test(bulb) #t検定の実施と格納 bulb_ci$conf.int #信頼区間の出力（デフォルトで95%信頼水準） ## [1] 1728.235 1941.140 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 出力されている [1] 1728.235 1941.140 が信頼区間、## attr(,\"conf.level\") ## [1] 0.95 が今回計算に用いられた信頼水準（confidence level）（もしくは信頼係数（confidence coefficient）ともいう）である。 分析の結果、95%の確率で真の新製品寿命期待値が 1728.2 から 1941.1 の間に含まれることがわかった。したがって、どうやら新製品寿命は平均的に旧型製品（1,700）よりも長そうである。 しかしながら、新型電球の例では正規分布を仮定しているため、t分布を仮定するよりも標準正規分布により信頼区間を求めるほうが好ましい。 先述の通り、標準正規分布に基づく信頼区間は以下のように示すことができる。 \\[ \\bar{X}\\pm z_{\\alpha/2}\\cdot \\sigma/\\sqrt{n} \\] このとき、仮定より \\(\\small \\bar{X}=1835\\), \\(\\sigma=180\\) であることがわかっている。また、慣習より95%信頼水準を仮定すると、確率 \\(\\small \\alpha = 0.05\\)となる。そのため、区間推定の計算で必要な要素のうち現時点で不明なのは、 \\(z_{\\alpha/2}=z_{0.025}\\)の値である。この値は、任意の確率に対応する区間の閾値を表している。今回の場合、分布が左右対称の分布であり正負どちらか一方の値さえ分かればよいため、閾値（\\(z_{0.025}\\)）以上の区間を取る確率が2.5%になるような閾値に着目する。このような閾値は、Rによって以下のように求める（なお先述の通り、統計学教科書に掲載されている分布表を使っても同様の数値を求めることができる）。 qnorm(0.025, lower.tail=FALSE) ## [1] 1.959964 これにより、計算に必要な情報が揃ったため、以下の要領で信頼区間を出力できる。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 #信頼区間の計算 upper &lt;- xbar+z*(sigma/sqrt(n)) lower &lt;- xbar-z*(sigma/sqrt(n)) #結果のまとめと出力 ci.bulb &lt;- matrix(c(lower,upper),nrow=1) colnames(ci.bulb) &lt;- c(&quot;ci.lower&quot;, &quot;ci.upper&quot;) knitr::kable(ci.bulb, caption = &quot;Bulb data CI（95%）&quot;, align = &quot;cc&quot;) (#tab:ci_bulb)Bulb data CI（95%） ci.lower ci.upper 1746.489 1922.886 分析の結果、新型製品の平均寿命に関する95%信頼区間は、標準正規分布と t 分布どちらの分布を仮定しても旧型の平均1700（時間）を含まず、それよりも大きい値を取るものであった。したがって、新型製品は製品寿命の面においても95%の確率で旧型製品よりも優れていると考えられる。 では、この95%信頼区間は、そもそもどのように解釈すべきだろうか。結論から述べると、95%信頼区間の直感的解釈については以下のように説明できる： 「母集団からサンプルを取り平均値の95%信頼区間を構築する」という手順を100回繰り返すと考える。95%という信頼水準（確率）が示していることは、計算された区間が100回に95回は母数を含むということである。言い換えると、今回得た標本平均に基づき計算された信頼区間がはずれ（真の母数を含まない区間）である可能性が5%存在するということである。 95%信頼区間の解釈として、「分析対象としている母数の値がこの区間の値をとる確率が95%である」という旨の説明を行う人がいるが、これは\\(\\color{red}{\\text{誤り}}\\)である。母数は未知だが、何かしらの定まった値なので確率的な議論を母数に用いるのは適切ではない。確率的に変動するのはあくまで区間の両端である点を理解しなければならない。なぜならば、\\(\\small \\bar{X}\\) が確率変数であるため、そこから計算される区間の両端もまた確率変数となるためである（岩田，1996）。ここで示されている信頼水準は、計算された区間が真の母数を含んでいる確率である。つまり、信頼水準は、サンプルを収集し、信頼区間を求めるという「手順そのものに対する信頼度」を表す指標であると解釈できる。 また、分析の結果、t.test によって出力された信頼区間よりも短い区間を得た。これは、t 分布のほうがテールが厚く、中心より遠い値を取る確率密度が高いことから、95%水準における臨界値が標準正規分布に比べて大きくなるためである。また、t 分布の自由度が大きくなるにつれ、正規分布との差が小さくなる。 ただし、標準正規分布の確率密度関数 \\(\\small f(z)\\) は、\\(\\small 1/\\sqrt{2\\pi}\\exp\\left(-z^2/2\\right)\\) だと知られている。↩︎ "],["統計的仮説検定.html", "6.8 統計的仮説検定", " 6.8 統計的仮説検定 6.8.1 検定の手順と仮説 データを用いた研究では、統計的分析によって提示した仮説が支持できるか否かを判断したいという目的を持つこともある。その時に用いられる方法が統計的仮説検定である。本節ではまず検定の手順について説明したあと、分析結果の意味や解釈について説明する。統計的仮説検定は、基本的に以下の手順によって実施される。 仮説（帰無仮説・対立仮説）を設ける。 仮説を検定するための統計量を選ぶ。 統計量の値について、有意確率に基づく臨界値を設定する。 帰無仮説が正しいと仮定した上で統計量を計算し、その値が棄却域と採択域のどちらの領域に入るかを分析する。 統計的仮説検定で重要になる仮説は、帰無仮説と対立仮説である。理解を容易にするために、「問題と分析をつなげる仮説の提示」という節で議論した仮説を「作業仮説」と呼ぶ。作業仮説は、リサーチクエスチョンに答えるための論理的予測である。例えば、「女性に比べ男性の方が新製品購買意図が高い。」のような予測が考える。このような仮説を検証する場合、男女（グループ）間で購買意図の平均値を比較することが現実的な分析方法として考えられる。 帰無仮説と対立仮説は、統計的仮説検定の基準になる母集団の統計的特徴に関する仮説であり、検定という手続き上ではこれらの仮説に着目する。特に、帰無仮説は、統計的仮説検定の考察、分析の基準となる仮説であり、この仮説を棄却（否定）できるか否かを調べることが基本的な統計的仮説検定の枠組みだと言える。帰無仮説は棄却しうる仮説であり、\\(\\small H_0\\) という記号で表される事が多い。また、多くの場合において「差がない」、「効果がない（0である）」や、「特定の値と等しい」といった仮説が設計される。一方で対立仮説は、帰無仮説とは排反な仮説であり、帰無仮説が棄却された際に採用される推測であり、\\(\\small H_1\\)や\\(\\small H_a\\)という記号で表される。データ分析を用いた研究においては対立仮説と作業仮説は論理的に整合的ないしは等しいことが好ましい。つまり、作業仮説という研究上重要な論理的推測を検証するために、その作業仮説とは排反な帰無仮説を設計し統計的仮説検定を実施する必要がある。それによってもしその帰無仮説が棄却されたならば、対立仮説ひいては作業仮説がデータ分析によって支持されたと解釈することが可能になる。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_m\\)、女性における購買意図の期待値を \\(\\small \\mu_f\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\(H_0:~\\mu_m=\\mu_f\\) \\(H_1:~\\mu_m\\neq\\mu_f\\) \\(\\small H_0\\) は、男性における購買意図の期待値と女性における期待値が等しいというものであり、 \\(\\small H_1\\)はそれらが等しくないということを示している。そのため上記の二つの仮説は、どちらも未知パラメータについての関係を捉えており、\\(\\small H_0\\) と \\(\\small H_1\\) は互いに排反であることがわかる。その上で、もし帰無仮説が棄却され、男性の平均値のほうが女性よりも高い場合には、作業仮説が支持されたと解釈することができる。つまり統計的な検定においては、作業仮説として提示している推測を直接検証するのではなく、作業仮説と排反な帰無仮説を設計し、それが棄却されるならば暫定的に作業仮説の主張を指示しようという立場で検証を行う。なお対立仮説として\\(\\small H_1:~\\mu_m&gt;\\mu_f\\) を設定することも可能である。このような仮説に基づく検定方法は片側検定と呼ばれ、その詳細については後述する。 ここで改めて、より一般的な形でマーケティングリサーチで用いられる仮説と検定で用いられる仮説との関係を整理する。 分析上の基準である帰無仮説は何かをきちんと理解し定義する。 それが棄却された際にはどのような結論（対立仮説）が採用されるのかを理解する。 そしてその結論が自身の立てた作業仮説と帰無仮説・対立仮説の関係が整合的かを考える。 言い換えると、自身の立てた作業仮説を帰無仮説・対立仮説の対比という分析手続きで証明できるような調査・分析法を採用する必要がある。ただし、レポートや論文には、帰無仮説・対立仮説を記載せず、作業仮説のみを記載することがほとんどである。 6.8.2 母平均の検定（両側検定）に関する理屈 ここで改めて、標準正規分布を用いた母平均の検定に着目し、統計的仮説検定に関する一般的な説明を提示する。\\(\\small X_1,..,X_n\\) を正規母集団 \\(N(\\mu, \\sigma^2)\\) からのサンプルサイズ n の無作為標本とする。このとき、帰無仮説の下でのパラメータの値を\\(\\mu_0\\)として、以下の帰無仮説と対立仮説を設計する。 \\[ H_0:~\\mu=\\mu_0,~~~H_1:~\\mu\\neq\\mu_0 \\] このとき、「区間推定」節の信頼区間の説明でも述べた通り、正規分布に従う母集団からの無作為標本 \\(\\small X_1,...,X_n\\) の標本平均は以下の分布に従うことがわかっている16。 \\[ \\bar{X}\\sim N\\left(\\mu,\\frac{\\sigma^2}{n}\\right) \\] また、これまでの議論の通り、\\(\\small \\bar{X}\\)を標準化した統計量Zは以下の分布に従うことが知られている。 \\[ Z=\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim N(0,1) \\] 統計的仮説検定においては、この標準化された統計量を検定統計量（検定に用いる統計量）として用いて計算を行うのだが、我々の関心の中心でもある \\(\\small \\mu\\) は未知であり、通常この統計量を計算することはできない。すなわち、未知であるパラメーターを何かしらの値で代替しなければ、上記の検定統計量は計算できない。そこで、統計的な仮説検定では、「帰無仮説が正しいと一旦仮定」した上で統計量を計算するというプロセスを経る。言い換えると、未知のパラメーターについて帰無仮説で示されている値を代入することで、検定統計量を計算可能にする。したがって、検定統計量 Z を以下のように定義する。 \\[ Z=\\frac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n}} \\] そして、もし「帰無仮説が正しければ」Zは標準正規分布に従うはずであり、言い換えると Z の計算結果は0に近い値を取る可能性が高いはずである。そこで、この Z を計算し、\\(\\small |Z|\\) がある閾値 c よりも大きい（十分に0から離れている）場合には帰無仮説を棄却する。なお、ここで用いる閾値 c のことを一般的に臨界値と呼ぶ。つまり、検定統計量 Z の計算結果に対して、以下の方針で仮説検定を行う（ただし、以下の表記は臨界値 c を特定していない場合の記述である）。 \\[ \\begin{cases} |Z|&gt;c &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq c &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 臨界値 c の求め方は区間推定と同様、分析に対応する確率分布（今回であれば標準正規分布）に基づくある区間の確率計算で求まる。研究者はまず、任意の確率 \\(\\small \\alpha\\) を決める。この確率は「有意水準（significance level）」と呼ばれ、この有意水準と標準正規分布に基づく確率計算によって臨界値（下図内では \\(\\small \\pm z_{\\alpha/2}\\)）を求める。その上で、統計量の計算結果が臨界値より外側（下図における斜線部）にある場合には帰無仮説を棄却する。そのため、斜線部のような領域を棄却域、確率 \\(\\small 1-\\alpha\\) に対応する範囲を採択域と一般に呼ぶ。 臨界値と確率計算 今回の場合、有意水準 \\(\\small \\alpha\\) に基づく両側臨界値 \\(\\pm z_{\\alpha/2}\\) を設定し、以下の方式で検定する。 \\[ \\begin{cases} |Z|&gt;z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |Z|\\leq z_{\\alpha/2} &amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] 6.8.3 片側検定の紹介 ここまでの議論では対立仮説を \\(H_1:~\\mu\\neq\\mu_0\\) とし、左右対称の分布の両端に棄却域を設定した。このような検定方法を一般的に両側検定と呼ぶ。しかし、現実的ないしは理論的な根拠をもとに、ある値よりも高い（もしくは低い）ことが事前に予測できる場合がある。その場合には、例えば \\(\\small \\mu&gt;\\mu_0\\) や \\(\\small \\mu&lt;\\mu_0\\)といった対立仮説を設定することも可能である。このような対立仮説を利用した検定方法を一般的に片側検定と呼ぶ。ここでは、仮に\\(\\small \\mu&gt;\\mu_0\\)という対立仮説を立てた場合を考えるが、\\(\\small \\mu&lt;\\mu_0\\)のような対立仮説を設計しても正負を入れ替えることで同様の議論ができる。なお、片側検定において帰無仮説が棄却された場合、直ちに帰無仮説の値よりも大きい値を取ると判断する。しかしながら、たとえ異なる対立仮説を提示しても、採用する検定統計量や帰無仮説に基づく分布の仮定などは同じである。 片側検定を利用した場合の特殊性はその棄却域に現れる。片側検定の場合の棄却域は以下の図のように片側のみとなる。なお、その場合分布の両端に棄却域を設ける必要がないため、正の方向に \\(\\small \\alpha\\) 分の棄却域を設定する。 片側検定（正の場合） 6.8.4 有意水準と検定における誤り ここまでの内容をまとめると、帰無仮説を仮定して検定統計量を計算する場合、帰無仮説が正しければ、棄却域内の値を取る確率は \\(\\small 100\\times \\alpha\\)%であると言える。そして、検定統計量の計算結果が棄却域に含まれる場合、帰無仮説を棄却するという判断を下す。そのため、統計的に帰無仮説を棄却したからと言って、その結果が必ず正しいとは言い切れない。統計的検定には、根本的に第一種の誤り（Type 1 error）と第二種の誤り（Type 2 error）という二種類の誤りの可能性が内包されている。 第一種の誤りとは、帰無仮説が真であるにも関わらず、帰無仮説を棄却してしまう誤りである。一方で第二種の誤りは、帰無仮説が真ではないのにも関わらず、帰無仮説を採択してしまう誤りである。例えば、ある薬に期待される効果があるかどうかを検証する場合を考える。この時、帰無仮説は「投薬による効果がない」、対立仮説は「投薬による効果がある」と設計する。この場合における第一種の誤りとは、「本当は効き目のない薬を効くと判断してしまう誤り」であり、第二種の誤りとは、「本当は効き目のある薬を聞かないと判断したしまう誤り」である。 Table 6.4: 検定の誤り H0が真 H0が偽 H0を棄却 Type 1 error ✓ H0を採択 ✓ Type 2 error どちらの誤りも見過ごすことのできないものではあるが、第一種の誤りによる損失と、第二種の誤りによる損失を比較し、一般的な統計的検定においては、第一種の誤りの確率を下げることに注視する。なお、研究によっては下記にある検定力という指標に着目し、第二種の誤りに対応した議論を提示することもあるが、本書では割愛する。仮説検定では特に、第一種の誤りの確率を有意水準 \\(\\small \\alpha\\) と設定し分析する。また有意水準は、先述の通り棄却域の特定に用いられる。つまり、統計的仮説検定とは、帰無仮説が正しいと仮定した上で有意水準 \\(\\small \\alpha\\) の分だけ第一種の誤りの確率を許容したうえで仮説が正しいか否かを確認する作業である。 上記の統計的検定に関わる誤りは、\\(\\small T_0\\) は統計量 T の観測値、Rは\\(\\small H_0\\) の棄却域、Aは\\(\\small H_0\\)の採択域とし、以下のように示される。 有意水準: \\(\\alpha\\) \\[ P(T_0\\in R|H_0~\\text{is True})=\\alpha \\] 第二種の誤りの確率: \\(\\beta\\) \\[ P(T_0\\in A|H_0~\\text{is False})=\\beta \\] 検定力: \\(1 - \\beta\\) \\[ P(T_0\\in R|H_0~\\text{is False})=1-\\beta \\] ここで、仮説検定に関わる有意水準を \\(\\small \\alpha=0.05\\) と設定し、適応する。帰無仮説が正しいという条件のもとで、帰無仮説を棄却する確率であるため、有意水準は以下のように示すことができる。 \\[ \\alpha=P(|Z|&gt;c|\\mu=\\mu_0) =0.05 \\] しかしながら、このままだと確率計算が複雑になるため、上式を以下のように書き換える。 \\[ 1-\\alpha=P(|Z|\\leq c|\\mu=\\mu_0) = \\int^c_{-c}f(Z)dZ~_{|\\mu=1700}=0.95 \\] 再掲になるが、上式の関係を表した図が、以下のものになる。 標準正規分布の両側検定 6.8.5 検定結果に関する解釈 このとき、検定統計量 Z は帰無仮説が正しければ標準正規分布に従うはずである。したがって、臨界値 \\(\\pm c\\) は、\\(\\pm z_{0.025}\\) として分布表などより導出が可能である。Rにおいては前節と同様、qnorm() により、\\(z_{0.025}=1.96\\) だとわかる。すなわち、検定統計量の計算結果が 1.96（-1.96）を上回る（下回る）場合には、帰無仮説を棄却するが、そこには第一種の誤りを犯す確率が5%残されていると解釈できる。 一方で、統計的に有意でない（帰無仮説を棄却できない）結果を得たときには、その解釈について注意が必要である。具体的には、統計的に有意でないからと言って、帰無仮説が正しい（つまり \\(\\small \\mu =\\mu_0\\) である）と結論づけることはできない。ここまでの説明の通り、有意水準とは第一種の誤りを犯す確率であり、有意水準に基づく統計的検定では主にこの確率に対応した分析を行っている。そのため、第二種の誤りである、本当は \\(\\small \\mu \\neq \\mu_0\\) であるにも関わらず、\\(\\small \\mu =\\mu_0\\) と判断している可能性については未対応である。これらの点から「有意でない」ということを理由に、帰無仮説が正しいと結論づけることは適切ではない。 なお、ここまでの議論のように母集団の分散が既知の場合、検定統計量は標準正規分布に従うと仮定できる。しかし母集団の分散が未知の場合は、信頼区間での議論と同様、標準偏差の不偏推定量を用いて、自由度 n-1 の t 分布を仮定した分析を行う。そして、t分布に基づく母平均に関する検定を一般的に「t検定（t-test）」と呼ぶ。 より実践的な視座から統計的な検定において解釈に注意が必要な点として、p-value（p値）がある。 t.test() を用いた分析例でも紹介するが、R (他のソフトウェアでも)で統計的検定を実行すると “p-value”（p値）という値を得る。p値は、帰無仮説が正しいと仮定したときに，手元のデータから計算した検定統計量以上に極端な値をとる確率だと定義できる（豊田, 2017）。そしてp値が有意水準よりも小さい場合には、帰無仮説が間違っていたという判断を下すというのが、p値に基づく帰無仮説棄却の判断である。p値については、「値が小さければ仮説が真であることを示す指標」や「小さいほど結果の重要性を示す指標」といった解釈を行うこともあるが、このような解釈は不適切である（Baker, 2016）。 棄却域と有意水準の関係に基づき述べると、以下のように説明することができる（cf. 西山など, 2019）。有意水準（帰無仮説が正しいが帰無仮説を棄却する確率）を小さく取ると、棄却域は狭くなる。例えば、ある仮説検定において、5%有意水準では帰無仮説を棄却できるが、1%ではできない場合がある。計算された検定統計量の実現値に基づき、有意水準を変えながら検定を行っていくと、これ以上有意水準を小さくすると帰無仮説が棄却されなくなるという有意水準の限界を見つけることができる。この限界をp値と呼ぶ。そのため、p値によって示されている確率は有意水準と同様のものを捉えているのだが、その計算過程が異なるという点において注意が必要である。 6.8.6 電球データと検定 先程の新型電球の例を再度使い母平均の検定を実施する。新型電球について我々が関心を持っていたのは、新型電球の製品寿命が旧型の寿命（1700時間）より長いか否かである。そのため、新型電球の製品寿命の期待値を \\(\\small \\mu\\) とすると、帰無仮説と対立仮説は以下のように設計できる。 \\(H_0:~\\mu=1700\\) \\(H_1:~\\mu\\neq1700\\) 改めて以下の通り、新型電球に関する16個の無作為標本から得た製品寿命の平均値を計算すると、\\(\\small \\bar{X}=1835\\) であった。では、この1835は 1700 から十分に離れていると言えるのだろうか？もし、十分に離れていると判断されれば帰無仮説を棄却するが、この差が十分でなければ帰無仮説を採択する。 mean(bulb) ## [1] 1834.688 Rを用いて統計的検定を実行すること自体は難しくない。まず、t検定に基づく母平均の検定は t.test() で実施することが可能である。母平均が特定の値を取るか否かについての検定では、mu= という引数を使って帰無仮説に対応する値を指定する。今回の分析に関するコマンドおよびその結果は以下のとおりである。 t.test(bulb, alternative = &quot;two.sided&quot;, mu = 1700) ## ## One Sample t-test ## ## data: bulb ## t = 2.6968, df = 15, p-value = 0.01657 ## alternative hypothesis: true mean is not equal to 1700 ## 95 percent confidence interval: ## 1728.235 1941.140 ## sample estimates: ## mean of x ## 1834.688 分析結果の t= と df = はそれぞれt値（検定統計量の推定値）と自由度を表している。p-valueはp値と呼ばれるある確率を表しており、先述の通り、注意の必要な指標である。また、t.test() は、信頼区間や標本平均も出力してくれるため、これらの結果に基づき解釈を行うことも可能である。帰無仮説の棄却に至るp値の基準は慣習的に、0.10（10%）、0.05（5%）、0.01（1%）が用いられる。今回の結果では、p値が0.016であり、5%水準で帰無仮説を棄却することができるため、新型電球の寿命は旧型（1700時間）よりも有意に高いと結論づけることができる。 上述の t.test() は母分散が未知である際に用いられる検定方法である。この点は、信頼区間において説明した内容と同様である。なお実際のデータ分析作業においては多くの場合母分散は未知であるため、t.test() を用いることが多い。しかしながら、電球の例では母集団の分散は \\(\\small 180^2\\) であることを仮定した。そのため、ここからは母分散が既知（\\(\\small \\sigma^2=180^2\\)）であることを仮定した標準正規分布に基づく母平均の検定を軸に説明していく。 電球の例においては、\\(H_0:~\\mu=1700\\)と設計していたため、検定統計量 Z は以下の通りに書き換えることができる。ただし、今回の例においては、 \\(\\small \\bar{X}=1835\\)、\\(\\small= \\sigma=180\\) であることがわかっている。 \\[ Z=\\frac{1835-1700}{\\sqrt{180^2/16}} \\] ここまでの議論を踏まえ、新型電球に関する統計的検定を標準正規分布に基づき以下のように実施する。 n &lt;- length(bulb) z &lt;- qnorm(0.025, lower.tail=FALSE) xbar &lt;- mean(bulb) sigma &lt;- 180 mu &lt;- 1700 #Test statistic Z &lt;- (xbar - mu)/(sigma/sqrt(n)) Z ## [1] 2.993056 分析の結果、検定統計量 Z の実現値が5%有意水準に基づく臨界値（1.96）よりも大きいことが示されたため、5%有意水準で帰無仮説が棄却された。つまり、5%の第一種の誤りの確率を残した上ではあるが、新型電球の製品寿命は旧型製品の寿命よりも長いと言える。このような結果は一般的に、「統計的に有意な結果」と表現される。 ここまでは、有意水準の意味を踏まえ、検定の手順及び結果の解釈について説明した。しかしながら、先述の通りもし統計的に有意でない（帰無仮説を棄却できない）結果を得たときには、その解釈について注意が必要であるもし今回の仮説検定で帰無仮説を棄却できていなかったとしたら、その結論は「新型電球寿命の平均は1700時間ではないとは言えない」となる。なんとも歯切れの悪い結論だということは理解できるが、統計的検定の特性上、このような解釈を提示しないといけない。 平均 \\(\\small \\mu\\)、分散 \\(\\small \\sigma^2/n\\)の正規分布↩︎ "],["平均値に関するその他の検定.html", "6.9 平均値に関するその他の検定", " 6.9 平均値に関するその他の検定 これまでは、母平均が特定の値を取るか否かに着目し、統計的仮説検定の基礎について説明した。しかしながら本章の冒頭でも例に挙げた通り、平均値をあるグループ間で比較したいと考えることも多い。本節では、期待値の比較に着目し、平均の差の検定と、分散分析について説明する。これらの検定では、用いる検定統計量は先述のものと異なるが、統計的仮説検定そのものの手順や、肝となる考え方は共通である。 前節で考えた、「女性に比べ男性の方が新製品購買意図が高い。」という作業仮説を再度考える。このとき、我々が観察可能なのは男性グループの標本平均（\\(\\small \\bar{X}\\)）と女性グループの標本平均（\\(\\small \\bar{Y}\\)）であるが、検定においてはそれぞれの期待値（\\(\\small \\mu_x\\) と \\(\\small \\mu_y\\)）に着目し、帰無仮説を作成する。なお、\\(\\small X_1,...,X_n\\) は\\(\\small N(\\mu_x,\\sigma^2_x)\\)に従う母集団からの無作為標本であり、\\(\\small Y_1,...,Y_n\\) は\\(\\small N(\\mu_y,\\sigma^2_y)\\)に従う母集団からの無作為標本であるとする。また、\\(\\small X_1,...,X_n\\) と \\(\\small Y_1,...,Y_n\\) は互いに独立であり、母分散は未知であるとする。 先述の男女間の購買意図の差に関する作業仮説について、男性における購買意図の期待値を \\(\\small \\mu_x\\)、女性における購買意図の期待値を \\(\\small \\mu_y\\)とすると、帰無仮説と対立仮説は以下のように示すことができる。 \\[H_0:~\\mu_x=\\mu_y,~~H_1:~\\mu_x\\neq\\mu_y\\] 統計的検定の手順と直感的な検定統計量の作り方は前節の内容と同じである。そのため、検定における推定量と帰無仮説条件下での未知パラメータの値を特定し検定統計量を定義したい。この検定ではグループ間の平均の差を捉えているため、標本上での情報として \\(\\small \\bar{X}-\\bar{Y}\\) という関係を捉える。したがって、上記の帰無仮説と対立仮説は以下のように書き直すことができる。 \\[H_0:~\\mu_x-\\mu_y=0,~~H_1:~\\mu_x-\\mu_y\\neq0\\] また、母分散が未知である場合にはt検定を行うということも前節と同様である。このことから、以下の検定統計量を用いる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})-(\\mu_x-\\mu_y)}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] ただし、\\(s^2\\) は標準誤差と呼ばれる母集団の標準偏差の推定量である。なお、\\(s^2\\)は母分散を捉えた推定量であるが、母分散が両群で等しい（等分散: \\(\\small \\sigma^2_x=\\sigma^2_y=\\sigma^2\\)）である場合には上記の検定統計量を自由度（\\(\\small m+n-1\\)）のt分布として分析可能である。一方で等分散出ない場合には、Welchのt検定と呼ばれる、自由度の計算を修正した分析方法を用いる。なお、Welchのt検定で用いられる自由度の式は複雑なのでここでは省略する。 このとき、帰無仮説が正しいという仮定のもとでは、\\(\\small \\mu_x-\\mu_y=0\\)である。そのため、上記の検定統計量は以下のように観察可能な情報のみで構成される形で書き換えることができる。また、帰無仮説が正しければ、この検定統計量は自由度（\\(\\small m+n-1\\)）のt分布に従うと考えられる。 \\[ t=\\frac{(\\bar{X}-\\bar{Y})}{\\sqrt{s^2\\left(\\frac{1}{m}+\\frac{1}{n}\\right)}}\\sim t(𝑚+𝑛−1) \\] そのため、データに基づき計算された検定統計量tの実現値を用いて、以下の方式で検定を行う。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(m+n-1) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(m+n-1)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] Rにおいて平均の差の検定を行うことはさほど難しくない。先程の等分散性についても、var.equal=TRUEまたはvar.equal=FALSEという引数で設定できる。var.equal= 引数についてはTRUEが等分散性を仮定するが、デフォルトでは、FALSEとなっている。 平均の差の検定では、 t.test(outcome ~ category) のように、はじめに着目する成果変数を、その後 ~（チルダ）のあとに着目するカテゴリ変数を指示することで、どの変数（outcome）の平均の差をどのカテゴリ変数（category）で検定するのかが特定化できる。ここでは、以前の章で利用した `firm2018’ データを利用して、広告集中度の高い企業と低い企業とで売上高の平均値に差があるか否かを以下のように分析する。 library(tidyverse) firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) firm2018 &lt;- firmdata %&gt;% filter(fyear == 2018) %&gt;% mutate(ad_dummy = ifelse(adint &gt; median(adint),1, 0)) t.test(sales ~ ad_dummy, data = firm2018) ## ## Welch Two Sample t-test ## ## data: sales by ad_dummy ## t = -3.377, df = 87.234, p-value = 0.001096 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -1649371.7 -427200.4 ## sample estimates: ## mean in group 0 mean in group 1 ## 725009.7 1763295.7 t.test(sales ~ ad_dummy, data = firm2018,var.equal=T) ## ## Two Sample t-test ## ## data: sales by ad_dummy ## t = -3.4141, df = 146, p-value = 0.0008287 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -1639334.7 -437237.4 ## sample estimates: ## mean in group 0 mean in group 1 ## 725009.7 1763295.7 分析の結果、等分散性を仮定するか否かで、計算結果は微妙に異なるが、どちらの検定結果においても1%有意水準で帰無仮説を棄却できた。そのため、グループ間で売上高には差があり、広告集中度の高いグループのほうが売上高が高い（もしくは、売上高の高い企業ほど広告集中度が高いグループに属している）といえる。 なおRにおいては、等分散性に関する検定もvar.test(outcome ~ category)で実行可能である。先程のidposデータに関する男女差について、等分散性の分析を以下のように実施する。 var.test(sales ~ ad_dummy, data = firm2018, ratio = 1) ## ## F test to compare two variances ## ## data: sales by ad_dummy ## F = 0.10973, num df = 74, denom df = 72, p-value &lt; 2.2e-16 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.06905383 0.17412361 ## sample estimates: ## ratio of variances ## 0.1097348 等分散性の検定では、ひとつのグループの分散ともう一方のグループの分散の比が1（等分散）であるという帰無仮説を設計する。詳細は割愛するが、帰無仮説が正しい場合には両グループの不偏標本分散の比が自由度（\\(m-1\\), \\(n-1\\)）のF分布に従う。分析の結果、帰無仮説は棄却されたため、等分散とは言えないと結論づけることができる。そのため、平均の差の検定においては、Welchのt検定を利用した分析結果を採用して議論することが好ましい。 "],["分散分析.html", "6.10 分散分析", " 6.10 分散分析 ここまでの内容では、２グループ間の平均の差に関する分析を捉えた。しかしながら、三つ以上のグループ間の平均の差に関心があることもある。例えば、異なる地域における売上高の差を比較したい場合が挙げられる。このような目的を持つ場合によく用いられるのが分散分析（Analysis of Variance; ANOVA）である。本節では、ANOVAの実行方法を中心に説明を行う。なお、ANOVAに関するより詳細な説明は別添の補足資料を参照してほしい。 ANOVAの構造については、要因と水準という二つの要素から説明が行われる。要因とは、観測値に影響を与えていると考えうるカテゴリ変数のことを指し、水準とは、要因を構成するいくつかの条件やグループを指す。例えばある小売企業における各店舗の一定期間内の売上高が出店エリア特性によって差があるのかという問いに関心があるとする。その際各店舗を、都市エリア、郊外エリア、農村エリアという三つのグループに分類し、それぞれのグループにおける標本平均を求めれば、エリアごとの差を分析できるだろう。この場合、「地域」が売上高に影響を与えうる要因であり、「都市、郊外、農村」という三つのグループが水準だと言える。 分析において取り上げる要因が一つである分散分析を一元配置分散分析と呼ぶ。二元配置分散分析については本資料では扱わないため、補足資料を参照してほしい。なお、分散分析をRで実行することは難しくない。ここでは、reshape2というパッケージに内包されてる tipping データを用いてANOVAを実行するため、以下のようにreshape2パッケージをインストールして欲しい。 install.packages(reshape2) パッケージをインストールしたら、以下のようにreshape2を起動し、今回使用する tips データを確認する。本データに含まれている変数は以下の通りである。 total_bill: 支払料金（ドル） tip: チップ額（ドル） sex: 支払い者の性別 smoker: グループ内に喫煙者がいるか day: 曜日 time: 時間帯 size: 人数 library(reshape2) str(tips) ## &#39;data.frame&#39;: 244 obs. of 7 variables: ## $ total_bill: num 17 10.3 21 23.7 24.6 ... ## $ tip : num 1.01 1.66 3.5 3.31 3.61 4.71 2 3.12 1.96 3.23 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 1 2 2 2 2 2 ... ## $ smoker : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ day : Factor w/ 4 levels &quot;Fri&quot;,&quot;Sat&quot;,&quot;Sun&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ time : Factor w/ 2 levels &quot;Dinner&quot;,&quot;Lunch&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ size : int 2 3 3 2 4 4 2 4 2 2 ... ここでは、以下の通りチップ額が曜日によって異なるか否かを分析する。ANOVAの実行においてはaov()関数によって分析するモデルとデータを指定し、anova() によって分析結果を出力する（summary() を用いることも可能である）。なお、上記のデータサマリーより、day という要因には四水準 (levels) 含まれていることがうかがえる。 s &lt;- aov(tip ~ day, data = tips) anova(s) ## Analysis of Variance Table ## ## Response: tip ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## day 3 9.53 3.1753 1.6724 0.1736 ## Residuals 240 455.69 1.8987 分析結果における Sum Sq は、水準間変動和（i.e.,平方和）と呼ばれ、特定要因の水準間によって説明される観測値の変動を表している。 Mean Sq は平均平方と呼ばれ、Sum SqをDF(自由度)で割ったものである。F value は F値という検定統計量の実現値であり、Pr(&gt;F) は本検定の p値を表している。また、Residuals の行で示されているのは、残差平方和と呼ばれ、郡内変動、つまり同グループ（水準）内での値のばらつきの程度を表している。 分析の結果、チップ額について曜日による統計的に有意な差は確認されなかった。それでは、ANOVAでは具体的にどのような帰無仮説を用いた検定を行っているのだろうか？結論を先に述べると、「すべての水準間で平均値は同じ」という帰無仮説を検定しており、仮に帰無仮説が棄却された場合、「少なくとも一つの水準では値が異なる」という対立仮説を指示する。そのため、ANOVAにおける帰無仮説の棄却は、少なくとも1つの群は全体と異なる平均値を持っているという結論につながる。そのため、ANOVAの検定結果だけでは具体的にどの水準間に差があるのかはわからない。そこで、ANOVAを用いた研究では事後分析として、多重比較と呼ばれる分析を行うことが多い。しかし、この多重比較には統計的な問題が伴うと言われている（詳細は補足資料参照）。その問題に対応した手法として広く用いられているのが、Tukeyのpair-wise 比較である。Rではこの分析を、TukeyHSD() という関数で実行できる。具体的には、aov() でストアしたANOVAの分析結果を用いて以下のように実施する。また、plot()を用いて、pair-wise比較の結果を95%信頼区間とともに図示化することもできる。 tukey_result &lt;- TukeyHSD(s) tukey_result ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = tip ~ day, data = tips) ## ## $day ## diff lwr upr p adj ## Sat-Fri 0.25836661 -0.6443694 1.1611026 0.8806455 ## Sun-Fri 0.52039474 -0.3939763 1.4347658 0.4558054 ## Thur-Fri 0.03671477 -0.8980753 0.9715049 0.9996235 ## Sun-Sat 0.26202813 -0.2976929 0.8217492 0.6203822 ## Thur-Sat -0.22165184 -0.8141430 0.3708394 0.7678581 ## Thur-Sun -0.48367997 -1.0937520 0.1263921 0.1724212 plot(tukey_result) 分析結果における diff 列は平均値の差を表している。 lwer と upr は信頼区間の下限と上限を表しており、一番右の列はp値を示している。分析の結果、P-valueが10%水準よりも低い結果がないため、どのペアに関する検定でも有意な差は確認できなかった。したがって、分析結果は必ずしもチップ額が曜日によって変化するとは言えないことを示した。この結果は、アメリカにおけるチップ額が会計額に対する割合や提供されたサービス品質によって決まるという慣習から考えると妥当な結果である。しかしながら、前節で注意した通り、このような統計的に非有意な結果をもって「曜日はチップ額に影響を与えない」と結論づけるのは不適切である。 なお、今回の分析においてはANOVAもTukeyのpair-wise比較も有意な結論を得ることができなかったという点で、両者の結果に一貫性があった。しかしながら、ANOVAでは有意だが、Tukeyの分析ではどの組み合わせも有意ではないという一見整合でない結果を得ることもある。その場合には、慣習としてTukeyの多重比較結果を優先して解釈を提示することが多い。しかしながら、研究者が自身の実施した検定の「意味」を理解し、解釈や議論を提示することも重要である。例えば、ANOVAとTukeyでは用いている帰無仮説が異なるため、異なる比較対象を用いた検定を実施している。そのため、自身が実行した分析がどのような帰無仮説を採用しており、何と何の比較を行っているのかを正確に把握し、実施した検定の意味に適した解釈や議論を展開する事が重要になる。 本章では、基礎的な統計学の復習として、主に区間推定と統計的仮説検定について説明した。区間推定では、主に信頼区間の計算に着目し、信頼区間の意味についてきちんと理解、解釈することの重要性を強調した。また、統計的仮説検定では、母平均の検定を起点とし統計的検定の基礎的な構造と考え方について説明した。検定においては、母集団の統計的特徴に関する予測である帰無仮説と対立仮説を設計することが重要である。また、グループ間の差異に着目した検定を行う場合には、関心のある未知パラメータについての差や比に着目し検定統計量を作成する事が多いが、基本的な統計的検定の考え方と手順は変わらないという点についても説明した。 "],["pwr.html", "6.11 検定力分析とサンプルサイズ", " 6.11 検定力分析とサンプルサイズ 統計的検定を行う場合、サンプルサイズの多さが実務的含意の弊害になりうるという議論も存在する。例えば、平均の差の検定においてサンプルサイズが著しく多いと、実務的にはあまり意味を持たない極めて小さな差であっても、「統計的に有意な差」として判断される事がある。このような弊害を避けるために、検定力分析と呼ばれる分析枠組みからサンプルサイズを計算する方法が用いられる（Cohen, 1988）。 ここでは、以下のような帰無仮説と対立仮説を用いる平均の差の検定を例に取りこのアプローチによるサンプルサイズの検討を紹介する。 \\[ H_0:~~\\mu_1=\\mu_2\\\\ H_1:~~\\mu_1\\neq\\mu_2 \\] 検定力分析では、検定力と効果量という指標を用いる。検定力は第二種の誤りを犯す確率 \\(\\beta\\) を用いて \\(1-\\beta\\) と定義される。概念的に説明すると検定力とは、「母集団において 差があるとき，サンプルにおいて有意な結果が得られる確率」といえる（南風原, 2002, p.143）。一方で効果量は、サンプルサイズで変化することのない、標準化された効果の大きさについての指標である（水元・竹内, 2008）。先述の通り、サンプルサイズが大きいと、この効果量が小さくても統計的に有意な差があると判断されることもある。そのため、効果量を捉えることで実務的にどのような差があるのかを議論することが可能になる。なお、平均の差の検定においては、0.2、0.5、0.8をそれぞれ小さい、中くらい、大きい、効果量の目安として捉えられている（水元・竹内, 2008）。 検定力分析の枠組みでは、有意水準（\\(\\alpha\\)）、検定力（\\(1-\\beta\\)）、効果量（\\(d\\)）、サンプルサイズ（\\(n\\)）の4つの指標については、他の3つが定まると残りの1つの指標も自動的に求まるという関係を有する（水元・竹内, 2008）。検定力分析においては元々、分析における有意水準、効果量、サンプルサイズから分析の検定力を計算する方法として用いられてきた。しかしながら、上述の各指標同士の関係から、自身が想定する分析・検定の条件（\\(\\alpha\\)、\\(1-\\beta\\)、\\(d\\)）を指定することで、その条件を満たす \\(n\\) を求める事も可能である。この枠組みを採用することで、自身の求める大きさの効果量（例えば、\\(d=0.8\\)）を想定する条件（\\(\\alpha\\)、\\(1-\\beta\\)）で検定するために必要な \\(n\\)を逆算することが可能になる。 ここでは、検定力分析の詳細な計算方法について省略し、Rを用いた分析方法を紹介する。具体的な計算は、pwr::pwr.t.test() というパッケージおよび関数を用いる。なお、pwr には他にも様々な分析モデルに対応するサンプルサイズの計算が可能である。まずは、以下の通りパッケージをインストールし読み込む。 install.packages(&quot;pwr&quot;) library(pwr) ここでは、先行研究で推奨されている\\(\\alpha=0.05\\)、\\(1-\\beta=0.8\\)という水準を用いて、平均の差の検定を行う場合のサンプルサイズを以下のように求める（Cohen, 1988）。 est_n &lt;- pwr.t.test(d=0.8, power=0.8, sig.level=0.05) est_n ## ## Two-sample t test power calculation ## ## n = 25.52458 ## d = 0.8 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 出力結果の通り、上記の条件に基づくサンプルサイズは各グループ26人ずつであるとわかる。そのため、2グループ間の比較を行う場合のサンプルサイズの合計は52となる。また、検定力とサンプルサイズの関係について、plot() 関数を用いて以下のように出力することも可能である。 plot(est_n) なお、pwr.t.test() 関数内の引数設定次第で、検定力を計算することも可能である。 例えば、\\(d=0.8\\), \\(n=30\\), \\(\\alpha=0.05\\) の場合の検定力は以下のように計算できる。 pwr.t.test(d=0.8, n=26, sig.level=0.05) ## ## Two-sample t test power calculation ## ## n = 26 ## d = 0.8 ## sig.level = 0.05 ## power = 0.8074866 ## alternative = two.sided ## ## NOTE: n is number in *each* group そのため、研究者が採用した検定の条件や、分析結果としての効果量が算出できれば、その検定における検定力も同じ関数から計算可能であることが伺えた。 "],["練習問題-3.html", "6.12 練習問題", " 6.12 練習問題 6.12.1 仮説検定の理屈 以下の統計的仮説検定についての問いに答えよう。 \\(x\\)は、区間[\\(-\\theta\\), \\(\\theta\\)–]を持つ一様分布に従う確率変数である。確率密度関数は以下とする: \\[ f(x) = \\begin{cases} \\frac{1}{2\\theta} &amp;-\\theta\\leq x\\leq\\theta \\\\ 0 &amp; \\text{それ以外} \\end{cases} \\] \\(H0\\)、\\(H1\\)、\\(c\\) をそれぞれ帰無仮説、対立仮説、臨界値とし、いかのような統計的仮説検定を行うこととする： \\[ H0:~\\theta=3,~~H1: \\theta\\neq3 \\] \\[ \\begin{cases} H0\\text{を棄却} &amp;\\text{if}~~c&lt;|x|\\\\ H0\\text{を棄却しない} &amp; \\text{それ以外} \\end{cases} \\] 上記の統計的仮説検定に関する仮定に基づき、\\(\\alpha=0.2\\) とした場合の、臨界値 \\(c\\) を求めよう。 6.12.2 仮説検定の解釈 統計的検定を用いた研究の例として、以下の内容を読み、研究手法と結論に関する問題点を考えよう。 ある学生がある変数 \\(x\\) と \\(y\\) との関係に関心を持っている状況を考える。このとき、その学生は先行研究をレビューした結果として、以下のような仮説を立てた： H1: \\(x\\) は \\(y\\) に有意な影響を与えない。 その学生はこの仮説を検証するために、綿密な調査設計のもと、着目する変数 \\(x\\) と \\(y\\) を含むデータを取得し、（回帰分析などで）統計的に変数間の関係を検証した。そして、分析の結果、\\(x\\) と \\(y\\)との間に統計的に有意な関係は見られなかった。そのためその学生は、「H1は支持された」と結論付けた。 "],["参考文献-4.html", "6.13 参考文献", " 6.13 参考文献 浅野正彦・矢内勇生（2018）「Rによる計量政治学」，オーム社. 岩田暁一（1996）「経済分析のための統計的方法 第２版」，東洋経済新報社. 倉田博史・星野崇宏（2011）「入門統計解析」，新世社. 豊田秀樹（2017）「p値を使って学術論文を書くのは止めよう」,『心理学評論』,60(4), 379-390. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. 宮川公男（2002）「基本統計学」，有斐閣. Baker, M. Statisticians issue warning over misuse of P values. Nature 531, 151 (2016). "],["regintro.html", "Chapter 7 回帰分析 ", " Chapter 7 回帰分析 "],["本章の概要-4.html", "7.1 本章の概要", " 7.1 本章の概要 二変数間の関係を捉える分析手法として 5 章では相関係数を紹介した。相関係数は二変数間の線形関係を表す -1から 1 の値を取る指標である。 しかしながら、相関係数は線形関係の強さ（どれだけデータが直線上に近く分布しているか）を表しているものの、示されている直線の切片や傾きといった線形関数の特徴は捉えられない。例えば、下図の二つのデータは異なる切片と傾きを有しているが、相関係数は等しくなるはずである。 相関と線形関係 そのため、二変数間の関係をより詳細に捉えるためには散布図においてどのような直線を引けるのか、その直線の特徴（切片や傾き）について分析することが求められる。本章では、このような目的のもと、ビジネス領域で最も広く用いられる手法の一つである回帰分析を紹介する。回帰分析は、理論や産業的知識から導出された仮説を検証したり、変数間の関係について予測を行うために用いられる方法である。回帰分析の最も基本的な構造は以下のように線形関数の形で変数間の関係を捉えるモデルを定式化するものである： \\[ y_i=\\alpha+\\beta x_i+u_i \\] このとき、\\(y_i\\) は被説明変数（従属変数）、\\(x_i\\) は説明変数（独立変数）、\\(u_i\\) は誤差項と呼ぶ。\\(\\alpha\\) と \\(\\beta\\) はそれぞれ切片と傾きを表すパラメータであり、経営学・マーケティング領域の研究では、これらのパラメータについて推定・検定することが主たる目的になる事が多い。 また、回帰分析では複数の説明変数を含むモデルの定式化も可能であり、以下のようなモデルを重回帰モデルと呼ぶ。 \\[ y_i=\\alpha+\\beta_1 x_{1i}+...+\\beta_k x_{1ki}+u_i \\] 本章では回帰分析の基本的な原理原則と、Rを用いた分析手法および、分析結果の解釈について紹介する。Rを用いて回帰分析を実行すること自体は難しくない。そのため、本書では回帰分析結果に関する解釈について、特に誤解が多い点を強調する形で説明する。第一に回帰分析により推定された結果の解釈について強調する。回帰分析では、切片と傾きパラメータを用いた線形関数で被説明変数と説明変数の関係を示しているが、これはこれらの変数間の平均的な関係を捉えたものである。より具体的には、ある \\(x\\) の値が与えられたときの \\(y\\) の「平均値（期待値）」と \\(x\\) の間には線形の関係があることを示している。そのため、たとえ回帰分析の結果、\\(x\\) と \\(y\\) の間に正の関係を見出したとしても、それはあくまで平均的な関係であるため、中には \\(x\\) は高いが \\(y\\) は（比較的）低い値を取る観測個体がいることもあるだろう。 第二に、係数の検定について述べる。Rなどのソフトウェアで回帰分析を実行すると、回帰係数に対する検定を行ってくれる。しかしながら、この検定が何を意味しているか（どのような帰無仮説を採用しているか）を理解する事が重要になる。ソフトウェアで自動的に出力される検定では、「係数がゼロか否か」を検定している。そのため、係数の検定結果（統計的に有意か否か）をもとに、\\(x\\) が \\(y\\) に与える影響の強さや程度について議論することはできないという点に注意が必要である。 第三に、重回帰モデルにおける係数解釈とその重要性について強調する。回帰分析を用いた研究では基本的に重回帰モデルが採用される。その理由として、重回帰モデルにおける説明変数の係数は、同モデル内の他の変数の影響をコントロールしたうえでの説明変数が被説明変数へ与える影響を表現している。これは、説明変数が持つ変動のうち他の説明変数とは無関係な変動だけを抽出し、被説明変数との関係を分析する構造になっているためである。このような効果の解釈に基づき、重回帰モデルによって明らかになる効果をパーシャル効果（Partial effects）として解釈される。この特性は分析におけるコントロール変数の採用や、より信頼性の高い効果検証を目的として広く活用されている。 第四に重回帰モデルにおける変数選択についての説明を加える。重回帰モデルを活用する研究者においては、ただ闇雲にモデル内の説明変数を増やせば良いわけではない。重回帰モデル含める変数は、主に（1）欠落変数バイアス、（2）多重共線性、（3）過剰統制の三点から考慮される（西山ほか, 2019）。理論や産業的知識から必要な変数を含めずに重回帰モデルを推定すると、係数に欠落変数によるバイアスがかかることが知られている。そのため、必要な変数は追加する必要がある。 一方で変数を増やすことの弊害として広く知られているのが多重共線性である。しかしながらマーケティング領域では多重共線性について誤解されているケースも散見されるため、以下の点について強調する。多重共線性は、完全な共線関係（説明変数同士の相関係数が1もしくは-1の場合）と、そうでない（説明変数同士の相関が高い）場合とに分けて論じる必要がある。完全な共線関係である場合は係数の推定値が計算できないため、どちらか一方の変数をモデルから除外する必要がある。他方で相関係数が高い場合には、推定量の分散を大きくしてしまうという問題につながる。そのうえで本書では、相関係数が高いという理由で欠落変数バイアスのリスクを冒して変数を除外するという判断は勧めない。説明変数同士の相関が高い場合には、モデルに採用している変数の意義を検討し、その変数をそのまま残す、何かで比を取る、対数化するなどの変数変換の工夫をしながら、その変数をモデルに含めることを現実的かつ実践的な対応として推奨する。なおこの立場は、本書が回帰分析における係数の検証や解釈に注視していることに大きく依存するため、注意してほしい。これは、もし分析における主な関心が着目する説明変数の被説明変数への効果を検証・解釈することであり、かつそれをきちんと捉えるために採用した説明変数の存在が必要なのであれば、説明変数間の相関は無視して構わない、とする Wooldridge（2013）に沿った考え方である。 変数選択のもう一つの考慮すべき点として過剰制御の問題がある。これは、主要な説明変数の効果に関する理論的なメカニズムにおいて中間経路として機能する変数をモデルに含めることによって生じる問題である。重回帰モデルの係数はパーシャル効果として解釈されるため、説明変数が被説明変数へ影響を与えるメカニズムの経路が分断され、結果の理論的解釈が困難になってしまう。そのため、重回帰モデルの定式化においては、想定される理論的枠組みにおける各変数間の役割を慎重に検討し、モデルに含める変数を決定する必要がある。 本章では、上記の注意点について理解するための、回帰モデルに関する統計的な原理原則と、Rを用いた分析手法および結果の解釈を提示する。 "],["分析準備.html", "7.2 分析準備", " 7.2 分析準備 本章では、変数間の関係を捉える回帰分析について、そのモデルの基礎と統計的推測に基づく解釈を説明する。回帰分析結果から得られる含意は、「予測」と「検証」の二つに大別することができる。その上で特に本書では、「検証」という側面、特に「研究上関心のある説明変数の係数の解釈」を重視する立場を取る。立場が異なれば、回帰分析上何を重視するかという観点も異なるため、特に回帰分析による予測に関心のある読者においては別の図書を参照してほしい。 なお本章では、5 章でも利用した MktRes_firmdata.xlsxという企業データを用いた分析を行う。次節に移る前に以下の要領でデータを読み込んでほしい。 firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) 本章では主に、firmdata における2019年のデータ抽出し、クロスセクショナルデータとして用いる。以下の様に全データから2019年の情報を抽出してほしい。 library(tidyverse) firmdata19 &lt;- firmdata %&gt;% filter(fyear == 2019) データを用いた分析を行う場合、取得したデータの記述統計や分布を確認する必要がある。本来であれば研究上重要な変数を対象にデータの特徴を整理するが、ここでは複数の変数の特徴を一括で整理、図示化する方法を提示する。この方法では、GGallyというパッケージのggpairs()という関数を用いるため、以下のようにパッケージをダウンロードしてほしい。 install.packages(&quot;GGally&quot;) firmdata19 データセットから、例として四つの変数を抽出して、ggpairsを実行する。これにより、各変数のヒストグラム（密度形式）と、それぞれの変数間の相関係数と散布図が同図内で示されている。また、ggpairs()内の引数設定によって様々な図示形式を指定できるため、興味のある人は調べてみてほしい。 firmdata19 %&gt;% select(sales, mkexp, emp, operating_profit) %&gt;% GGally::ggpairs()+ labs(title = &quot;ggpairs example&quot;) なお、記述統計については既出の summary()関数にデータフレームを指定することで、データセット全体の記述統計を出力する。ここでは例として先程と同じ変数の記述統計を以下のように出力してみる。 ds1 &lt;- firmdata19 %&gt;% select(sales, mkexp, emp, operating_profit) %&gt;% summary() knitr::kable(ds1, align = &quot;cccc&quot;) sales mkexp emp operating_profit Min. : 11333 Min. :0.01137 Min. : 163 Min. :-40469 1st Qu.: 186830 1st Qu.:0.16714 1st Qu.: 3488 1st Qu.: 7788 Median : 464450 Median :0.25508 Median : 7825 Median : 24824 Mean :1194513 Mean :0.29894 Mean : 20156 Mean : 80811 3rd Qu.:1164243 3rd Qu.:0.37438 3rd Qu.: 24464 3rd Qu.: 63026 Max. :9878866 Max. :0.75650 Max. :160227 Max. :656163 "],["単回帰モデルと予測値.html", "7.3 単回帰モデルと予測値", " 7.3 単回帰モデルと予測値 本章では、二変数間の関係について「予測」と「検証」の二点からより深掘りしていく方法を紹介する。マーケティングを実行する企業の立場に立てば、マーケティングに関連する方策や投資を導入することで成果（売上や利益）を向上させたいと考えるだろう。しかしながら、マーケティングの施策により得られる成果や、そのために投入される労働力、金銭、時間などの資源は、採用する施策案により異なる。そのため、企業も闇雲に意思決定を行うわけには行かず、その意思決定によって、どの程度の成果の向上を見込めるかの予測ができると良い。最も基本的な予測の形式は、ある説明変数（先行要因）が被説明変数（成果）に対してどのような影響を与えるのかを特定し、それに基づき予測値を算出するという方法である。 回帰分析絵では、二つの異なる変数 \\(y,~x\\) の関係を \\(\\small y=f(x)\\) のように\\(y\\)を\\(x\\)の関数（\\(f(x)\\)）で示すというアイディアで分析を行う。このとき \\(y\\) を「被説明変数（Explained variable）もしくは従属変数（dependent variable）」、\\(x\\) を「説明変数（Explaining variable）もしくは独立変数（independent variable）」という。そして、被説明変数と説明変数の関係を特定化した式のことを一般的に回帰モデルという。最も基本的な関数型の特定方法は以下のような一次関数による特定化である。 \\[ y=\\beta_0+\\beta_1x \\] このとき、\\(\\small \\beta_0\\) は切片、\\(\\small \\beta_1\\) は傾きを表す係数であり、回帰係数と呼ばれる。 回帰モデルは線形の関係を捉えているものの、実際にデータを入手し散布図を作成すると、以下のように、直線とは異なる結果を得る。そのため、上記のモデルは正確な表現でないことがわかる。 分析者がデータとして得る情報は、\\(y\\) や \\(x\\) の実現値であり、回帰モデルの切片や傾きの値は直接はわからない。そこで、モデルで捉えた直線と実現値のズレを考え、得たデータから回帰モデルのパラメータ（係数）を推定するという方針をとる。モデルで捉えた直線による（係数の推定値に基づく）\\(y\\) と\\(x\\) の関係は、\\(\\small x=x_i\\) のとき、\\(y_i\\) の予測値である\\(\\small \\hat{y}_i\\)（ワイハット）と、係数の推定値 \\(\\small \\hat{\\beta}_0\\)、 \\(\\small \\hat{\\beta}_1\\) を用いて以下のように定義できる。 \\[ \\hat{y}_i=\\hat{\\beta}_0+ \\hat{\\beta}_1 x_i \\] 係数の推定値 \\(\\small \\hat{\\beta}_0\\) と \\(\\small \\hat{\\beta}_1\\) を求めるための計算方法は、（最尤法や積率法など）いくつかあるものの、本書では最小二乗法（Ordinary least square: OLS）という方法に着目し紹介する。OLS推定量（OLS Estimator: OLSE）の求め方の直感は、以下の図の通り、観測値と回帰直線間の距離の合計（残差平方和）を最小にするように計算される。 OLSE概要 予測値のモデルで示されているのは、データを分析した結果求めたOLSEに基づく説明変数 \\(x_i\\) と、\\(\\small \\hat{y}_i\\) との関係である。\\(\\small \\hat{y}_i\\) は被説明変数 \\(y_i\\) の「予測値（predicted value）」や「理論値（fitted value）」と呼ばれるものであり、\\(y_i\\) の観測値とは異なる値であることに注意が必要である。このとき、観測値と予測値のズレ（\\(\\small y_i-\\hat{y}_i\\)）を「残差（residual）」という。OLSは残差を \\(\\hat{u}_i\\) とし、以下で示される、残差平方和（二乗和）を最小にするように推定量を求める方法である。なお、OLSEの計算仮定については、秋山（2018）が詳しく説明をしてくれている。 \\[ \\sum_{i=1}^n\\hat{u}_i^2=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2=\\sum_{i=1}^n\\left(\\hat{y}_i-(\\hat{\\beta}_0+ \\hat{\\beta}_1 x_i)\\right)^2 \\] これを解くと、以下のような推定量を得る。 \\[ \\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x} \\] \\[ \\hat{\\beta}_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\] Rによる回帰分析は、lm()という関数（linear model）を用いて簡単に実行できる。この関数内では、lm(y ~ x, data = df) という要領で、説明変数と被説明変数を \\(\\sim\\)（チルダ）で繋いでモデルを指定する。例えば、先程の企業データにおける2019年の観測を用いて、従業員数と売上高の関係について分析するためには、以下のように分析を実行する。 reg1 &lt;- lm(sales ~ emp, data = firmdata19) coef(reg1) ## (Intercept) emp ## 22809.67863 58.13194 分析の結果、定数項（Intercept）は 22809.7 で、傾きは 58.1 であることがわかった。つまり、従業員数を一単位増やすと、売上高が58.1（百万円）増えることを示唆している。仮に従業員数が10人であれば、売上高の「予測値」は以下のように計算できる。 \\[ 22809.7+58.1\\times 10=23390.7 \\] 回帰分析によって被説明変数の予測値を計算が可能なことを説明したが、この予測値は実際の観測値とは異なる。では、ここで求められた予測値はどのように解釈できるものなのだろうか。それを理解するために、残差と予測値に関する以下の四つの性質を紹介する。 残差の和は0： \\[ \\sum_{i=1}^n\\hat{u}_i=0 \\] 残差と説明変数の積和は0： \\[ \\sum_{i=1}^n x_i\\hat{u}_i=0 \\] 1と2より： \\[ \\sum_{i=1}^n \\hat{y}_i\\hat{u}_i=0 \\] 予測値の平均と観測値の平均は等しい: \\[ \\bar{y}=\\bar{\\hat{y}} \\] 回帰直線は（\\(\\bar{x}, \\bar{y}\\)）の座標を通る: \\[ \\bar{y}=\\hat{\\beta}_0+\\hat{\\beta}_1\\bar{x} \\] つまりOLSでは、回帰直線と各観測値のプラス方向のズレとマイナス方向のズレが釣り合う（残差の和が0）ような予測を行っている。その上で、予測値は、説明変数 \\(x\\) が与えられたとき、被説明変数が「平均的に」どんな値を取るのかを示していると解釈できる。回帰モデルと平均との関係については次節で確率的側面からより詳しく説明を加える。 回帰分析による予測による精度を調べるために、分析したモデルがどの程度被説明変数全体の分散を説明しているかという指標によってモデルの適合度を測る。一般的には、決定係数（\\(\\small R^2\\)）という指標によってモデル適合度が示される。\\(\\small R^2\\) は以下のように定義される。 \\[ R^2=1-\\frac{\\sum(y_i-\\hat{y}_i)^2}{\\sum(y_i-\\bar{y})^2}=\\frac{\\sum(\\hat{y}_i-\\bar{y})^2}{\\sum(y_i-\\bar{y})^2} \\] この指標は、被説明変数の分散を説明変数がどの程度説明するかの割合を表しており、0以上1以下の値を取る。例えば \\(\\small R^2\\) が0.80であるならば、被説明変数の変動の80%をモデルが説明しているということになる。そのため、\\(\\small R^2\\) は、回帰モデルの説明力として解釈される。しかしながら、予測という目的に対して近年は、機械学習などの発展的な手法が応用される事が多く、\\(\\small R^2\\) を軸に予測を行うことは少なくなってきている。 また、予測ではなく説明変数の効果（係数）についての検証や解釈に関心がある場合、回帰分析における \\(\\small R^2\\) の重要性は低くなる。特に、ビジネス分野における研究では、係数の推定や検定に焦点をあわせることが多い。本書においても、予測よりも係数に関する検証を重視する立場を取る。社会科学領域での分析では、\\(\\small R^2\\) が低くなることは珍しくない。そんな中で、「\\(\\small R^2\\) が低いからその回帰分析結果は意味がない」ということにはならない。研究者の目的が、関心のある変数同士（例、市場志向と企業成果）の関係性（有意性や影響の強さ）を検証したいというものである場合、仮に \\(\\small R^2\\) が低くても、きちんと両変数の関係を分析できる調査設計や分析を実行しているならば、その検証は有意義なものになる。つまりここで強調したいのは、係数の検証や解釈を重視して研究を行う場合、「\\(\\small R^2\\) がいくつ以上（以下）だから良い（ダメ）」という議論は目的と整合的ではなく、重要ではなくなるということである。 本節では、OLSを中心にデータから回帰係数を推定するプロセスに目を向け、予測値と決定係数について紹介した。しかしながら、先述の通り我々は多くの場合特定の変数が成果変数に与える影響の検証に関心がある。次節では確率的な視点から理論的に回帰分析を理解する事により、回帰分析の結果の解釈についてより詳しく学ぶ。 "],["回帰分析における推定と検定.html", "7.4 回帰分析における推定と検定", " 7.4 回帰分析における推定と検定 回帰分析によって係数を推定すると、それが真の（母集団での）値であると勘違いしてしまう人もいる。しかし、分析の結果傾きの係数が正の値だったとしても、母集団においては0と相違がないかもしれない。したがって、説明変数が被説明変数に与える影響を検証するためには回帰係数を計算するだけでは不十分であり、区間推定や検定を行う必要がある。そこで本節では回帰分析に関わる理論的な説明を行う。以下は、我々の着目する回帰モデルである。 \\[ y_i = \\beta_0+\\beta_1x_i+u_i, \\] ただし、\\(\\small u_i\\) は誤差項、\\(\\small \\beta_0\\)切片、\\(\\small \\beta_1\\)は傾きのパラメータである。つまり、このモデルは母集団での統計的特徴を反映した理論的モデルだと理解できる。なお、通常レポートや論文内に回帰モデルを記載する際には、上記のような回帰パラメータと誤差項を含んだ理論モデルを用いる。理論的なモデルは \\(\\small y\\) を説明する要素として確率的な誤差項 \\(u\\) を含んでおり、被説明変数 \\(\\small y\\) は確率変数として捉えられる。一方で、説明変数 \\(\\small x\\) は、定数として扱う。また、前節で紹介した回帰係数の推定量（OLSE）は、 \\(\\small y\\) や \\(\\small \\bar{y}\\) を含んでいる ため、推定量（OLSE）もまた確率変数であると考えられる。 \\(y\\) の値が確率的にバラつくと考え誤差項を含んでいるのだとしたら、回帰直線は何を表しているのだろうか。理論的には、ある \\(x\\) の値が与えられたとき、\\(y\\) の「平均値（期待値）」と \\(x\\) の間には線形の関係があることを捉えている。より具体的には、回帰直線は以下のように \\(x\\) を所与とした際の \\(y\\) の条件付き期待値として表される。 \\[ E(y|x)=\\beta_0+\\beta_1x \\] この関係は、以下の図のように示すことができる（Wooldridge, 2013）。 条件付期待値としての回帰直線 したがって、回帰モデルは、ある \\(x\\) の値に基づき \\(y\\) の期待値（平均）示してくれるが、具体的にどのような値を取るかは確率的に決まるものだと言える。つまり、回帰分析の結果によって言えることは、例えば、「身長（\\(x\\)）の高い人は、\\(\\color{red}{\\text{平均的に}}\\)重い（\\(y\\)）」ということである。 なお、モデル化に際して誤差項について以下のようないくつかの仮定が存在する。なお、5つめの仮定は回帰係数の検定の際に用いられる。 \\(E(u)=0\\) \\(E(u|x)=0\\) \\(Var(u)=E(u^2)=\\sigma^2\\) \\(Cov(u,x)=E(xu)=0\\) \\(u\\) は正規分布に従う このような仮定に従うOLSEは、以下の統計的性質を持つ。 不偏性: \\(E(\\hat{\\beta})=\\beta\\) 漸近的性質: \\(\\hat{\\beta}\\) は、サンプルサイズ \\(n\\) が十分大きいときには正規分布 \\(N(\\beta,~se(\\hat{\\beta})^2)\\) に従う（\\(se(\\hat{\\beta}\\))は、OLSEの標準誤差）。 先述の通り、推定されたOLSEは、モデルの真の値ではない。そのため、仮に分析の結果得た推定値が正の値であっても、母数においては0と大差がないかもしれない。そのため、統計的検定や推測を用いて、未知パラメータに対する検討を行う。 7.4.1 企業データを用いた回帰分析の実行 なお、Rで回帰係数の検定結果を得るのは非常に簡単である。 lm() 関数の実行結果をストアしたオブジェクトに対して、summary() 関数を実行することで統計的検定結果を得ることができる。先程分析した reg1 を再度利用すると、以下のような結果を得る。 summary(reg1) ## ## Call: ## lm(formula = sales ~ emp, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1835142 -280706 -34523 134095 3292333 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 22809.679 88556.098 0.258 0.797 ## emp 58.132 2.559 22.721 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 872800 on 145 degrees of freedom ## Multiple R-squared: 0.7807, Adjusted R-squared: 0.7792 ## F-statistic: 516.2 on 1 and 145 DF, p-value: &lt; 2.2e-16 回帰係数の推定と検定に関する結果は Coefficients: の下に記載されている。推定・検定結果は行列形式で表示されており、Estimate の列は回帰係数の推定結果、Std. Error は標準誤差（詳細は省略するが、誤差項の分散推定量の平方根）、 t valueはt値、 Pr(&gt;|t|)はp-value をそれぞれ示している。そして、出力結果下欄には決定係数（R-squared）や自由度調整済み決定係数（Adjusted R-squared）、F検定結果、といったモデル適合度に関する結果が提示されている（詳しくは次節で説明する）。 上記の結果を解釈するために、回帰分析における検定について説明する。ソフトウェアで自動的に出力される統計的仮説検定は、基本的には以下の帰無仮説と対立仮説を採用したものである（添字は省略）。 \\[ H_0:\\beta=0,~~H_1:\\beta\\neq0 \\] なお、R以外のソフトウェアを用いて回帰分析を実行しても係数に関する検定結果を返すが、通常はこの帰無仮説を採用した検定結果を出力する。 検定では、以下のような検定統計量を用いる。 \\[ t=\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})} \\] \\(H_0\\) が正しいと仮定する（\\(\\small \\beta=0\\)）と、検定統計量 t は計算可能であり、自由度（\\(\\small n-2\\)）のt分布に従う。検定の手順は 7.8 節で紹介したのと同様、有意確率に基づく臨界値を定めた後、t 値を計算し、棄却域と採択域のどちらに入るのかを確認する。 \\[ \\begin{cases} |t|&gt;t_{\\alpha/2}(n-2) &amp; \\Rightarrow \\text{H0を棄却する。}\\\\ |t|\\leq t_{\\alpha/2}(n-2)&amp; \\Rightarrow \\text{H0を採択する。} \\end{cases} \\] これを踏まえて分析結果を確認すると、emp が sales に与える影響（係数: 58.132）は有意に0とは異なると理解できる。また、切片の係数（Intercept）は大きな値を取っているが、統計的には0ではないとは言えないことが示されている。この項は、従業員数が0のときの企業の売上を示しており、この結果が統計的に有意ではないということは、我々の直感とも整合的である。 上記の検定によって、どうやら emp の係数は0ではなさそうだということが伺えた。しかし、具体的にどのような値を取るのだろうか。おおよその値だけでも把握したいのが人情である。そこで、信頼区間を求め、おおよその確率（95%など）で真のパラメータが含まれている区間を確認したい。OLSEの漸近的性質と中心極限定理により、サンプルサイズが十分に大きいとき、先述の統計量 t は標準正規分布にに近づく（西山ほか,2019）。 そのため、7.6節で紹介した、標準正規分布に基づく信頼区間の推定を応用できる。信頼係数を \\(\\small \\alpha\\) とすると、以下の確率と区間の対応関係を得る。 \\[ P\\left(\\left|\\frac{\\hat{\\beta}-\\beta}{se(\\hat{\\beta})}\\right|\\leq z_{\\alpha/2}\\right)=1-\\alpha \\] そして、上記を \\(\\small \\beta\\) に関する不等式に変換すると、以下の信頼区間を得る。 \\[ P(\\hat{\\beta}-se(\\hat{\\beta})\\cdot z_{\\alpha/2}\\leq\\beta\\leq\\hat{\\beta}+se(\\hat{\\beta})\\cdot z_{\\alpha/2})=1-\\alpha \\] したがって、\\(\\small [\\hat{\\beta}\\pm se(\\hat{\\beta})\\cdot z_{\\alpha/2}]\\) という観察可能な情報によって信頼区間推定できる。Rによって信頼区間を得るには、回帰分析の結果に対して、confint() 関数を用いる（デフォルトで95%信頼係数が設定されている）。例えば、先程の reg1の結果を用いて、99%信頼区間を得ると、以下のような結果を得る。 confint(reg1,level = 0.99) ## 0.5 % 99.5 % ## (Intercept) -208335.9722 253955.32943 ## emp 51.4537 64.81018 したがって、emp の99%信頼区間が [51.45, 64.81] であることがわかった。すなわち、企業の従業員が一名多いと、売上高が 51から64 百万円高くなりそうだと解釈できる。一方で、(Intercept) の信頼区間には0を含んでいることが伺える。なお、confint() 関数によって計算される信頼区間の計算では上述の通り正規分布が仮定されており、詳しくはヘルプ（?confint）で確認できる。 "],["重回帰モデル.html", "7.5 重回帰モデル", " 7.5 重回帰モデル ここまでは、回帰分析の概要や係数の検定・推定について説明した。回帰分析を実行することで得る情報は前節の内容がほとんどなのだが、モデルの特定化に関して、もう一つ重要な点が存在する。それが本節で扱う重回帰モデル（multiple regression model）の採用である。重回帰モデルとは、二つ以上の説明変数を含む回帰モデルのことである。一方で、前節で扱ったような説明変数が一つの回帰モデルのことを単回帰（simple regression model）という。回帰分析を用いた研究を行う際には、基本的に単回帰分析ではなく、重回帰分析を実行することが好ましい。通常の分析においては、ある被説明変数に対して考慮すべき説明変数は一つだけではなく、複数の説明変数を考慮すべき状況が多い。しかし、分析に不慣れ学生においては、複数の説明変数に関心がある場合であっても、複数の単回帰モデルを分析することで、それぞれの変数についての分析結果を得ようとすることが散見される（例えば、三つの説明変数の影響を捉えるために単回帰モデルを三本分析する等）。しかしながら本書は、基本的にはこのような分析アプローチは好ましくなく、複数の説明変数を含めた一本の重回帰分析を実施すべきだと主張する。本節では、この主張の理由と、重回帰モデルの特徴・結果解釈について説明していく。 7.5.1 重回帰モデル概要 ある成果変数を説明するために、複数の説明変数が必要になることは、マーケティングリサーチにおいても珍しいことではない。例えば、ある製品のパフォーマンスを月次売上高で測るとする。マーケティング部門として、売上高に対してプロモーション施策がどれだけ貢献しているかを分析する際、プロモーションと売上高の関係を回帰分析で捉えるというアプローチが実現可能な分析方法として考えられる。しかしながら、売上高を説明する変数として、プロモーションだけで十分だろうか。マーケティング変数に着目するだけでも、価格や製品品質、流通網など、異なる変数が売上に関係していることが考えられる。例えば、一見プロモーションによる効果のような結果を得たとしても、実際には同時期に実行していたディスカウント（価格）の影響であり、プロモーションそのものにはあまり効果がないかもしれない。そのため、他の要素の影響を排除した上での純粋なプロモーション効果を明らかにすることは務的有意義な研究課題となりうる。そしてこのような研究課題に対応する分析方法が、重回帰分析である。本節ではまず、重回帰モデルに関する特徴を整理する。 売上とマーケティング変数 7.5.2 重回帰モデルの記述と性質 重回帰分析においても単回帰同様、回帰モデルを記述する。k 個の説明変数を含む重回帰モデルは、以下のように示される。 \\[ y_i = \\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}+...+\\beta_kx_{ki}+u_i \\] 論文やレポート内に重回帰モデルを記載する際にも、多くの場合上記の誤差項を含む理論モデルを用いる。 以下ではまず、重回帰モデルの係数、予測値や、残差に関する性質について説明する。係数の推定は、以下のような行列モデルで捉えることで、単回帰モデルと同様OLSで求められることができる（詳細は省略）。 \\[ Y = X&#39;\\beta + u \\] \\[ \\hat{\\beta}=(X&#39;X)^{-1}X&#39;Y \\] 重回帰分析を実行すると、各説明変数に対応する係数が推定される。各OLSE（\\(\\small \\hat{\\beta}\\)）は 未知パラメータ（\\(\\small \\beta\\)）の不偏推定量である。また、それらの検定や区間推定では、各変数に対応する係数の検定・推定を個別に行う。OLS推定に関わる残差と予測値はそれぞれ以下のように意義される。 予測値: \\[ \\hat{y}_i = \\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i}+...+\\hat{\\beta}_kx_{ki} \\] 残差: \\[ \\hat{u}_i=y_i-\\hat{y}_i =y-( \\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i}+...+\\hat{\\beta}_kx_{ki}) \\] そして、残差は以下の \\(k+1\\) 個の制約を満たす。 \\(\\sum_{i=1}^n\\hat{u}_i=0\\) \\(\\sum_{i=1}^n x_{1i}\\hat{u}_i=0,~\\sum_{i=1}^nx_{2i}\\hat{u}_i=0...,~\\sum_{i=1}^nx_{ki}\\hat{u}_i=0\\) そのため、重回帰モデルの残差の自由度は \\(n-(k+1)\\) となる。 7.5.3 重回帰分析におけるモデル適合度 単回帰モデルにおけるモデル適合度指標として前節では決定係数を紹介した。しかしながら、この指標は致命的な欠陥を有している。それは、モデルに含む説明変数の数が増えると決定係数も上昇する（より正確には、説明変数の数に対して非減少）ということである。つまり、被説明変数と全く関係ない変数をモデルに加えても、決定係数は上昇し、そのモデルの説明力が高いという結論に至ってしまう。そのため、通常の決定係数から説明変数の数を調整した指標である調整済み決定係数（Adjusted R-squared: \\(\\bar{R}^2\\)）を用いて適合度を検討する。この指標は、以下のように定義される。 \\[ \\bar{R}^2= 1 - \\left(\\frac{\\sum(y_i-\\hat{y}_i)}{n-k-1}\\cdot \\frac{n-1}{\\sum(y_i-\\bar{y})}\\right) \\] モデルの適合度を考えるもう一つの分析として、Rの分析結果で出力されていたF検定について説明する。回帰分析の結果として必ず出力されるF検定は、重回帰モデルにおける\\(\\small \\beta_0\\)（定数項） 以外の係数が全て0であるか否かをチェックする検定である。この検定では、k個の説明変数を含む回帰モデル（\\(\\small y_i = \\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}+...+\\beta_kx_{ki}+u_i\\)）に対して（これをフルモデルと呼ぶ）、以下のような帰無仮説と対立仮説を用いた検定を行う。 \\[ H_0:~\\beta_1=...=\\beta_k=0,~H_1:\\text{少なくともどれか一つの係数は0ではない} \\] 帰無仮説が正しいと仮定した場合、重回帰モデルは以下のようになり、この定数項のみのモデルをモデル0と呼ぶ。 \\[ y_i = \\beta_0+e_i \\] そして、フルモデルとモデル0の残差平方和の比を取った統計量は、帰無仮説が正しいときには自由度（\\(k,~n-k-1\\)）の F分布に従うことが知られている。この性質を活かし、回帰分析においては自由度（\\(k,~n-k-1\\)）のF分布を前提とした統計検定を行い、それをF検定（F-test）と呼ぶ。回帰分析結果にて出力される F-statistic:は、この検定統計量の実現値である。なお、この検定の検定統計量は、以下のように示される。 フルモデルとモデル0の残差平方和（\\(SSR_1\\)と\\(SSR_0\\)）をそれぞれ以下のように定義する。 \\(SSR_1=\\sum\\hat{u}^2_i=\\sum\\left[y-( \\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i}+...+\\hat{\\beta}_kx_{ki})\\right]^2\\) \\(SSR_0=\\sum\\hat{e}^2_i=\\sum(y_i-\\bar{y})^2\\) そして、以下の統計量 F は帰無仮説が正しければ、自由度（\\(k,~n-k-1\\)）の F分布に従うことが知られているため、これを検定統計量として用いて検定を行う。 \\[ F=\\frac{(SSR_0-SSR_1)/k}{SSR_1/(n-k-1)}=\\frac{SSR_0-SSR_1}{SSR_1}\\cdot\\frac{n-k-1}{k} \\] 7.5.4 回帰係数の解釈 ここからは、重回帰分析の係数の解釈について説明する。ここで説明する解釈は「なぜ基本的には重回帰モデルを採用すべきなのか」を理解するために重要な内容である。結論から述べると、重回帰分析における説明変数の係数は、「説明変数が持つ変動のうち他の説明変数とは無関係な変動だけを抽出し、被説明変数との関係を分析している」と解釈できる。この特徴が、同モデル内の「他の変数の影響をコントロールしたうえで」説明変数が被説明変数へ与える影響を捉える方法として、学術的にも実務的にも活用されている。 重回帰モデルにおける各説明変数の係数は、パーシャル効果として解釈できる。以下では、このパーシャル効果の直感について、Wooldridge（2013）を参考に説明する。まず、以下のような説明変数が二個である重回帰モデルを考える。 \\[ y_i=\\beta_0+\\beta_1x_{1i}+\\beta_2x_{2i}+u \\] そして、上モデルの予測値は以下のように示すことができる。 \\[ \\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+\\hat{\\beta}_2x_{2i} \\] このとき、説明変数 \\(\\small x_1\\) と \\(\\small x_2\\) の変化を \\(\\small \\Delta x_{1i}\\) と \\(\\small \\Delta x_{2i}\\) とすると、予測値の変化（\\(\\small \\Delta \\hat{y}\\)）は以下のように表すことができる。 \\[ \\Delta\\hat{y}_i=\\hat{\\beta}_1\\Delta x_{1i}+\\hat{\\beta}_2\\Delta x_{2i} \\] ここで、\\(\\small x_2\\) を固定（\\(\\small \\Delta x_{2i}=0\\)）すると、以下を得る。 \\[ \\Delta\\hat{y}_i=\\hat{\\beta}_1\\Delta x_{1i} \\] つまり、重回帰モデルにおける \\(\\hat{\\beta}_1\\) は、別の説明変数をコントロール（\\(\\small \\Delta x_{2i}=0\\)）した上で、\\(\\small x_1\\) が \\(\\small \\hat{y}\\) に与える影響（\\(\\small x_1\\) が変化した際の \\(\\small \\hat{y}\\)の変化の程度）を捉えていると解釈できる。また、\\(\\small \\hat{\\beta}_2\\) についても同様に解釈できる。そしてこの特徴は、k個の説明変数を用いたモデルにも同様に適応できる。 \\(x_{2}\\) を固定すると上では述べたが、このような分析を行った際の\\(x_1\\) と \\(\\beta_1\\) については具体的にどのような変動・効果を捉えているのだろうか。以下では、パーシャル効果についてもう少し詳細に説明することで、係数の解釈についての理解を深める。先ほど重回帰モデルの回帰係数は、「説明変数が持つ変動のうち他の説明変数とは無関係な変動だけを抽出し、被説明変数との関係を分析している」と解釈できると述べた。この点を理解するために、次の2つの説明変数を含む回帰モデルを考える。 \\[ y_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} +\\hat{\\beta}_2 x_{i2}+\\hat{u}_i = \\hat{y_i} + \\hat{u}_i \\] このとき、変数 \\(x_{i1}\\) と \\(x_{i2}\\) の関係は次のように表される。 \\[ x_{i1} = \\hat{\\gamma}_0 + \\hat{\\gamma}_2 x_{i2} +\\hat{r}_{i1} = \\hat{x}_{i1} + \\hat{r}_{i1} \\] ここで、\\(\\hat{r}_{i1}\\) は、\\(x_1\\) を \\(x_2\\) に対して単回帰したときの OLS 残差を表す。つまり、\\(\\hat{r}_{i1}\\) は \\(x_{i1}\\) のうち、\\(x_{i2}\\) と無相関な部分である。これらを総合すると\\(x_{1}\\) の変動は、別の説明変数 \\(x_{2}\\) との関係で説明できる部分（変動：\\(\\hat{x}_{1}\\)）と、\\(x_{2}\\) とは無関係な部分（\\(\\hat{r}_{1}\\)）とに分けて捉えることができる。 上の式に基づき、\\(\\hat{x}_{i1}\\) と \\(\\hat{u_i}\\) の関係について以下を得る。 \\[ \\sum \\hat{x}_{i1} \\hat{u_i} = \\sum (\\hat{x}_{i1} + \\hat{r}_{i1})\\hat{u}_i = \\sum \\hat{x}_{i1}\\hat{u}_i + \\sum \\hat{r}_{i1}\\hat{u}_i = 0. \\] 次に、\\(\\sum \\hat{x}_{i1}\\hat{u}_i\\) は以下のように書き換えられる。 \\[ \\sum \\hat{x}_{i1}\\hat{u}_i = \\sum (\\hat{\\gamma}_0 +\\hat{\\gamma}_2 x_{i2})\\hat{u}_i = \\hat{\\gamma}_0 \\underbrace{\\sum \\hat{u}_i}_{0} + \\hat{\\gamma}_2 \\underbrace{\\sum x_{i2}\\hat{u}_i}_{0} = 0 \\] したがって、\\(\\sum \\hat{r}_{i1}\\hat{u}_i\\) は以下を満たすはずである。 \\[ \\begin{aligned} \\sum \\hat{r}_{i1}\\hat{u}_i = \\sum\\hat{r}_{i1}(y_i -\\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2})\\\\ = \\sum \\hat{r}_{i1} y_i -\\hat{\\beta}_0 \\underbrace{\\sum \\hat{r}_{i1}}_{0} - \\hat{\\beta}_1 \\sum \\hat{r}_{i1} x_{i1} - \\hat{\\beta}_2 \\underbrace{\\sum \\hat{r}_{i1} x_{i2}}_{0}\\\\ = \\sum \\hat{r}_{i1} y_i - \\hat{\\beta}_1 \\sum \\hat{r}_{i1} x_{i1} = 0 \\end{aligned} \\] よって、\\(\\hat{\\beta}_1\\) は次のように表される。 \\[ \\hat{\\beta}_1 = \\frac{\\sum \\hat{r}_{i1} y_i}{\\sum \\hat{r}_{i1} x_{i1}}=\\frac{\\sum \\hat{r}_{i1} y_i}{\\sum \\underbrace{\\hat{r}_{i1}(\\hat{x}_{i1}}_{0} + \\hat{r}_{i1})}=\\frac{\\sum \\hat{r}_{i1} y_i}{\\sum \\hat{r}_{i1}^2} \\] 上記の議論に基づき、\\(\\hat{\\beta}_1\\) は \\(x_2\\) の影響を取り除いた後の \\(y\\) と \\(x_1\\) の関係を示すと解釈できる。この関係を用い、回帰係数の解釈に対するイメージは以下のように示すことができる。下図の（1）は単回帰モデルによって \\(x_1\\) と \\(x_2\\) が持つ\\(y\\) との関係を捉えた構造をイメージしたものであり、（2）は重回帰モデルの係数についてのイメージを捉えたものである。（2）では、\\(x_1\\) と \\(x_2\\) の重複部分（相関）を取り除いた \\(\\hat{r}_1\\) と \\(\\hat{r}_2\\) それぞれと \\(y\\) の関係を捉えていることが伺える。コントロール変数の採用は便利ではあるが、自身が重回帰モデルを採用する際には、どのような条件で変数の効果を捉えているのかについて、モデルに含まれる変数同士の関係から検討する必要がある。特に、7.5.7 で論じる 過剰統制の問題については慎重に考える必要がある。 回帰係数イメージ 7.5.5 単回帰モデルと重回帰モデルの比較 では、このパーシャル効果という重回帰モデルの特徴は、どのように応用できるのだろうか。多くの実証研究では、重回帰モデルの特徴を利用し、「コントロール変数」を用いた分析を行っている。本節では、先程の企業データを用いて、「企業の広告支出が営業利益に与える影響を明らかにする」という問いを考える。まずは、学習的意図から以下のように単回帰分析を実施してみる（通常の論文・レポートであればこのようなプロセスを記載する必要はない）。 reg2 &lt;- lm(operating_profit ~ adv, data = firmdata19) summary(reg2) ## ## Call: ## lm(formula = operating_profit ~ adv, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -450515 -55314 -40160 -1096 599313 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.685e+04 1.050e+04 5.414 2.49e-07 *** ## adv 1.258e+00 2.152e-01 5.846 3.20e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 117200 on 145 degrees of freedom ## Multiple R-squared: 0.1907, Adjusted R-squared: 0.1852 ## F-statistic: 34.18 on 1 and 145 DF, p-value: 3.197e-08 confint(reg2) ## 2.5 % 97.5 % ## (Intercept) 3.609692e+04 77602.573216 ## adv 8.325719e-01 1.683065 分析の結果、広告支出（adv）の係数は正に有意であり、その95%信頼区間は [0.83, 1.68] であることが確認できた（8.325719e-01は0.8325719）。 しかしながら、このモデル化は不十分であり他の要素も考慮すべきである。営業利益に影響を与えうる要因は色々とあり 、実際の研究においては先行研究を参照しつつ、コントロールすべき変数を含める形で回帰モデルを特定する必要がある。しかしながら、ここでは便宜上いくつかの要因にのみ焦点を合わせて簡単に特定化する。本データは主に小売・サービス産業の企業に焦点を合わせている。そのため、対人サービス水準は企業のパフォーマンスに影響を与えうる要因である。そのため、従業員に関する変数（従業員数: emp、パートタイム従業員数: temp）と人件費（labor_cost）をモデルに含める。また、資産合計（total_assets）、研究開発費（rd）もモデルに含める。今回の回帰モデルは以下のように示される。 \\[ \\text{opretating_profit}_i = \\beta_0 + \\beta_1 adv_i + \\beta_2emp_i+\\beta_3temp_i+\\beta_4\\text{labor_cost}_i+\\beta_5\\text{total_assets}_i+\\beta_6rd_i+u_i \\] Rにおいて重回帰分析を実行するのは簡単である。lm(y ~ x1 + x2 + x3) のように \\(+\\) 記号と変数を追加すれば、重回帰モデルとして分析を実行してくれる。 reg3 &lt;- lm(operating_profit ~ adv + temp + emp + labor_cost + total_assets + rd, data = firmdata19) summary(reg3) ## ## Call: ## lm(formula = operating_profit ~ adv + temp + emp + labor_cost + ## total_assets + rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -360570 -27367 -14970 3349 284990 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.168e+04 8.056e+03 2.691 0.00800 ** ## adv -1.430e+00 2.946e-01 -4.852 3.22e-06 *** ## temp -1.866e+00 6.292e-01 -2.965 0.00356 ** ## emp -1.489e+00 7.008e-01 -2.125 0.03533 * ## labor_cost 8.770e-01 1.686e-01 5.202 6.86e-07 *** ## total_assets 3.507e-02 5.823e-03 6.023 1.43e-08 *** ## rd 1.380e+00 5.254e-01 2.627 0.00957 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 78740 on 140 degrees of freedom ## Multiple R-squared: 0.6474, Adjusted R-squared: 0.6323 ## F-statistic: 42.84 on 6 and 140 DF, p-value: &lt; 2.2e-16 confint(reg3) ## 2.5 % 97.5 % ## (Intercept) 5749.50978947 3.760568e+04 ## adv -2.01201729 -8.470087e-01 ## temp -3.10966464 -6.216177e-01 ## emp -2.87467416 -1.038058e-01 ## labor_cost 0.54369783 1.210293e+00 ## total_assets 0.02355591 4.657974e-02 ## rd 0.34159701 2.419018e+00 見ての通り、結果の出力方式も単回帰分析のものとほぼ同様である。回帰係数の結果下にあるモデル適合度については前節を参照して欲しい。 7.5.6 分析結果の解釈と注意点 分析の結果、広告支出の係数は「負」に有意であり、その信頼区間も [-2.01, -0.84] であった。したがって、本データの分析によると、労働や資産に加え研究開発といった側面を一定とすると、広告支出は営業利益に負の影響を与えることがわかった。他の変数に着目すると、従業員数に関する変数はどちらも負に有意であった。一方で、人件費と総資産、研究開発費は正に有意な影響を与えることが示された。これらの結果から、単純に従業員数を増やしても営業利益には負の影響を与える一方で、従業員数を一定とした上で人件費を上げるほうが営業利益が高いことが示された。また、資産や研究開発費も営業利益につながることが示された。 このように、重回帰モデルを採用し複数の説明変数を含めることで、各係数の持つ含意が大きく変わることに注意して欲しい。また、reg2 と reg3の比較のように、特定の説明変数に対応する係数の符号が変わることも珍しくない。そのため、回帰モデルの定式化には非常に慎重になる必要があり、先述の通り、先行研究を参照して必要な変数をコントロールすることが求められる。 また、これらの結果を踏まえて、3.1節で強調した「マネジメントとリサーチの分離」の重要性を思い出して欲しい。あなたが reg2 と reg3の分析を実施したリサーチャーであり、本リサーチのクライアントがマーケティング・広告部門の部長だっとしよう。そして、（1）あなた自身もマーケティング部門の社員であり、クライアントは直属の上司、（2）あなた自身はクライアントとは無関係の立場、という二つの異なる立場に立っている状況を想像して欲しい。（1）の場合、逆恨みによるあなた自身への不利益を恐れて重回帰分析の結果を「ありのまま」伝えられないかもしれない。もちろん研究倫理に基づけば、都合の良い研究成果を作為的に発表することは問題であり、そのようなことはすべきではない。したがって研究成果を「ありのまま」を伝えるべきであり、都合のいい結果を発表する決断をした人（リサーチャー）が悪い。しかしながら、その背後にある利害関係を鑑みれば、reg2 の結果を報告したくなる人がいることも理解できる。個人の判断を批判するだけでなく、このような研究不正を働く誘因や構造についても考慮すべきである。例えば、上記のようなリサーチャーをとりまく利害関係には気をつけなければならない。なお、定量的な分析に慣れている研究者からすると、特別な理由がないにも関わらず重回帰でなく単回帰分析を用いるのは不自然な分析アプローチである。そのため、 もし調査・分析の設計上意図的に単回帰分析を実施する場合には、その意図と有用性をきちんと説明すると良い。 7.5.7 重回帰モデルにおける変数選択 重回帰モデルでは、複数の説明変数を採用することができるため、どの変数をモデルに含めるべきかという点を考察しモデルを特定化する必要がある。このような説明変数の選択に関わる問題として、本サブセクションでは、欠落変数バイアス、多重共線性、過剰統制の三つを紹介する。 7.5.7.1 欠落変数バイアス まず、欠落変数バイアスについてだが、本来含めるべき変数を含めずにモデルを定式化し推定を行うと、推定された係数にバイアスが生じ、OLSEが不偏推定量でなくなるという問題が生じる。また、このバイアスの方向（正負）は欠落された変数と被説明変数との真の関係（欠落された変数が被説明変数に対して持つ母集団での回帰係数）と、モデルに含まれる説明変数と欠落変数との相関で決まる（Wooldridge, 2013）。以下で欠落変数バイアスについての簡単な説明を提示する。 はじめに、以下の式が正しいモデルだと仮定する。 \\[ y=\\beta_0+\\beta_1x_1+\\beta_2x_2+u \\] これに対して\\(x_2\\)を含まずに欠落変数モデルを推定した場合、以下のような結果を得る。 \\[ \\tilde{y}=\\tilde{\\beta}_0+\\tilde{\\beta}_1x_1 \\] このとき、真のモデルの推定値（\\(\\small \\hat{\\beta}_1\\)）と \\(\\small \\tilde{\\beta}_1\\) の間には、以下の関係がある。 \\[ \\tilde{\\beta}_1=\\hat{\\beta}_1+\\hat{\\beta_2}\\tilde{\\delta_1} \\] ただし、\\(\\small \\tilde{x}_2=\\tilde{\\delta}_0+\\tilde{\\delta}_1x_1\\)とする。つまり、\\(\\small \\tilde{\\delta}_1\\) は \\(\\small x_1\\) の \\(\\small x_2\\) に対する回帰係数の推定値である。したがって以下のように、欠落変数モデルでは推定結果に\\(\\small \\beta_2\\tilde{\\delta}_1\\) の分だけバイアスが生じる。 \\[ E(\\tilde{\\beta}_1|x_1,x_2)=E(\\hat{\\beta}_1)+E(\\hat{\\beta_2} \\tilde{\\delta_1})=\\beta_1+\\beta_2\\tilde{\\delta}_1 \\] そして、欠落変数によってバイアスが生じるということが、重回帰分析の重要性を主張する根拠となる。そのため、仮に研究課題上ではあまり重要でない変数であっても、自身の関心のある説明変数の影響を分析するためにコントロール変数をモデルに含めることが重要になる。 7.5.7.2 多重共線性 第二の論点として、単回帰分析の際には存在しなかった多重共線性（multicollinearity）という問題がある。これは説明変数同士の相関が高いことによる推定上の問題であり、Variance Inflation Factor という指標17を使ってその程度を測ることもある。また、マーケティング領域の研究においてはまれに「多重共線性があるから、説明変数を除外すべきだ」という主張を聞くことがある。しかしながら、本書はできる限り説明変数の除外を行わないほうが良いという立場を提示する。詳しくは後述するが、分析の対象となる説明変数によって極端にVIFが高くなる場合には、次の段落にあるようにその変数の意義を検討し、その変数をそのまま残す、何かで比を取る、対数化する（詳しくは次章）などの変数変換の工夫をしながら、その変数をモデルに含めるということが現実的かつ実践的な対応となる。なおこの立場は、本書が回帰分析における係数の検証や解釈に注視していることに大きく依存するため、注意してほしい。 多重共線性について語る際には、(1) 説明変数同士が完全に相関している（相関係数が 1 もしくは -1）場合と、(2) 完璧ではないが相関係数が高い場合、という二つの異なる状況を区別し理解する必要がある。まず一つ目の場合、そもそも係数の推定値が計算できないという問題が生じる。そのため、完璧に相関しあっている変数を同時にモデルに含めることは出来ず、変数の除外を考えないといけない。この問題は言い換えると「同じ変数を同モデル内に複数入れてはいけない」という制約だと理解できる（西山ほか, 2019）。では（2）の場合にはどのような問題が生じるのか。結論から述べると、説明変数同士の相関は、推定量の分散 \\(\\small Var(\\hat{\\beta}_j)\\) を高めてしまう（Wooldridge, 2013）。しかし、 \\(\\small Var(\\hat{\\beta}_j)\\) は、多重共線性だけでなく以下の三つの要素から影響を受ける。第一に、誤差項の真の分散（\\(\\small \\sigma^2\\)）、第二に独立変数間の相関18、第三に、独立変数の変動（\\(\\small SST_j=\\sum(x_{ji}-\\bar{x}_j)^2\\)）である。第一と第二の指標が高い場合には、推定量の分散は大きくなる。一方で、第三の要素である\\(\\small SST_j\\) が高い場合には \\(\\small Var(\\hat{\\beta}_j)\\) は小さくなる。そのため、\\(\\small Var(\\hat{\\beta}_j)\\) を改善するためには、サンプルサイズを大きくし\\(\\small SST_j\\) を大きくすることも有用な対処方法となる。しかしながら、社会科学分野においてはデータ取得可能性の観点からそれが難しいことも多い。そのときには、推定量分散の増加と説明変数除外による弊害とのトレードオフを考慮して意思決定することになる。そのうえで本書は、先述の通りできる限り説明変数を除外することは避けるほうが良いという立場を取る。必要な説明変数を含めずに回帰分析を行うと、欠落変数バイアスの問題が生じる。また、もし分析における主な関心が、着目する説明変数 \\(\\small x_{ji}\\) の \\(y_{i}\\) への効果を検証・解釈することであり、かつそれをきちんと捉えるために他の説明変数の存在が必要なのであれば、VIFは無視して構わない（cf. Wooldridge, 2013）。そのため、「VIFが10以上19だから、多重共線性があり、変数を除外すべきだ」というような考えは、上記の研究上の関心に対しては恣意的であり有意義でないと考える。そもそも先述のように、VIFによって生じる問題は\\(\\small Var(\\hat{\\beta}_j)\\) の増加であるが、この分散は他の要素にも影響を受けるため、VIFが10以上だから必ず \\(\\small Var(\\hat{\\beta}_j)\\) が大きすぎて推定量が有用でないということはない。これらのことから、回帰モデルに含む変数選択は、VIFの値に依存して判断するよりも、分析モデルの意図や用いる変数の理論的・分析的意義について検討し決断されるべきものだと考える。 7.5.7.3 過剰制御 第三に過剰制御（過剰コントロール）は、モデル内での係数解釈の変化を捉えた問題である。ここまでの説明では、欠落変数バイアスの議論を中心に、基本的には変数をモデルに含めることの重要性を説明してきた。しかしながら、重回帰分析の係数の解釈（パーシャル効果）を鑑みると、主要な説明変数の効果に関する理論的なメカニズムにおいて中間経路として機能する変数をモデルに含めることには注意が必要である。例えば、製品開発におけるクラウドソーシングという非専門家（一般消費者）の意見を製品開発に活用する戦略（e.g., Nishikawa et al., 2017）が製品の売上に与える影響を、製品レベルデータを用いて分析する場合を考える。その際、クラウドソーシングの有効性に関するメカニズムとして、実際の製品ユーザーである消費者の意見を反映することで、品質の高い製品を開発でき、結果として売上向上につながるというものを考えているとする。つまり、クラウドソーシングが売上に与える中間経路として、製品の品質が機能するというメカニズムを考えていることになる。しかしながら、売上を被説明変数とする回帰モデルを考える際に、製品品質も売上に影響を与えうる変数なので説明変数として回帰モデルに含めたいと考えるかもしれない。このような考えが、過剰制御の問題につながる。製品品質は、クラウドソーシングから売上への影響に関する中間経路として機能する変数であり、これをコントロールしてしまうことは、クラウドソーシングの係数の解釈を大きく変えてしまう。具体的には、このようなモデルでは、製品品質を一定とした上でクラウドソーシングが売上に与える影響を捉えることになってしまう。上記の関係は以下の図のように示される。 クラウドソーシングと売上の中間経路（品質） 製品品質以外のクラウドソーシングの効果が研究の関心と整合的であるならば、品質を含めた定式化で問題はない。しかしながら、重回帰モデルの変数選択では、自身が論じているメカニズムと係数解釈の整合性を保つように、中間経路変数を含めることで過剰制御の問題に陥っていないかを慎重に検討する必要がある。 重回帰モデルに含める変数の指針について、西山ほか（2019, p.186）は以下の表のように整理している。なお、本書においては主に欠落変数バイアスと係数の解釈という観点から変数選択に関する考え方を整理した。これは、研究上関心のある変数の効果（係数）について検証・解釈するという観点に基づく議論である。しかしながら、「予測」という側面に着目すれば推定や予測の精度を高める（誤差を小さくする）ことが重要になり、変数選択の基準も変わる。そうなれば、欠落変数による問題や係数の解釈の変化はあまり重要でなくなるかもしれない。このように、立場が変わることによって回帰モデルの特定化の基準も変化することを最後に付け加えておく。 Table 7.1: 変数選択の指針 x に影響を与える or x と同時決定 x から影響を受ける x とは無相関 y に影響を直接与える 必ず含める（欠落変数を防ぐ） 含めていけない（過剰制御） 含めることで推定誤差は減る（含めなくてもバイアスは増えない） y に影響を与えない 含めないほうが良い（ただし推定誤差は増えるがバイアスは増えない） 左と同様 左と同様 後述する \\(\\small R^2_{j}\\) を用いて、\\(VIF_j=1/(1-R^2_{j})\\) と定義される。↩︎ より具体的には説明変数 \\(x_{ji}\\)を従属変数に、その他の全ての説明変数を独立変数として回帰分析をした際の決定係数 \\(R^2_j\\)、。なお、先述のVIFはこの\\(R^2_j\\)の関数である。↩︎ 慣習としてよく用いられる閾値↩︎ "],["練習問題-4.html", "7.6 練習問題", " 7.6 練習問題 本章で使った2019年のfirmdataを用いて回帰分析を実行してみよう。その際、自身の着目する被説明変数と「メインの」説明変数をひとつ特定し、仮説を提示すること。そのうえで、自身が着目する二変数間の関係を捉えるために必要なコントロール変数も考え、仮説を検証するための重回帰モデルを特定化し分析を実行しよう。 "],["参考文献-5.html", "7.7 参考文献", " 7.7 参考文献 秋山裕（2018）「Rによる計量経済学 第2版」，オーム社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Nishikawa, H., Schreier, M., Fuchs, C., &amp; Ogawa, S. (2017). The Value of Marketing Crowdsourced New Products as Such: Evidence from Two Randomized Field Experiments. Journal of Marketing Research, 54(4), 525-539. Wooldridge, J. (2013) Introductory Econometrics A Modern Approach,Cengage Learning. "],["regtechnic.html", "Chapter 8 回帰分析上の工夫紹介 ", " Chapter 8 回帰分析上の工夫紹介 "],["本章の概要-5.html", "8.1 本章の概要", " 8.1 本章の概要 7 章では、回帰分析の基本的な構造と重回帰分析を中心に結果の解釈方法について説明した。しかしながら、回帰分析は単純な二変数間の直線関係（傾き）を調べる以上の複雑な分析に応用できる。例えば、マーケティング領域の研究では、説明変数が被説明変数へ与える影響が、別の要因によって変化するか否かも分析するが可能である。着目する二つの異なる変数どちらの方が被説明変数へ与える影響が強いかを比較したいという研究目的を持つこともあるかもしれない。もしくは収穫逓減（逓増）や弾力性といった非線形のモデルを推定したいと考えることもあるだろう。しかしながらこれらの発展的な手法については、その背後にある原則を理解していないと、不適切な手法の活用や結果の解釈に至ってしまう可能性もある。そのため、本章ではこれらの目的を達成するためのモデル定式化や結果の解釈について、その背後にある統計的な理屈も含めて紹介する。具体的には、以下の内容について説明する。 ダミー変数の活用 交差項を用いた回帰モデル 係数比較 対数線形モデル ダミー変数とは、観測個体があるカテゴリに属するなら1を、そうでなければ0を取るような変数のことである。本章ではこのような二値変数を説明変数として用いる方法を紹介する。このような変数を用いた場合、係数の解釈が通常の回帰係数とは異なり、ダミー変数に対応する回帰係数は切片の差、つまり、被説明変数の値に関するグループ間での相対的な高低について示している。 次に扱う方法は交差項を用いたモデルである。マーケティング領域ではよく「調整効果」という表現でこのモデル定式化が用いられる。しかし、研究において誤まった用い方をしているケースも散見されるため、注意が必要である。調整効果という呼称からは、主に着目しているメインの説明変数とその効果を調整する副次的な変数があるかのようなニュアンスが読み取れる。しかしながらモデル化においてそのような扱いの差は存在せず、以下のように定式化される。 \\[ y_i = \\beta_0 + \\beta_1 x_i+\\beta_2z_i+\\beta_3(x_i\\times z_i)+u_i \\] このとき、仮に研究者が \\(x\\) を主たる研究の関心として持っていたとしても、交差項（掛け算の項）で用いる二つの変数についてはどちらも同様に単独項と交差項の両方を含めることが必要となる。なお、このような交差項を用いた定式化では、ダミー変数と連続変数どちらも利用することが可能である。そのうえで本章では、以下の点について注意を促す： 交差項には、条件付き効果に関する作業仮説を論じる必要がある。 例、XがYに与える影響は、Zの値に応じて変化する。 交差項を含むモデルには交差項を構成する二つの変数も含める。 推定モデル上、どちらか一方がメインかのような特定化は行わない。 交差項を構成する二つの変数の係数を従来の回帰分析結果と同じように解釈してはいけない。 説明変数独立項の係数の意味について注意が必要。 第三に扱う手法が係数比較を意図した定式化である。マーケティング領域では稀に異なる説明変数のうちどちらの係数のほうが大きいのかを比較するような議論を行う研究が見受けられる。係数の大小関係を統計的な観点から比較するための現実的な方法のひとつは、説明変数の単位を統一した上で信頼区間を計測することである。第二の方法として本章では、大小比較に関する統計的検定を実行するためのモデル定式化を紹介する。 第四に、マーケティングの枠組みと密接に関わる定式化方法として、対数線形モデルを紹介する。マーケティングでは、需要の価格弾力性など変数間の非線形の関係を分析したいという目的を持つことも多い。回帰分析では、推定される係数について線形の形で扱うことで、分析が可能になる。そのような定式化の方法として、説明変数と被説明変数の両方に対して、自然対数（\\(\\ln\\)）を用いた変数変換を行うことで、以下のようなモデルを設計できる。 \\[ \\ln y_i = \\ln \\alpha+\\beta\\ln x_i+u_i \\] このような定式化は一般的に線形対数モデルと呼ばれ、説明変数に対応する回帰係数は「弾力性」を表すことが知られている。より具体的には、このようなモデルの回帰係数は、説明変数が 1% 増加したとき、被説明変数が何%変化するか（変化率）を表していることに注意が必要である。 また、本章末にRを用いた分析結果表の作成方法も紹介しているので、必要に応じて参照し活用してほしい。 "],["dummy.html", "8.2 ダミー変数", " 8.2 ダミー変数 本節では、説明変数としてカテゴリ変数を用いる場合の方法と、その結果の解釈について説明する。マーケティング領域の研究においては、あるカテゴリに属することが成果変数にどのような影響を与えるのかに関心を持つことも多い。そのような場合には、「ダミー変数」と呼ばれる形にカテゴリ変数を定義し、分析することが多い。ダミー変数とは、特定のカテゴリに属するならば1を、それ以外なら0を取るような変数を指す。例えば、女性ならば1、それ以外の性別であれば0を取るようなダミー変数を、「女性ダミー」として扱うことができる。ダミー変数 D を用いた回帰モデルは以下の様に表すことができる。 \\[ y_i=\\alpha+\\beta x_i + \\gamma D_i+u_i \\] ただし、\\(x_i\\) は連続尺度の説明変数である。ダミー変数は取りうる値が1か0に限定されているため、y の条件付期待値は以下のように解釈できる。 \\[ E(y_i|D_i=1)=\\alpha+\\beta x_i + \\gamma \\] \\[ E(y_i|D_i=0)=\\alpha+\\beta x_i \\] つまり、ダミー変数に対応する回帰係数はベースライン（\\(D_i=0\\)）グループとの「切片の差」を表しているということがわかる。例えば、\\(\\small \\alpha\\)、\\(\\small \\beta\\)と、\\(\\small \\gamma\\)が正の値を取るような場合、上記のダミー変数の関係は以下の図のように示す事ができる。例えば、このダミー変数が女性ダミーであるならば、女性はその他の性別に比べて、y の値が相対的に高い、と解釈できる。 ダミー変数関係図 ダミー変数を説明変数に含む回帰分析をRで実行することはとても簡単である。lm() 関数内のモデル定義において、カテゴリ変数を含めば良いだけである。Characterという文字列情報のデータ型で示されているカテゴリ変数を用いると、自動でダミー変数化して、回帰分析を実行してくれる。 8.2.1 企業データを用いた分析の実行 例えば、本章で使っている firmdata19 を使って分析を行ってみる。具体的には、営業利益率（営業利益/売上）が小売産業に属する企業とそれ以外で異なるかを分析する。しかし、データセット上にこの分類に対応するカテゴリー変数は存在しないため、以下のように、“Retail Stores, NEC” もしくは “Supermarket Chains” のどちらかに含まれる企業であれば1、それ以外であれば0を取るカテゴリー変数（format）を作成する。%in% は二つのベクトル間の要素の一致を確認するための演算子で、下記mutateコマンドでは、ind_en 列に書かれている情報が retail リストに「含まれる」か否かをチェックしている。そのため、以下コマンドでは、事前に作成した retail を参照し、ind_en で観察されるカテゴリーが retail に当てはまれば、\"Retail\"を、そうでなければ、\"Others\" を返すように指示していることになる。 library(tidyverse) firmdata &lt;- readxl::read_xlsx(&quot;data/MktRes_firmdata.xlsx&quot;) firmdata19 &lt;- firmdata %&gt;% filter(fyear == 2019) #産業名を特定したオブジェクト&quot;Retail&quot;の作成 retail &lt;- c(&quot;Retail Stores, NEC&quot;, &quot;Supermarket Chains&quot;) # Retailを使ったカテゴリー変数の作成 firmdata19 &lt;- firmdata19 %&gt;% mutate(format = ifelse(ind_en %in% retail, &quot;Retail&quot;, &quot;Other&quot;)) #カテゴリーの頻度チェック with(firmdata19, table(format)) ## format ## Other Retail ## 98 49 そして、作成したカテゴリー変数も含めて、以下のような回帰モデルを分析する。 fit.d1 &lt;- lm(op ~ mkexp + format, data = firmdata19) summary(fit.d1) ## ## Call: ## lm(formula = op ~ mkexp + format, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.09224 -0.03182 -0.00752 0.01799 0.26461 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.098489 0.008859 11.117 &lt; 2e-16 *** ## mkexp -0.069204 0.025444 -2.720 0.00734 ** ## formatRetail -0.023734 0.009751 -2.434 0.01616 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05454 on 144 degrees of freedom ## Multiple R-squared: 0.1042, Adjusted R-squared: 0.09178 ## F-statistic: 8.377 on 2 and 144 DF, p-value: 0.0003617 上記のように、カテゴリー変数（format）を回帰モデルに含めるだけで、自動的にダミー変数化して分析を実行してくれる。今回はたまたま我々の意図通り Othersをベースライングループに設定されているが、これは、指示した結果ではない。  もし確実に特定のカテゴリーを1と定義したい場合には、自身でダミー変数を作成して回帰分析を行えばいい。 firmdata19 &lt;- firmdata19 %&gt;% mutate(retail = ifelse(format == &quot;Retail&quot;, 1, 0)) #確認 with(firmdata19, table(retail, format)) ## format ## retail Other Retail ## 0 98 0 ## 1 0 49 fit.d2 &lt;- lm(op ~ mkexp + retail, data = firmdata19) summary(fit.d2) ## ## Call: ## lm(formula = op ~ mkexp + retail, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.09224 -0.03182 -0.00752 0.01799 0.26461 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.098489 0.008859 11.117 &lt; 2e-16 *** ## mkexp -0.069204 0.025444 -2.720 0.00734 ** ## retail -0.023734 0.009751 -2.434 0.01616 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05454 on 144 degrees of freedom ## Multiple R-squared: 0.1042, Adjusted R-squared: 0.09178 ## F-statistic: 8.377 on 2 and 144 DF, p-value: 0.0003617 分析の結果、マーケティング支出と小売ダミーのどちらも営業利益率に対して負に有意な影響を持つことがわかった。したがって、小売企業はデータセット内の他の産業よりも利益率が低いといえる。 "],["交差項.html", "8.3 交差項", " 8.3 交差項 マーケティングに関する研究では、ある説明変数の被説明変数に与える影響が、別の説明変数に影響を受ける形で変化することを捉えることも多い。このような変数間の関係に影響するような効果を調整効果（moderating effect）もしくは相互作用効果（interaction effect）と言い、以下のような図で示されることが多い。 ダミー変数関係図 上の図では、メインの説明変数と調整変数が存在するかのように見えるが、分析に置いてはどちらか一方をメインと扱うことはなく、XとZをかけ合わせた交差項を含めたモデルを分析する。なお、調整効果もしくは相互作用効果の分析ではダミー変数もしくは量的変数のどちらも用いることができる。 交差項概念図 回帰モデルにおける交差項の利用には、以下の点に注意が必要である。 交差項には、条件付き効果に関する作業仮説を論じる必要がある。 例、XがYに与える影響は、Zの値に応じて変化する。 交差項を含むモデルには交差項を構成する二つの変数も含める。 推定モデル上、どちらか一方がメインかのような特定化は行わない。 交差項を構成する二つの変数の係数を従来の回帰分析結果と同じように解釈してはいけない。 説明変数独立項の係数の意味について注意が必要。 8.3.1 傾きダミー 先述の注意点について、具体例を見ながら確認していく。まず、量的変数とダミー変数の交差項について考える。ダミー変数と量的変数の交差項を作ることによって、傾きがグループによって変わるか否かを捉えることができる。交差項を含む回帰モデルは以下のように示される。 \\[ y_i = \\beta_0 + \\beta_1 x_i+\\beta_2D_i+\\beta_3(x_i\\times D_i)+u_i \\] \\(\\small x_i\\) が \\(\\small y_i\\) に与える影響を \\(\\small \\Delta y_i/\\Delta x_i\\) とすると、それぞれ以下のように示される。 \\[ \\begin{cases} D_i=1\\Rightarrow &amp; \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3\\\\ D_i=0\\Rightarrow &amp; \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1 \\end{cases} \\] つまり、\\(\\small x\\) が \\(\\small y\\) に与える影響（傾き）が、\\(\\small \\beta_3\\) の分だけ、ダミー変数のカテゴリーによって変化することが伺える。\\(\\small \\beta_1\\) は \\(\\small D=0\\) の際の x の効果であり、 \\(\\small D=1\\) の際の効果は、\\(\\small \\beta_1+\\beta_3\\) で表される。 そのため、このようなダミー変数の使い方を「傾きダミー（Slope dummy）」と呼ぶことも多い。 8.3.2 傾きダミーの実行 先程の firmdara19 にこの分析モデルを適応してみる。Rにおける交差項の導入は、lm(y ~ x * d) のように指定すれば、xとdの交差項とそれぞれの独立項を自動でモデルに含めてくれる。 fit.d3 &lt;- lm(op ~ mkexp * retail, data = firmdata19) summary(fit.d3) ## ## Call: ## lm(formula = op ~ mkexp * retail, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.096735 -0.028600 -0.005523 0.018005 0.258568 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.108430 0.009217 11.764 &lt; 2e-16 *** ## mkexp -0.105662 0.027505 -3.842 0.000183 *** ## retail -0.088147 0.023231 -3.794 0.000218 *** ## mkexp:retail 0.191432 0.063025 3.037 0.002837 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05305 on 143 degrees of freedom ## Multiple R-squared: 0.1585, Adjusted R-squared: 0.1409 ## F-statistic: 8.979 on 3 and 143 DF, p-value: 1.73e-05 分析の結果、マーケティング支出と小売ダミーは負に有意である一方で、両変数の交差項（`mkexp:retail）は正に有意であることが示された。このとき、mkexp の単独項は、retail が0のとき、つまりその他グループにおけるマーケティング支出と利益率の関係を示しており、それが負に有意であると理解できる。そして、mkexp:retail の項が正に有意であることから、その他グループの傾きと、小売グループの傾きは有意に異なることが伺える。もう少し詳細にこの結果を整理すると、本分析による利益率の予測値 \\(\\hat{y}\\)は以下のように示すことができる。 \\[ \\hat{y}_i = 0.108430 - 0.105662\\text{mkexp}_i - 0.088147\\text{retail}_i + 0.191432\\text{mkexp}_i * \\text{retail}_i \\] \\[ \\begin{cases} \\text{Retail}: &amp;\\hat{y}_i = 0.020283 + 0.08577\\text{mkexp}_i\\\\ \\text{Others}: &amp;\\hat{y}_i = 0.108430 - 0.105662\\text{mkexp}_i \\end{cases} \\] 交差項を用いた分析を実行する場合、分析結果をより詳細に理解するため、結果を図示化することが大切になる。具体的には以下のように、分析結果からカテゴリ別に予測値をそれぞれ計算し、図示化することで、含意を得ることができる。ここでは、sjPlotというパッケージの plot_model() 関数を用いて、分析結果に基づく被説明変数の予測値を図示化する。まず、パッケージをインストールして欲しい。 install.packages(&quot;sjPlot&quot;) 今回の図示化のために、plot_model() 関数内では、type = \"pred\" と引数を指定した上で、terms において着目する変数名を特定する。また、この関数では、ggplot2のように、図示化に関する凡例や軸ラベルなどの様々な情報を書き足していくことができる。 library(sjPlot) pred &lt;- plot_model(fit.d3, type = &quot;pred&quot;, terms = c(&quot;mkexp&quot;,&quot;retail&quot;), ci.lvl = .95) + labs(title = &quot;Slope Analysis&quot;, subtitle = &quot;(Predicted Values of Profitability with 95% Confdence Intervals)&quot;, x = &quot;Marketing Expense&quot;, y = &quot;Profitability&quot;) + scale_color_discrete(name = &quot;Retail Dummy&quot;) pred 分析の結果、小売企業においてはマーケティング支出と利益率の関係は右上がりである一方で、その他の企業では右下がりである。回帰分析における交差項の係数がこの傾きの違いを示している。また、実線を比較すると マーケティング支出が 0.5 辺りを境に小売企業の利益率の予測値のほうがその他グループよりも高くなっているように見える20。しかしながら、両直線の95%信頼区間は重なっており、統計的に有意な差があるとは言えない。そのため、マーケティング支出が高いとき、小売企業の利益率（の予測値）のほうが有意に高いとは言えない。 8.3.3 量的変数同士の交差項 交差項を用いた分析は量的変数同士にも応用できる。しかしながら、交差項を用いた分析結果の解釈には注意が必要であり、分析の実行においても工夫が求められる。量的変数同士の交差項モデルとして、以下の回帰式を考える。 \\[ y_i = \\beta_0 + \\beta_1 x_i+\\beta_2z_i+\\beta_3(x_i\\times z_i)+u_i \\] 上記における x と z はどちらも量的変数であり、x が y に与えるパーシャル効果は、以下のように表すことができる。 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3z_i \\] 同様に、z が y に与えるパーシャル効果は、 \\[ \\frac{\\Delta y_i}{\\Delta z_i}=\\beta_2+\\beta_3x_i \\] となる。したがって、x と z が y に与える影響は、互いに依存しあっていることがわかる。上式の \\(\\small \\beta_3\\) は調整効果や相互作用効果と呼ばれる。 交差項を用いた回帰モデルでは、説明変数の独立項に関する解釈に注意が必要となる。例えば上式の \\(\\small \\beta_1\\)（\\(\\small \\beta_2\\)）はどのような条件で x（z）が y に与える影響だと解釈できるだろうか。例えば、上記のモデルにおける y が体重、 x と z がそれぞれ筋肉量と身長だったとする。このとき、筋肉量が体重に与える影響は、 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3z_i \\] であり、\\(\\small \\beta_1\\) は「身長（z）が0」という条件下で筋肉量が体重に与える影響を表している。しかしながら、身長が0のときという非現実的な条件下での結果は、我々にとって解釈が難しく、また情報としても有益でないかもしれない。この問題への対策のひとつが「平均値での中心化（mean-centering）」である。これは、交差項に用いる説明変数に関して、平均からの偏差を用いる方法である。中心化された変数による交差項モデルは以下のように示される。 \\[ y_i = \\beta_0 + \\beta_1 (x_i-\\bar{x})+\\beta_2(z_i-\\bar{z})+\\beta_3(x_i-\\bar{x})\\times (z_i-\\bar{z})+u_i \\] これにより、x の効果は以下のように示される。 \\[ \\frac{\\Delta y_i}{\\Delta x_i}=\\beta_1+\\beta_3(z_i-\\bar{z}) \\] そのため、中心化されたモデルにおける\\(\\small \\beta_1\\) は、\\(\\small z_i-\\bar{z}=0\\)、つまり「z が平均値」の際の、x 効果だと解釈できる。また、平均値以外の値を用いた中心化も当然可能であるため、研究・実務上関心の強い値（何らかの閾値など）がある場合には、それを基準とした中心化もできる。 8.3.4 交差項モデルの実行 8.3.4.1 モデルの推定と検定 ここで、量的変数を用いた交差項モデルをRで分析する。分析の実行方法は、前節の傾きダミーと同様である。ここでは、“headphone07.csv” という、ある年のヘッドフォン製品の売上を捉えたデータセット（人工架空データ）を用いる。このデータは、以下の変数を含んでいる。 売上（百万円） プロモーション投資額（百万円） R&amp;D投資額（百万円） このデータセットに対して、本分析ではR&amp;Dによって向上する（と仮定する）製品品質とプロモーションの相互作用が売上に与える影響を分析する。具体的には、たとえ良いものを作っても、きちんとその情報を消費者に伝達しないといけないのではないかという課題を捉える。そのため本分析を通じて、製品品質への投資（R&amp;D投資）が売上に与える影響が、プロモーション量に応じて変化するのではないかという研究課題に答える。まず以下のようにデータを読み込み、データフレームを確認する。 Headphone07 &lt;- readr::read_csv(&quot;data/headphone07.csv&quot;, na = &quot;.&quot;) #データフレームの確認 glimpse(Headphone07) ## Rows: 221 ## Columns: 4 ## $ ID &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… ## $ sales &lt;dbl&gt; 118.8377, 548.6312, 197.3075, 104.2657, 748.8251, 947.8850, … ## $ rd &lt;dbl&gt; 404.0893, 252.1270, 444.3374, 407.5876, 841.7605, 336.8744, … ## $ promotion &lt;dbl&gt; 75.63163, 102.74572, 97.98040, 83.46613, 105.69250, 80.17476… このデータに対して、まずは以下の通り中心化していない変数を用いて分析を行う。 fit_int &lt;- lm(sales ~ rd*promotion, data = Headphone07) summary(fit_int) ## ## Call: ## lm(formula = sales ~ rd * promotion, data = Headphone07) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.522 -12.861 -0.638 13.578 70.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.033e+04 1.090e+02 186.5 &lt;2e-16 *** ## rd -5.187e+01 2.844e-01 -182.4 &lt;2e-16 *** ## promotion -1.914e+02 1.052e+00 -181.9 &lt;2e-16 *** ## rd:promotion 4.979e-01 2.730e-03 182.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.55 on 217 degrees of freedom ## Multiple R-squared: 0.9935, Adjusted R-squared: 0.9934 ## F-statistic: 1.111e+04 on 3 and 217 DF, p-value: &lt; 2.2e-16 分析の結果、rdとpromotion の交差項は正に有意だが、どちらの独立項も負に有意であった。また、先述の通りこれらの独立項の係数は、もう一方の変数が0のときのそれぞれの効果を表しており、現実的には解釈が難しい結果になっている。したがって、以下の様に中心化変数を作成し、回帰分析を実行する。 Headphone07 &lt;- Headphone07 %&gt;% mutate(promotion_c = promotion - mean(promotion, na.rm = TRUE), rd_c = rd - mean(rd, na.rm = TRUE)) fit_int_c &lt;- lm(sales ~ rd_c*promotion_c , data = Headphone07) summary(fit_int_c) ## ## Call: ## lm(formula = sales ~ rd_c * promotion_c, data = Headphone07) ## ## Residuals: ## Min 1Q Median 3Q Max ## -46.522 -12.861 -0.638 13.578 70.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 382.27812 1.51459 252.40 &lt;2e-16 *** ## rd_c -0.91767 0.01196 -76.75 &lt;2e-16 *** ## promotion_c 0.69039 0.03972 17.38 &lt;2e-16 *** ## rd_c:promotion_c 0.49792 0.00273 182.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 20.55 on 217 degrees of freedom ## Multiple R-squared: 0.9935, Adjusted R-squared: 0.9934 ## F-statistic: 1.111e+04 on 3 and 217 DF, p-value: &lt; 2.2e-16 分析の結果、交差項の結果については非中心化モデルと同じであることが伺える。一方で独立項については、rdは負に、promotionは正に有意であることが明らかになった。つまり、プロモーションが平均値である場合、rdにコストを掛けても売上には繋がらない一方で、rdが平均値の場合、プロモーションによって売上が伸びることが伺える。これをまとめると、品質が平均的ならプロモーションで売上は上がるが、プロモーションが平均的で品質投資をしても売上に逆効果となることが伺える。R&amp;D投資による負の効果については価格との関係もあるかもしれない。品質が向上すると通常価格も上がるため、プロモーションがあまり高くない状態においては、価格の向上によって売上を損ねるかもしれない。この点については、価格も含めたさらなる調査、分析が必要になる。 8.3.4.2 分析結果の図示化 量的変数同士の交差項分析結果の図示化においては、二種類の方法がある。一つ目は調整変数として捉える変数を高水準（例、平均 \\(+\\) 1標準偏差）と低水準（例、平均 \\(-\\) 1標準偏差）に二分し、前節の傾きダミーの図示化のようにそれぞれの場合でメイン変数による被説明変数への傾きを描画する方法である。これは、前節で使用したsjPlotパッケージで実行できる。 二つ目は、メインの変数が被説明変数へ与える影響を縦軸に、調整変数の値を横軸に取ることによって、メイン変数が持つ効果が調整変数によって変化する様子を連続的に描画する方法である。これは、marginaleffectsというパッケージを用いる。 まず、一つ目の方法は、以下の通り実行することができる。分析の結果、やはりプロモーション水準が低い場合にはR&amp;D投資が売上に与える影響は負であるものの、プロモーション水準が高い場合には正に転じることが伺える。 leg = c(&quot;Mean - 1sd&quot;, &quot;Mean&quot;, &quot;Mean + 1sd&quot;) int_fig1 &lt;- plot_model(fit_int_c, type = &quot;int&quot;, mdrt.values = &quot;meansd&quot;, ci.lvl = .9999999999) + labs(title = &quot;Predicted values of Sales (R&amp;D * Promotion)&quot;, x = &quot;R&amp;D Investment&quot;, y = &quot;Sales&quot;)+ scale_color_discrete(name = &quot;Promotion level&quot;, labels=leg) ## Scale for colour is already present. ## Adding another scale for colour, which will replace the ## existing scale. int_fig1 続いて、二つ目の方法を実行するために、以下の要領でパッケージをインストールしてほしい。marginaleffects は、margins や emtrends の機能を引き継ぎ、限界効果に関する計算結果を図示化するためのパッケージである21。 install.packages(&quot;marginaleffects&quot;) R&amp;D が 売上に与える影響（傾き）がどのように変化するかは、plot_slopes() という関数を用いる。以下は、傾きの変化に関する図の出力結果である。 library(marginaleffects) int_fig2 &lt;- plot_slopes(fit_int_c, variables = &quot;rd_c&quot;, condition = &quot;promotion_c&quot;, conf_level = .99999999) + labs(title = &quot;Marginal effects of R&amp;D on Sales&quot;, x = &quot;Promotion&quot;, y = &quot;Slope of R&amp;D on Sales&quot;) + geom_hline(aes(yintercept=0), linetype = &quot;dashed&quot;) int_fig2 分析の結果、promotionが0（平均）である条件を基準に、R&amp;Dの傾きが正に転じていることが伺える。なお、よく見ると、直線の周りに灰色の影が描画されていることがわかる。これは、99%信頼区間を示している。今回は人工的に作成したデータなので、非常に当てはまりがよく信頼区間がとても狭くなっているが、実際のデータを用いて同様の図示化をすればもう少し明確に信頼区間を視認できる。このように、交差項を利用した回帰モデルを分析する際は、事後的な図示化を行うことを心がけると良い。この作業により実務的・学術的により有益な含意につながることがある。 以下は、plot_slopes() に関するおまけである。図示化の確認のために、firmdata19を使って、マーケティング支出と総資産との交差項により、利益率を説明する回帰モデルを考える。以下では、中心化と回帰分析の実行、図の出力を実施している（回帰分析結果は省略）。 firmdata19 &lt;- firmdata19 %&gt;% mutate(mkexp_c = mkexp - mean(mkexp), asset_c = total_assets - mean(total_assets)) fit_int2 &lt;- lm(op ~ mkexp_c * asset_c, data = firmdata19) int_fig3 &lt;- plot_slopes(fit_int2, variables = &quot;mkexp_c&quot;, condition = &quot;asset_c&quot;, conf_level = 0.99) + geom_hline(aes(yintercept=0), linetype = &quot;dashed&quot;) int_fig3 図を見ると、Headphoneデータよりもはっきりと信頼区間が視認できる。マーケティング支出の効果は資産によって低下するように見えるが、信頼区間を考慮すると、資産額が平均値（asset_c \\(=0\\)）付近の値を取るときのみマーケティング支出は負に有意の影響を持つが、それ以外の区間では有意でない（信頼区間に0を含む）ことが伺える。このように、信頼区間の出力は結果の解釈を有意義なものにしてくれる。 試しに、plot_model 内の信頼区間に関する引数を ci.lvl = NAとし、実線のみの比較を確認して見て欲しい↩︎ 本講義では、詳細は割愛するが以下から詳細を確認できる https://vincentarelbundock.github.io/marginaleffects/dev/↩︎ "],["係数比較.html", "8.4 係数比較", " 8.4 係数比較 マーケティング領域においては、異なる説明変数のうちどちらの係数のほうが大きいのかを比較するような議論を行う研究が稀に見られる。しかし、その多くの場合において、（1）係数の推定値をそのまま比較することや、（2） 片方の検定結果が有意でありもう一方は有意でないというような検定結果の比較をもとに大小関係を論じている。しかしながら、たとえ説明変数の単位を（標準化などで）統一していたとしても、これら二つのような比較によって大小関係を結論づけるのは不十分である。係数の大小比較に関する現実的な方法のひとつは、説明変数の単位を統一した上で信頼区間を計測することである。以下では、もうひとつの方法として、大小比較に関する統計的検定を実行するための工夫を紹介する。 まず、以下のような被説明変数を\\(Y\\)、説明変数を \\(X_1,~X_2\\)とする重回帰モデルを考える。 \\[ Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + u \\] 係数の大小比較において重要となるのは、上式内の \\(\\alpha_1\\) と \\(\\alpha_2\\) の差である。つまり、\\(\\alpha_1 - \\alpha_2 = 0\\) であれば両者に差がないことになる。ここで、\\(\\theta = \\alpha_1 - \\alpha_2\\) と定義し、回帰モデルを以下のような修正版モデルに書き換える。 \\[ Y = \\alpha_0 +(\\theta - \\alpha_2) X_1 + \\alpha_2 X_2 + u \\\\ = \\alpha_0 +\\theta X_1 + \\alpha_2 (X_1 + X_2) + u \\] つまり、この式のように \\(X_1\\) と \\((X_1 + X_2)\\) という二つの説明変数を用いた重回帰モデルを推定すると、修正版における\\(X_1\\)の係数は \\(\\theta = \\alpha_1 - \\alpha_2\\) として解釈する事が可能になる。そしてこの修正版モデルにおける \\(X_1\\)の係数を検定することで、\\(\\alpha_1 - \\alpha_2=0\\)を帰無仮説とした検定と同義の結果を得ることができ、大小関係に関する含意を得ることができる。 8.4.1 係数比較の実行 ここでは例として、企業の広告投資とR&amp;Dへの投資が売上に与える影響について比較する。分析にはfirmdata19を用いる。このデータの広告とR&amp;D変数の単位はともに百万円であり揃っているが、本書では教育的意図から変数を標準化するプロセスを提示し、標準化した変数を用いる。そのため、分析結果の係数解釈には注意が必要になる。本書ではまず、以下の通り変数を作成した後、通常の重回帰モデルを実行する。 firmdata19 &lt;- firmdata19 %&gt;% mutate(adv = (adv -mean(adv))/sd(adv), rd = (rd -mean(rd))/sd(rd), ad_rd = adv +rd) fit_linear &lt;- lm(sales ~ adv + rd, data = firmdata19) summary(fit_linear) ## ## Call: ## lm(formula = sales ~ adv + rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4356502 -459524 -238384 126951 2695201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1194514 73839 16.177 &lt;2e-16 *** ## adv 1627341 74544 21.831 &lt;2e-16 *** ## rd 30111 74544 0.404 0.687 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 895200 on 144 degrees of freedom ## Multiple R-squared: 0.7709, Adjusted R-squared: 0.7677 ## F-statistic: 242.3 on 2 and 144 DF, p-value: &lt; 2.2e-16 これを見ると、一見広告投資のほうが係数が大きそうである。では次に、ad+rd を用いた係数比較モデルを分析することでこの差が統計的に有意かを検討する。 fit_comp &lt;- lm(sales ~ adv + ad_rd, data = firmdata19) summary(fit_comp) ## ## Call: ## lm(formula = sales ~ adv + ad_rd, data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4356502 -459524 -238384 126951 2695201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1194514 73839 16.177 &lt;2e-16 *** ## adv 1597230 111072 14.380 &lt;2e-16 *** ## ad_rd 30111 74544 0.404 0.687 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 895200 on 144 degrees of freedom ## Multiple R-squared: 0.7709, Adjusted R-squared: 0.7677 ## F-statistic: 242.3 on 2 and 144 DF, p-value: &lt; 2.2e-16 分析の結果、adv の独立項の係数が有意であったため、二つの係数は有意に異なると理解できる。しかしながら、このような係数の比較を行う際には、その背景にある社会的実務的比較可能性について考慮する必要がある。例えば、特定の産業において、広告への投資もしくはR&amp;Dへの投資をしづらい状況はないだろうか。反対に多くの企業が広告への投資を行っているが、R&amp;Dには投資がされていないという状況は無いだろうか。仮に多くの企業がすでに広告への投資を十分に行っているならば、その中で広告支出額が1単位（1標準偏差）増やすことの意味は非常に大きいはずである。したがって、たとえ分析上変数間の比較が可能であったとしても、その比較がどのような意味をもつのか、もしくはその比較はフェアなものなのかという点については慎重に議論・検討する必要がある。 "],["対数線形.html", "8.5 対数線形", " 8.5 対数線形 ここまでの回帰モデルの特定化では、一次関数を利用していた。しかしながら、以下の図のように傾きの大きさが途中で変化するような非線形の関係には一次関数のモデルは上手くフィットしない。 非線形例 非線形の関係として例えば、以下のような関数形がある。 \\[ Y=AL^\\alpha K^\\beta \\] ここで、Yは生産量、Lは労働投入量、K は資産を仮定すると、上記の式は、経営学分野にも応用される、経済学におけるコブダグラス型の生産関数として知られている。しかしながら、このような関数をそのまま線形回帰分析に当てはめることはできない。そこで、右辺と左辺両側の変数に対して、自然対数を用いた変数変換を行うことで、回帰分析で推定可能なモデルを構築する。このような変換された説明変数（例、\\(\\small \\ln K_i\\)）を用いる定式化であれば、推定される係数については線形の形で扱うことができる。対数による変数変換を伴う線形回帰モデルは、一般に対数線形モデルと呼ばれる。 \\[ \\ln Y_i = \\ln A + \\alpha\\ln K_i+\\beta\\ln L_i + u_i \\] 8.5.1 対数線形モデルの分析実行 このような回帰式をR上で構築することは難しくない。lm() 関数内のモデル定式化において、log() を用いれば良い。例えば、上の式を firmdata19 に当てはめて、Yを売上、Lを人件費、K を有形固定資産とおいて、以下のようにモデルを推定してみる22。 fit_prod &lt;- lm(log(sales) ~ log(labor_cost) + log(ppent), data = firmdata19) summary(fit_prod) ## ## Call: ## lm(formula = log(sales) ~ log(labor_cost) + log(ppent), data = firmdata19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4058 -0.3725 -0.0108 0.3870 1.6627 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.17985 0.31763 10.01 &lt;2e-16 *** ## log(labor_cost) 0.53793 0.03684 14.60 &lt;2e-16 *** ## log(ppent) 0.34438 0.02797 12.31 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5176 on 144 degrees of freedom ## Multiple R-squared: 0.8735, Adjusted R-squared: 0.8718 ## F-statistic: 497.2 on 2 and 144 DF, p-value: &lt; 2.2e-16 このような対数線形モデルでは、これまでのような一次関数モデルとは異なる係数の解釈を行う必要がある。両側対数の回帰モデルにおける回帰係数は、「弾力性」を表している。つまり係数は、説明変数が 1% 増加したとき、被説明変数が何%変化するか（変化率）を表している。また、この係数解釈の特徴から、マーケティング分野においても対数線形モデルはよく用いられる。具体的には、対数線形モデルによって需要の価格弾力性（価格が1%増えたときに、需要量が何%減るか）を分析することができる。 需要の価格弾力性（\\(\\eta\\): イータ）は、消費者による価格感度を捉える指標である。通常、他の要素を一定とした場合、価格が低いと需要量は上がる。では、どの程度価格の低下が需要量に影響を与えるのかという問いは、実務的にも学術的にも重要になる。その際に、単位に依存しない価格感度の尺度として最も広く使われているものが、需要の価格弾力性である。また弾力性は、1を重要な閾値として実務的に重要な含意を与える。弾力性値に応じた含意は以下のように整理することができる。 \\(\\small \\eta= 1\\) の場合: 価格が1%下がった時に需要量が1%上がることを意味する。価格と需要の変化率がちょうど釣り合った状態。 \\(\\small \\eta &gt; 1\\) の場合: 価格の上昇によって需要量が著しく減る（価格に敏感に反応する）。 価格の上昇によって売上を損ねる（需要量の損失が価格上昇の便益を上回っている） 。 \\(\\small \\eta &lt; 1\\) の場合: 価格の上昇によって需要量があまり減らない（価格に敏感に反応しない） 価格の上昇によって売上が伸びる（価格上昇の便益が需要量の損失を上回っている） 。以下では、対数線形モデルと需要の価格弾力性との関係に関する補足的な説明を追加している。 8.5.2 需要の価格弾力性追加説明 ここでは、需要の価格弾力性についてもう少し詳細な説明を加える。興味のない場合は読み飛ばしてもらって構わない。需要の価格弾力性は、需要量（q）と価格（p）とし、以下の \\(\\eta\\)のように定義できる。 \\[ \\eta=-\\frac{dq}{dp}\\times \\frac{p}{q} \\] この価格弾力性の定義に基づき、なぜ１が重要な閾値になるのかを説明する。 価格の変化による売上の変化\\(dpq/dp\\) の関係を変形することで \\(\\small \\eta\\) について以下を得る。 \\[ \\eta=\\frac{dpq}{dp}=-p\\cdot\\frac{dpq}{dp}+p=q\\left[1-\\left(-\\frac{dq}{dp}\\cdot\\frac{p}{q}\\right)\\right]=q(1-\\eta) \\] そのため、\\(\\small \\eta\\) が１より小さい時、価格の増加による売上の変化が正（\\(dpq/dp&gt;0\\)）であることが伺える。 8.5.3 対数線形の推定と検定 ここで、需要の価格弾力性に関する分析を実行するために、“price_data.csv” という人工データを用いる。このデータは製品の価格と需要量の情報を含んでいる。以下のようにデータを読み込み、分析を実行してほしい。 price &lt;- readr::read_csv(&quot;data/price_data.csv&quot;) fit_q &lt;- lm(log(q) ~ log(p), data = price) summary(fit_q) ## ## Call: ## lm(formula = log(q) ~ log(p), data = price) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.11220 -0.13668 0.02804 0.16166 0.51039 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.47781 0.08135 116.5 &lt;2e-16 *** ## log(p) -0.18008 0.01354 -13.3 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2309 on 998 degrees of freedom ## Multiple R-squared: 0.1506, Adjusted R-squared: 0.1497 ## F-statistic: 176.9 on 1 and 998 DF, p-value: &lt; 2.2e-16 分析の結果、log(p) の係数は負に有意であった。しかしここで、デフォルトで出力される検定結果の「帰無仮説」を思い出して欲しい。これは、係数が0か否かの検定を行っている。しかしながら、我々は係数が 1 より高いか、低いかに興味がある。そのため、\\(\\small H_0:\\beta = 1\\)という帰無仮説による検定を採用すべきである。このような検定の実行には、色々とやり方はあるが、本書では carというパッケージのlinearHypothesis() 関数 を用いる方法を紹介する。まず、以下のようにパッケージをインストールして欲しい。 install.packages(&quot;car&quot;) そして、linearHypothesis()において、参照する分析結果と、着目する変数（の係数）とその値について入力することで、検定を行う。 library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some linearHypothesis(fit_q, c(&quot;log(p) = -1&quot;)) ## Linear hypothesis test ## ## Hypothesis: ## log(p) = - 1 ## ## Model 1: restricted model ## Model 2: log(q) ~ log(p) ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 999 248.811 ## 2 998 53.225 1 195.59 3667.3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 検定の結果、fit.q による係数の推定値は、有意に -1 とは異なることがわかった。つまり、弾力性は 1 より低く、非弾力的である（価格にあまり敏感でない）ことが伺えた。なお、一つの説明変数の有意性検定にF検定を用いる場合のF分布は対応するt分布の二乗値に等しい。そのため、car による検定では、F値を返しているが、それ自体に問題はない。 最後に、回帰モデルにおいて対数化を右辺（説明変数）と左辺（被説明変数）のどちらに適用するかによって、係数の解釈が変わることを説明する。モデルの特定化と係数解釈の対応関係は、以下のように整理できる。 両側対数（\\(\\ln y_i = \\alpha + \\beta\\ln x_i + u_i\\)）: \\(\\beta\\) は x が 1%変化したときの y の変化率を表している。 被説明変数のみ対数（\\(\\ln y_i = \\alpha + \\beta x_i + u_i\\)）: \\(\\beta\\) は x が 1単位変化したときの y の変化率を表している。 説明変数のみ対数（\\(y_i = \\alpha + \\beta\\ln x_i + u_i\\)）: \\(\\beta\\) は x が 1%変化したときの y の変化量を表している。 なお、実際に研究において生産関数を推定する場合には、変数の特定化に関して、先行研究に基づきより慎重に検討する必要があるため注意して欲しい↩︎ "],["おまけ分析結果のまとめ出力上の工夫.html", "8.6 （おまけ）分析結果のまとめ・出力上の工夫", " 8.6 （おまけ）分析結果のまとめ・出力上の工夫 これまで本書では、分析結果について、Rで出力されるものをそのまま示していた。しかしながら、Rでの出力結果をレポートや論文にベタ貼りすることは、可読性の低下につながるため好ましくない。論文やレポート執筆の際には、Rで出力された結果から表を作成することが必要になるのだが、Excelなどのスプレッドシートに一つ一つの値をコピー・アンド・ペースト（コピペ）して表を作成していく方法は、操作ミスによる間違いの可能性が高まることと、作業プロセスを記録できない上に、何より面倒くさい。そこで、分析結果を読みやすく見た目も良い表の形式で楽に出力してくれるコマンドをR上で実行するした。そのために用いるパッケージは色々あるが、本書では modelsummary パッケージを 紹介する。以下のようにインストールして欲しい。 install.packages(&quot;modelsummary&quot;) ここでは試しに、本節で実行した fit_intを msummary() 関数で出力する。Rstudio上でmsummary() を実行すると、結果がコンソールではなく、Viewerに表示される。 library(modelsummary) ## `modelsummary` 2.0.0 now uses `tinytable` as its default table-drawing ## backend. Learn more at: https://vincentarelbundock.github.io/tinytable/ ## ## Revert to `kableExtra` for one session: ## ## options(modelsummary_factory_default = &#39;kableExtra&#39;) ## options(modelsummary_factory_latex = &#39;kableExtra&#39;) ## options(modelsummary_factory_html = &#39;kableExtra&#39;) ## ## Silence this message forever: ## ## config_modelsummary(startup_message = FALSE) msummary(fit_int, statistic = &#39;conf.int&#39;) tinytable_q03mvfj38fu9wgcoho9s .table td.tinytable_css_83phy0kdt0miq6dvdrj0, .table th.tinytable_css_83phy0kdt0miq6dvdrj0 { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_wffv3iys844na9kk6488, .table th.tinytable_css_wffv3iys844na9kk6488 { text-align: left; } .table td.tinytable_css_qbemzp6wiflgxlh3fenw, .table th.tinytable_css_qbemzp6wiflgxlh3fenw { text-align: center; } .table td.tinytable_css_4z9dmmlsau139c1a4r6e, .table th.tinytable_css_4z9dmmlsau139c1a4r6e { border-bottom: solid 0.05em black; } (1) (Intercept) 20326.588 [20111.797, 20541.378] rd -51.872 [-52.433, -51.312] promotion -191.431 [-193.505, -189.358] rd × promotion 0.498 [0.493, 0.503] Num.Obs. 221 R2 0.994 R2 Adj. 0.993 AIC 1969.2 BIC 1986.2 Log.Lik. -979.617 F 11110.648 RMSE 20.36 上記のように引数をほぼ指定せずとも（statistic = 'conf.int' は信頼区間を出力する引数。デフォルトでは標準誤差）、このような表形式で結果を出力することができる。引数を用いることで、研究者に都合の良い表なるよう編集可能であることもこの関数の利点である。例えば、不必要な情報があれば、以下のように表から削除することもできる。 msummary(fit_int, gof_omit = &quot;Log.Lik.|AIC|BIC|RMSE&quot;) tinytable_2okx431ou7we7fnel41r .table td.tinytable_css_pqeyht3kmgfe5eniqa2t, .table th.tinytable_css_pqeyht3kmgfe5eniqa2t { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_o9xksnxrvin8x75zq86x, .table th.tinytable_css_o9xksnxrvin8x75zq86x { text-align: left; } .table td.tinytable_css_a42a1fgntdy32yumypoy, .table th.tinytable_css_a42a1fgntdy32yumypoy { text-align: center; } .table td.tinytable_css_gp7avl2ifi8c7br88lse, .table th.tinytable_css_gp7avl2ifi8c7br88lse { border-bottom: solid 0.05em black; } (1) (Intercept) 20326.588 (108.978) rd -51.872 (0.284) promotion -191.431 (1.052) rd × promotion 0.498 (0.003) Num.Obs. 221 R2 0.994 R2 Adj. 0.993 F 11110.648 また、msummary() は二つ以上のモデルを並べて表示させる場合にも便利である。複数の分析結果を list としてまとめ以下のように出力できる。ここでは、交差項を用いた回帰モデルにおける中心化のありなしを併記し比較可能にする。更に、表示される変数名も調整する。 var_nam &lt;- c(&quot;rd&quot; = &quot;R&amp;D&quot;, &quot;promotion&quot; = &quot;Promotion&quot;, &quot;rd:promotion&quot; = &quot;R&amp;D * Promotion&quot;, &quot;rd_c&quot; = &quot;R&amp;D_c&quot;, &quot;promotion_c&quot; = &quot;Promotion_c&quot;, &quot;rd_c:promotion_c&quot; = &quot;R&amp;D_c * Promotion_c&quot;, &quot;(Intercept)&quot; = &quot;定数項&quot;) Int &lt;- list() Int[[&quot;Without centering&quot;]] &lt;- fit_int Int[[&quot;With centering&quot;]] &lt;- fit_int_c msummary(Int, coef_map = var_nam, title = &quot;Comparing Interaction Models&quot;, notes = &quot;Values in [ ] show 95% confidence intervals&quot;, stars = TRUE, statistic = &#39;conf.int&#39;, conf_level = .95, gof_omit = &quot;Log.Lik.|AIC|BIC|RMSE&quot;) tinytable_4wn6ixteqn90jen2htmr .table td.tinytable_css_uzq246gxfd2n00v4xd9f, .table th.tinytable_css_uzq246gxfd2n00v4xd9f { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_n9ckjqslxhatkqr9d23s, .table th.tinytable_css_n9ckjqslxhatkqr9d23s { text-align: left; } .table td.tinytable_css_o9n1a6h67uk7vmn3ez2k, .table th.tinytable_css_o9n1a6h67uk7vmn3ez2k { text-align: center; } .table td.tinytable_css_x562f3be0xy751vi2wgw, .table th.tinytable_css_x562f3be0xy751vi2wgw { text-align: center; } .table td.tinytable_css_nuscvgb6q34hm5q5m5th, .table th.tinytable_css_nuscvgb6q34hm5q5m5th { border-bottom: solid 0.05em black; } Comparing Interaction Models Without centering With centering + p Values in [ ] show 95% confidence intervals R&D -51.872*** [-52.433, -51.312] Promotion -191.431*** [-193.505, -189.358] R&D * Promotion 0.498*** [0.493, 0.503] R&D_c -0.918*** [-0.941, -0.894] Promotion_c 0.690*** [0.612, 0.769] R&D_c * Promotion_c 0.498*** [0.493, 0.503] 定数項 20326.588*** 382.278*** [20111.797, 20541.378] [379.293, 385.263] Num.Obs. 221 221 R2 0.994 0.994 R2 Adj. 0.993 0.993 F 11110.648 11110.648 対外的に分析結果を共有する場合には、このような分析結果をViewer上ではなく、文書ファイル等に貼り付けたいと考えることも多いだろう。msummary() 関数では、引数を変更することで、出力形式を変更することも可能である。そのうえで、本書が最もおすすめする方法が、Latex形式での表の出力と、それを用いたLatexによる文書作成である。 Latex（ラテック or レイテック）は、ソースファイルにコマンドと文章を記載していきながらpdfを出力することで文書を作成するためのツールである。Latexは、数式や表の入力が簡単であり、出力結果もきれいであることや、文書フォーマットについて細かく気にせず文書を作成できるという利点がある。そのため、定量的な分析を伴う文書を作成するのに適したツールである。逆に短所としては、ソースコードの入力や、コンピュータ内へのLatex用環境構築に関する手間がかかるという点が挙げられる。しかし近年では、ウェブブラウザ上でLatexを動かせる “Overleaf” というサービスも普及し、環境構築による問題は解決されている。本書では、LatexやOverleafの詳細や設定方法については割愛するが、基本的にはOverleafの活用をおすすめするので、以下のリンクを参照して欲しい（https://dreamer-uma.com/overleaf/）。特に、卒業論文や修士論文に取り組む学生にとって、Overleafによってウェブ上にファイルを保管できることは非常に重要な利点となるだろう（毎年少なくない学生が論文のファイル保管ミスやノートPCの故障などのトラブルに見舞われるのを観察している）。 先程の交差項比較の表について、output = \"latex\" と引数を設定することで、出力形式を変更する。 msummary(Int, title = &quot;Comparing Interaction Models&quot;, notes = &quot;Values in [ ] show 95% confidence intervals&quot;, stars = TRUE, statistic = &#39;conf.int&#39;, conf_level = .95, gof_omit = &quot;Log.Lik.|AIC|BIC|RMSE&quot;, output= &quot;latex&quot;) tinytable_xx0g3hblibhg447kg8r5 .table td.tinytable_css_lzytcp18bmoxssjr5bvd, .table th.tinytable_css_lzytcp18bmoxssjr5bvd { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_9zpe22kvfowskp73divf, .table th.tinytable_css_9zpe22kvfowskp73divf { text-align: left; } .table td.tinytable_css_4prdrcpmhv8p5icnp4xd, .table th.tinytable_css_4prdrcpmhv8p5icnp4xd { text-align: center; } .table td.tinytable_css_xu9ftb7jeoz60zixb2fs, .table th.tinytable_css_xu9ftb7jeoz60zixb2fs { text-align: center; } .table td.tinytable_css_yk767yz5ysfi98ghmj9l, .table th.tinytable_css_yk767yz5ysfi98ghmj9l { border-bottom: solid 0.05em black; } Comparing Interaction Models Without centering With centering + p Values in [ ] show 95\\% confidence intervals (Intercept) \\num{20326.588}*** \\num{382.278}*** [\\num{20111.797}, \\num{20541.378}] [\\num{379.293}, \\num{385.263}] rd \\num{-51.872}*** [\\num{-52.433}, \\num{-51.312}] promotion \\num{-191.431}*** [\\num{-193.505}, \\num{-189.358}] rd × promotion \\num{0.498}*** [\\num{0.493}, \\num{0.503}] rd\\_c \\num{-0.918}*** [\\num{-0.941}, \\num{-0.894}] promotion\\_c \\num{0.690}*** [\\num{0.612}, \\num{0.769}] rd\\_c × promotion\\_c \\num{0.498}*** [\\num{0.493}, \\num{0.503}] Num.Obs. \\num{221} \\num{221} R2 \\num{0.994} \\num{0.994} R2 Adj. \\num{0.993} \\num{0.993} F \\num{11110.648} \\num{11110.648} これにより出力される結果のうち、\\begin{table} から \\end{table} までのコード内容をそのままlatexにコピペし、コンパイル（文書出力）をすれば、以下のような表が出力される23。 Latex出力例 いくらLatexが便利だと言っても、マーケティング分野においてはマイクロソフトWordによって文書を作成している教員や研究者も多い。そのため、指導教官や共同研究者との兼ね合いでLatexやOverleafを使えないという場合もある。そのため、もしどうしてもスプレッド形式で表を作成・編集したいと考えるのであれば、Googleスプレッドシートの拡張機能を利用するのが良いだろう。Google スプレッドシートに対応した “Spread-Latex”（https://workspace.google.com/marketplace/app/spreadlatex/218144906748） というアドオンによって、Latexコードで記載された表のソースコードをスプレッドシートに変換することができる。この拡張機能を利用すれば、ウェブブラウザで開いたGoogle スプレッドシート上にmsummary() によって出力された Latex コードを貼り付け、いくつかのクリック操作をするだけで、Latexコードからスプレッドシート型の表に変換できる。その後、少しの微調整（消えていないコードの削除など）を行うと、以下のような表を得る。 Google スプレッドシート出力例 もしExcelで作業を行いたい場合には、このGoogleスプレッドシートに変換された表をExcelにコピペすれば良い。しかしながら、2023年現在ではこのアドオンの更新は続いているが、今後デベロッパーがどのように対応するかは不明であるため、これはあくまで一時的な対応策にとどめておくことが賢明かもしれない。 以上のように、既存のパッケージやサービスを用いることで、少ない労力で分析結果を表にまとめ、出力することが可能になる。ここで紹介した以外にも有力なパッケージがあるので、関心のある人は色々と調べて、自分にあったやり方を探してみて欲しい。 なお、Latexにおいては % 記号によってコメントアウトされるため、ソースコード内にある場合には、バックスラッシュを用いる調整が必要になる。↩︎ "],["練習問題-5.html", "8.7 練習問題", " 8.7 練習問題 Rの“wooldridge”パッケージ内にある、“wage2” というデータセットを用いて分析を実行してみよう。 wage2 は935個の観測と17の変数を含むデータセットである。本データセットの変数リストは以下のCRANウェブサイト（https://search.r-project.org/CRAN/refmans/wooldridge/html/wage2.html ）に以下のように示されている： wage: monthly earnings hours: average weekly hours IQ: IQ score KWW: knowledge of world work score educ: years of education exper: years of work experience tenure: years with current employer age: age in years married: =1 if married black: =1 if black south: =1 if live in south urban: =1 if live in SMSA sibs: number of siblings brthord: birth order meduc: mother’s education feduc: father’s education lwage: natural log of wage このデータを利用し、賃金に関するが含意を得てみよう。なお、データセット “wage2” は以下の手順で利用できる。 install.packages(&quot;wooldridge&quot;) library(wooldridge) data(&quot;wage2&quot;) "],["参考文献-6.html", "8.8 参考文献", " 8.8 参考文献 秋山裕（2018）「Rによる計量経済学 第2版」，オーム社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Arel-Bundock V (2023). marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests. R package version 0.13.0.9002, https://vincentarelbundock.github.io/marginaleffects/. Wooldridge, J. (2013) Introductory Econometrics A Modern Approach,Cengage Learning. "],["choice.html", "Chapter 9 消費者の選択と離散選択モデル ", " Chapter 9 消費者の選択と離散選択モデル "],["本章の概要-6.html", "9.1 本章の概要", " 9.1 本章の概要 マーケティング施策を新たに導入したり、戦術要素（例えば、4Ps）を変更したりすることはマーケティング意思決定において重要な問題である。その際、マーケティング意思決定者は、その変更によってどの程度消費者の自社製品への反応が変化するかに関心を持つだろう。 ここで、マーケティングリサーチによって需要の変化を予測したいのだが、どのように行えばよいのだろうか？1つのアプローチはアンケートを用いて現在の利用状況を捉えつつ、マーケティングについての変更がなされた際の利用意向について実験的なシナリオや刺激を利用して調査をするというものである。しかし、アンケートを用いて実際の購買や選択においてマーケティング要素がどのような影響を与えるのか、そしてそれが変化した際に需要がどのように変化しそうなのかを分析し予測することは難しい。そこで、マーケティングリサーチでは、「製品を購買したか否か」や、複数ある製品の候補から「どのブランドを選んだか」というような観察可能な行動の結果を情報として収集し活用することも多い。 このような選択行動は離散データとして扱われる。例えば、ある製品を購買していれば 1 を、していなければ 0 を取るような離散変数として定義される。マーケティングリサーチでは、このようなデータに対して「購入をした人は何%か？」というような円グラフを描くだけではなく、統計的なモデルを用いることで価格や広告などのマーケティング戦略要素が個人の選択へ与える影響について分析を行う。具体的には、消費者の行動を捉えた離散変数を被説明変数とし、マーケティング戦略要素などを説明変数とする分析モデルを構築する。このような分析モデルは、離散選択モデルと呼ばれ、マーケティングのみならず、交通や都市計画分野などにおいても頻繁に用いられている手法である。 このような分析に対してRでは、プロビットモデルやロジットモデルを実行するための関数を用いて分析が可能である。しかしながら、分析の実行が容易であることに反してこれらの分析モデルでは、引数の設定や出力される結果の解釈、ひいてはモデルそのものに内包されている理論的前提について注意も必要である。そのため本章では、以下の項目について説明する。 選択結果に関する情報から消費者行動を類推するための理論枠組み 線形回帰モデルの応用可能性と問題点 プロビット・ロジットモデルの統計学的説明 離散選択モデルの消費者理論への応用 多項選択モデル 第一に、離散選択モデルに基づき消費者行動を研究するための理論的枠組みを紹介する。消費者が何を考えて行動（選択）に至ったのかについて観察することはできない。そのため、本章で扱うアプローチでは、消費者の選択という観察可能な行動に着目しつつ、観察できない消費者の思考については、理論によって補完するという方法を取る。より詳細には、「首尾一貫した選択を分析の対象とする」という仮定を置くことで、消費者の選択行動結果に基づき、本来観察できない選択肢の効用を推察できるようになる。したがって、離散選択モデルに基づく含意を得るためには、その背後に存在する理論的枠組みに対する理解が必要になる。そこで本章では、消費者の選好に関する理論（顕示選好）と効用最大化との関係について説明する。 第二に、統計的な分析に着目し、離散変数を被説明変数とする回帰モデルを最小二乗法で推定する場合について説明する。このような方法は線形確率モデルと呼ばれ、変数間の関係を検証することに着目する際には意図的に採用されることもある。線形確率モデルによって推定された係数は、説明変数が変化したときに被説明変数（ダミー変数）が1を取る確率（反応確率）が、どのように変化するかを示す。そのため、説明変数と被説明変数の関係を捉えるという目的のもとでは線形確率モデルも一定の役割を果たすことに加え、最小二乗法の方が好ましい統計的性質を満たす手法（操作変数法や固定効果推定など）を応用できるという利点も有している。 しかしながら線形確率モデルの最も大きな問題として、推定結果に基づく予測値が論理的整合性を満たさないというものがある。具体的には、線形確率モデルによって推定された予測値は反応確率を示しているはずであるにも関わらず、予測値が1を上回ったり、負の値を取ることがある。そのため、予測を重視する研究を行う場合や、効果の程度や推定値についての議論が重要である場合には線形確率モデルは適さない。 第三に、線形確率モデルではない推定方法としてプロビットモデルとロジットモデルを紹介する。本章では主にプロビットモデルを中心に説明を行っているが、どちらのアプローチでも非線形の（回帰直線ではなく、回帰曲線を引く）分析を行うことで、推定結果に基づいて計算される反応確率が 0 から 1 の範囲を越えないように調整される。この統計的なモデルの定式化について、まずは選択行動という点に着目し反応確率の観点から説明する。 その後潜在変数アプローチと呼ばれる、観察可能な離散変数の背後に、観察できない連続的な変数が存在するという視点からの定式化を紹介する。マーケティングにおける研究群では、多様な背景を持った研究者が論文を書いているため、同じ離散選択モデルを用いた論文でもその定式化の説明方法が異なる場合がある。その際の手助けになるように、本章では上記の二つの異なる定式化アプローチを紹介する。その後、最尤法を用いてプロビットモデルを推定する方法を紹介する。ここではRでの分析実行方法と結果の解釈についても組み合わせて説明する。 ここまでの内容で、消費者の選択を巡る理論と、統計的な離散選択モデルを説明した。これらの内容を踏まえ、第四に離散選択モデルの消費者理論への応用について説明する。具体的には効用最大化という理論枠組みから効用関数の構造についてモデル化し、それを分析する手順や考え方について説明する。効用の大小関係に基づきプロビットやロジットモデルといった離散選択モデルを定式化する考え方は照井・佐藤（2022）などのマーケティングリサーチのテキストでも紹介されている。しかし本章では、これらの議論を巡る理論や統計的モデルについて順を追って確認してきた。これは、効用最大化を前提として用いられる選択モデルがなぜ、どのように定式化されるのかを理解することを意図して構成している。 本章後半には、選択肢が三つ以上の場合の選択モデルとして、多項ロジットモデルの概要とRを用いた分析方法を紹介している。また、最後には離散選択モデルの推定から実務的含意を得るための具体例も紹介しているため、こちらも合わせて確認してほしい。 "],["消費者選択と合理的消費者像概観.html", "9.2 消費者選択と合理的消費者像概観", " 9.2 消費者選択と合理的消費者像概観 本章で扱う離散選択モデルは、消費者の観察可能な行動に着目し、消費者の選択行動などをモデル化することでその選択行動を定量的に分析、予測を行うことを目的としたアプローチである。消費者がなぜ特定の製品を買ったのか、どのような思考プロセスを経て購買に至ったのか、という頭のなかで起きている潜在的な心理プロセスは観察できないため、分析対象を「観察可能」な顕在的行動に限定し、分析を行う。しかしながら、このようなアプローチにおいても「消費者が何を考えているのか」という問いを無視しているわけではない。離散選択モデルにおいては主に経済学における理論に依拠することで、観察可能な行動に着目することの正当性を確保している。 離散選択モデルが依拠する理論的枠組みにおいて消費者は「合理的な行動」をとる人として捉えられている。ここにおける合理的な行動とは、自身の効用を最大にする行動と言える。このような前提に対し「消費者が自分の望む欲求の強さを測って効用として表すと想定するのは現実的ではない。」や「仮に効用を測れたとしても、いちいち買い物をする際に効用の最大化計算なんて行っていない」といった反論を考えるかもしれない。実際にこのような批判が一部のマーケティングや消費者行動関連のテキストに記載されることもある。しかしながら、このような批判は誤解であり、離散選択モデルにおける合理的消費者像は、このような指摘が想定しているものとは異なる。本節では、理論を直感的に概観することで、合理的な消費者を想定することの背後にある理論的根拠について学ぶ。 ここでいう合理的な消費者とは、「自分が何を望んでいるのかをわかっていて、かつ利用できる機会を最大限に活用する個人」を指す。言い換えると、合理的とは自身の目的に沿った行動を取る個人を捉える。つまり、自身の満たすべき欲求がわかっており、商品についてその欲求をどれだけ満たすかという尺度である効用から考えることができる消費者と言い換えることができる。そのため、消費者は欲求を満たすという目的を実現するように（つまり効用を最大にするように）購買を行うと考える。また効用最大化については、本アプローチが想定する理論では、必ずしも消費者自身が購買において効用最大化問題を解いていることを意味しない。効用最大化の仮定は、消費者の選択がある一定の仮定を満たす場合、その行動について「あたかも」効用を最大化するように決めたものとして、研究者が分析、再現、予測することが可能であるという理論に依拠している。本節では、この理論的枠組みについて直感的に説明する。 効用について直感的に理解するためには消費者の選好についての議論を整理することが重要になる。選好とは、与えられた選択肢に対してどちらが自身にとって好ましいかを表すものである。 ここで、ある選択対象 \\(x,y\\) をもとに、選好関係の表記方法について整理する。任意の選択対象に対する選好関係は通常「\\(\\succsim\\), \\(\\sim\\), \\(\\succ\\)」といった記号で表される24。まず、「\\(x\\succsim y\\)」は「\\(x\\) は \\(y\\) よりも同等以上に好ましい（弱く選好する）」ことを表す。次に、\\(x\\succsim y\\) と \\(y\\succsim x\\) の両方が成り立つとき、「\\(x\\sim y\\)」として「\\(x\\) は \\(y\\) と同じぐらい好ましい（無差別である）」ことを表す。そして、\\(x\\succsim y\\) だが、\\(y\\succsim x\\)ではないとき、「\\(x\\succ y\\)」として「 \\(x\\) は \\(y\\) よりも厳密に好ましい（強く選好する）」ことを表す。 表 9.1は、消費者の弱選好（\\(x\\succsim y\\)）、無差別（\\(x\\sim y\\)）、強選好（\\(x\\succ y\\)）の間の関係を表したものである。\\(x\\succsim y\\) と \\(y\\succsim x\\) の両方が成り立つ時、\\(x\\) と \\(y\\) は無差別であると言える。しかしながら、\\(x\\succsim y\\)（\\(y\\succsim x\\)）は成り立つが、\\(y\\succsim x\\)（\\(x\\succsim y\\)）は成り立たない場合、\\(y\\)（\\(x\\)）よりも厳密に \\(x\\)（\\(y\\)）を好むといえる。また、ここではどちらの弱選好も成り立たないような状況は排除して考える。 Table 9.1: 選好関係 \\(x\\succsim y\\) \\(y\\succsim x\\) 選好関係 成り立つ 成り立つ \\(x\\sim y\\) 成り立つ 成り立たない \\(x\\succ y\\) 成り立たない 成り立つ \\(y\\succ x\\) 成り立たない 成り立たない このケースは排除 このような表記を行う選好に関する議論は様々な個人の好みを許容するものであるが、「完備性」と「推移性」という首尾一貫性に関する性質を満たすと仮定する。完備性とは、どんな選択対象 \\(x,y\\) についても、\\(x\\succsim y\\)もしくは \\(y\\succsim x\\) の少なくとも一方が成り立つことである。つまり、表9.1 にもあるように\\(x \\succsim y\\) と \\(y \\succsim x\\) の両方が成り立たないというケースを排除しており、これは「\\(x,y\\) のいずれが好ましいかの判断ができない」ということを排除しているものであると理解できる。 一方で推移性とは、選択対象 \\(x,y,z\\) について、\\(x\\succsim y\\) かつ \\(y\\succsim z\\) ならば \\(x\\succsim z\\) が成り立つことを意味している。 選好に関するこれらの性質について例を用いて考えてみる。例えば、コーラとコーヒーがある場合、必ずどちらが好ましいか一方を選べるというのが完備性である。また、ここで、コーラとコーヒーという選択肢にお茶が加わった場合を考える。コーラとコーヒーを比べてコーラの方が好ましく、かつコーヒーとお茶を比べてコーヒーの方が好ましいと考えている消費者は、コーラとお茶を比べた場合、やはりコーラの方が好ましいと考えるだろう。このように一貫した順位が成立することが推移性である。これらの首尾一貫性が成立しているとはつまり、選択対象すべてを一番良いものから一番悪いものまで順番に並べることができるということを意味する。 ここまでの議論に基づき合理的行動を説明すると、「首尾一貫した好みの下、自身にとって最も望ましいものを常に選択することである」と言える。しかしながら、より分析上扱いやすい形でこの関係を表すためには、どちらが好きかではなく、数量的な大小関係で表すことができた方が便利である。そこで、選好関係に整合的な「効用」という値を割り当てることで見やすく表現する。このとき、選好関係と整合的な効用を割り当てるために用いられる関数が「効用関数」である。 効用と効用関数の関係においては、それぞれの選択対象 \\(x\\) に、効用値 \\(u(x)\\) を、「より好ましいものほど大きな数値を持つように」割り当てる。つまり、\\(x\\succsim y\\) ならば、\\(u(x)\\geq u(y)\\) となる \\(u(\\cdot)\\) を割り当てる。このように選好に整合的な効用を割り当てるためのルールを「効用関数」と呼ぶ。ここで重要になるのは、効用の値自体は選好関係と整合的であれば良く、表現される数値そのものは特に重視しないということである。例えば先程のように \\(x\\succsim y\\) という選好関係を効用で表す場合、\\(u(x)=10\\) かつ \\(u(y)=1\\) でも、\\(u(x)=1000\\) かつ \\(u(y)=500\\) でもどちらでも構わない。つまり、どの値を使っても選好関係と結論が変わらないように効用関数を用いることが重要になる。消費者の脳内の効用（満足）の大きさは測れないため、効用自体の絶対的な大きさには意味がなく、効用とはあくまでどちらが好きかを表す工夫と考える。このような考え方に基づく効用関数を「序数的効用関数」という25。 このように、効用関数を用いて選好関係を効用の大小関係で表現する事によって、消費者の合理的行動は「自身の効用を最大化する行動」と説明することができる。先程の選好関係に基づく合理的行動の定義と比べるとだいぶ簡潔な定義になっただろう。ミクロ経済学やマーケティングの講義、テキストなどで紹介される「自身の効用を最大にする」という言葉の背景には、このように消費者の選好との密接な関係がある。 ここまでの説明を踏まえて改めて、「自身の効用なんて数値化できない！」「いちいち効用の計算をして、大小関係を比較することを考えて選択していない！」という批判に立ち戻りたい。先述の通り、このアプローチにおける合理的消費者像は、決して人間が毎回効用最大化計算を行って行動を決定していると主張しているわけではない。ここで想定している合理的消費者像は、ある規則性を満たすならば（本当の頭の中はどうあれ）、「あたかも」効用最大化を行った結果であるとみなすことができるというものである。つまり、合理性は選択を行う消費者自身ではなく、選択を理解しようとする（観察者や研究者などの）第三者がそのように捉えるということを表している。 このような合理的消費者像を正当化するためにはある仮定を満たす必要がある。具体的には、消費者の選択行動はセンの\\(\\alpha\\)条件と呼ばれる性質を満たす必要がある26。\\(\\alpha\\) 条件は、選択対象になりうる集合 \\(A\\) と、\\(A\\) のどのような部分集合 \\(B\\) に対しても、\\(A\\) から選択される選択肢 \\(x\\) が \\(B\\) に含まれているならば、その部分集合である \\(B\\) から選択されるものもまた\\(x\\)であるというような性質である。例えば、「兵庫県にある全ての居酒屋の中から一番好きな店を教えて」と言われた際に選ばれるお店が神戸市にあるならば、「神戸市にある全ての居酒屋の中から一番好きな店を教えて」言われて選ばれるお店も同じものである、ということを表している。 消費者の選択行動がこの条件を満たすことと、これを満たすような選択行動が完備性と推移性を満たす選好として合理化可能であることは同値であるということが知られている。つまり、このような性質を満たす選択行動に対しては、実際に消費者の頭のなかで何が起きているかに関わらず、効用最大化行動としてその消費者の行動を再現（観察・分析・予測）できると考えることができる。このような考え方は、顕示選好として知られている。そして、このような考え方の妥当性は顕示選好定理という数学的命題として示されているが、その詳細や証明についてはここでは取り扱わない。 記号\\(\\succ\\)は、不等号を表す記号「\\(&gt;\\)」とは異なることに注意すること。↩︎ 有限な選択肢における選好関係が完備性と推移性を満たす場合なら序数的効用関数で表現できる。選択肢の数が有限でない場合の問題について、これらを議論したり数学的証明を与えるのは、本書のレベルを超える上に目的からも外れるので、扱わない。↩︎ ただし、ここでは議論を簡単にするために、ある選択肢の中から1つを選択するということを仮定している。この仮定を緩めて、「\\(\\{x,y,z\\}\\) なら \\(x\\) か \\(y\\) のいずれかを選ぶ」という選択も許した場合に議論を拡張するには、センの\\(\\beta\\) 条件と呼ばれる追加的な条件が必要になる。\\(\\beta\\) 条件は、集合 \\(A\\) とその任意の部分集合 \\(B\\) に対して、\\(B\\) において複数の選択肢（\\(x,y\\)）が選ばれる時、もし集合 \\(A\\) からの選択において \\(x\\) が選ばれるならば、\\(y\\) も選ばれるという性質である。この \\(\\beta\\) 条件は、例えば「神戸市にある全ての居酒屋の中から一番好きな店を教えて」と言われて甲乙つけ難い飲み屋が2店（\\(x\\) と \\(y\\) とする）選ばれた場合、「 兵庫県にある全ての飲み屋の中から一番好きな店を教えて」と聞かれてその人が \\(x\\) と答えるならば、やはり \\(y\\) も兵庫県のベストとして選ばれることになるということを表している。↩︎ "],["選択行動と効用最大化についての例.html", "9.3 選択行動と効用最大化についての例", " 9.3 選択行動と効用最大化についての例 本節では、消費者の選択行動と効用最大化との関係について具体例を用いて説明する。ある消費者の行動例に焦点を合わせ、どのような消費者が分析可能な考察対象になるのかを説明し、次にその消費者の選択をあたかも効用最大化で表現できることを紹介する。 9.3.1 Step 1: どのような消費者を考察の対象にするのか 経済学において対象となる消費者像を捉えるために、東京都で居酒屋に行こうとしているある人物の選択を取り上げてみる。今、この人が行こうとしている居酒屋の候補は3件あると仮定し、その店をそれぞれ \\(x,y,z\\) と表す。この人は選択肢 \\(\\{x,y,z\\}\\) の中から、\\(x\\)を選択しようとしているとする27。 ここで、この人が居酒屋 \\(z\\) が定休日だったことを思い出したとする。 このとき選択肢は \\(\\{x,y,z\\}\\) から \\(\\{x,y\\}\\) へと変化するが、彼は選択肢 \\(\\{x,y\\}\\) のうちいずれを選択するだろうか？おそらく \\(x\\) を選択するのが自然だろうと考えられる28。この場合、この人物の選択はセンの \\(\\alpha\\) 条件を満たすと仮定することができる。 一方で、この人が選択肢 \\(\\{y,z\\}\\) に直面したときには居酒屋 \\(y\\) を選択したとする。 このとき、センの \\(\\alpha\\) 条件から、彼が様々な選択肢に直面したときの選択が、次の表のように観察もしくは予測できることになる。 表の中で \\(\\hat{x}\\) で示されている部分は、\\(\\alpha\\)条件から導かれる予測選択である。 Table 9.2: \\(\\alpha\\)条件を満たす選択 選択肢 選択結果 {\\(x,y,z\\)} \\(x\\) {\\(x,y\\)} \\(\\hat{x}\\) {\\(y,z\\)} \\(y\\) {\\(x,z\\)} \\(\\hat{x}\\) 9.3.2 Step 2: 選好モデルと効用 本節では、先述の消費者に関する選好関係に対応する効用についての議論を紹介する。なおここでは、単純化のために2つの選択肢からの選択結果に注目する。ある消費者の2つの選択肢からの選択が以下のような結果であったとする。 \\(\\{x,y\\}\\) からは \\(x\\) \\(\\{y,z\\}\\) からは \\(y\\) \\(\\{x,z\\}\\) からは \\(x\\) したがって、この人は \\(x\\) と \\(y\\) との比較では \\(x\\) 好むと解釈でき、\\(x \\succ y\\)と表すことができる。つまり、2つの選択肢からの選択結果という情報から、この消費者の選好を導き出すことが可能である。その他の選択肢への結果も踏まえると、\\(x\\)、\\(y\\)、\\(z\\) に対するこの消費者の選好関係は \\(x \\succ y \\succ z\\)であり、この人は「\\(x\\) を1番目、\\(y\\) を2番目、\\(z\\) を3番目に選好する」消費者だと理解できる。 これらの結果から、この人はどのような2つの選択肢に対しても、いずれが好ましいかを判断できており[完備性が成り立っているといえる。この完備性の成立には、Step 1において明示的には述べなかった「選択肢が与えられたときにどれかを選んでいる」という仮定によって満たされている。]、選択肢を好きな順番に1列に並べることができている[推移性が成り立っている。この「1列に並べることができる」という性質は、Step 1における \\(\\alpha\\) 条件が成り立つという仮定に依存していることに注意して欲しい。Step 1で登場した \\(\\alpha\\) 条件が成り立たず、\\(\\{x,z\\}\\) からは\\(z\\)を選択するとしよう。このとき、\\(x \\succ y\\)、\\(y \\succ z\\) であるが \\(z \\succ x\\) となってしまい、“\\(x \\succ y \\succ z \\succ x...\\)” という形でサイクルが起こってしまう。]。 Step 2の最後に、この選好順序を数値の大小で表現する。 ここで、効用の値自体は選好関係に整合的であれば良いので、数字の当てはめ方は特に問題にならない。ここでは仮に、この人の選好関係「\\(x \\succ y \\succ z\\)」を、効用関数「\\(u(x)=6,u(y)=2,u(z)=1\\)」で表すことにする29。 9.3.3 Step 3: 効用最大化 Step 2で効用関数の考え方を導入したが、効用を最大にするというプロセスがどのように選択行動と関係するか確認する。合理的意思決定で論じられる「自身の効用を最大にする行動」について、Step 2で導入した効用関数「\\(u(x)=6,u(y)=2,u(z)=1\\)」のもとで考える。 この効用関数を持つ人が、たとえば\\(\\{x,y\\}\\)という2つの選択肢から選択しなければならないとき、効用を最大にする選択肢は、\\(u(x)=6&gt;2=u(y)\\)なので、\\(x\\)である。 あるいは、\\(\\{x,y,z\\}\\)という3つの選択肢から選択しなければならないとき、効用を最大にする選択肢は、\\(u(x)=6&gt;2=u(y)\\)および\\(u(x)=6&gt;1=u(z)\\)なので、\\(x\\)である。 このように、選択肢が与えられたときに、効用の大小比較を行って効用を最大にする選択肢を調べた結果が表9.3 である。 Table 9.3: 効用最大化による選択再現結果 選択肢 効用を最大にする選択 {\\(x,y,z\\)} \\(x\\) {\\(x,y\\)} \\(x\\) {\\(y,z\\)} \\(y\\) {\\(x,z\\)} \\(x\\) 表9.2と表9.3 を比較すると、「選択結果」と「効用を最大にする選択」が一致していることが確認できる。\\(\\alpha\\) 条件を満たす人物の選択行動を、効用最大化というルールで再現することができた。つまり、本アプローチにおける消費者行動は、消費者の頭の中でなにが起こっているのかについて理解するのは困難であるため、実際に何を選んでいるのかという行動に着目し、効用最大化から消費者の行動を再現するというアプローチを取っている。 このような理論的な議論は、消費者の行動を分析するための重要な示唆を与える。消費者による「首尾一貫した選択を分析の対象とする」という仮定を置くことで、消費者の選択行動結果に基づき、本来観察できない選択肢の効用を推察できるようになる。例えば、消費者のある選択肢に対する選択結果と各選択肢の属性（価格など）に関する情報は観察可能である。これらの観察可能な情報に基づき分析を行うことで、選択肢の持つ属性が効用、ひいては選択肢の選択確率にどのような影響を与えるかを定量的に明らかにすることができる。 この選択を\\(y\\)や\\(z\\)に変えても、以下の議論は同様に成り立つ。↩︎ これが自然だと言いきれるかどうかについては議論の余地もあるが、言い換えると、このような状況を満たす選択がこのアプローチに基づく分析の対象だといえる。↩︎ 繰り返しになるが、数値の大小は選好する順番だけを表現したものであって、たとえば “\\(y\\)は\\(z\\)の2倍の効用を得るが、\\(x\\)だったら6倍である” などということは意味しない。これ以外にも、たとえば「\\(u(x)=2,u(y)=1,u(z)=0\\)」という効用関数を用いても、同じ選好関係を表現できている。↩︎ "],["離散選択モデル.html", "9.4 離散選択モデル", " 9.4 離散選択モデル 9.4.1 線形確率モデル 前節末で説明したような選択肢の持つ属性によって消費者の選択行動を説明するためには、被説明変数に選択結果をとり、説明変数に選択肢属性を取る以下のような回帰モデルを考える。なお、簡単化のために単回帰モデルで本アプローチを紹介しているが、以下の議論は任意の \\(k\\) 個の説明変数を含む重回帰モデルに応用することができる。 \\[ y_i=\\beta_0+\\beta_1x_i+e_i, \\] ただし、\\(y\\) は特定の製品を購入したか（\\(y_i=1\\)）、購入していないか（\\(y_i=0\\)）を表しており、\\(x_i\\) は価格を表しているとする。分析においては \\(y_i\\) と \\(x_i\\)についての情報を含むデータセットを用いて、パラメータ \\(\\beta_0\\) と \\(\\beta_1\\) を推定する。通常、（他の要素が一定である場合）価格が上がると製品を購入（選択）する確率は下がると考えられるので、\\(\\beta_1&lt;0\\) が予測される。 このように消費者の選択を捉えた分析においては、被説明変数がダミー変数である回帰モデルを考える必要がある。しかしながら、ダミー変数のような離散変数を被説明変数に用いる場合には注意も必要である。本節では、このようなモデルを最小二乗法（OLS）で推定するアプローチとその際に生じる注意点を説明したうえで、それを克服するために用いられる分析手法を紹介する。 被説明変数がダミー変数であるモデルは線形確率モデル（Linear Probability Model: LPM）と呼ばれる。ただし、\\(y\\) が2値しか取らないため、\\(\\beta_1\\) は、「\\(x_i\\) が1単位変化した際の \\(y\\) の変化」として解釈することはできない。 ここで、誤差項の条件付き期待値について 0 であるという仮定（\\(E(e_i|x_i)=0\\)）に基づくと、\\(y_i\\) の条件付き期待値について以下を得る。 \\[ E(y_i|x_i)=\\beta_0+\\beta_1x_i \\] ここで、\\(y\\) は 0 か 1 を取るダミー変数なので、\\(P(y_i = 1|x_i)= E(y_i|x_i)\\) と示すことができる（Wooldridge, 2003）。したがってLPMでは、以下のように条件付き確率について線形にモデル化したものだと理解できる。 \\[ P(y_i = 1|x_i)=\\beta_0+\\beta_1x_i \\] このとき、\\(P(y_i = 1|x_i)\\) は反応確率（response probability）や成功確率（probability of success）と呼ばれる。また、確率の合計は 1 になるため、\\(P(y_i = 0|x_i)=1-P(y_i = 1|x_i)\\) もまた \\(x_i\\) に関する線形の関数になる。LPMにおいて定数項（\\(\\beta_0\\)）は \\(x\\) が0のときの反応確率を表しており、傾きの係数 \\(\\beta_1\\) は \\(x_i\\) の変化に伴う反応確率の変化を表していると解釈できる。より具体的には、\\(\\beta_1\\) は以下のように表現できる。 \\[ \\Delta P(y_i = 1|x_i)=\\Delta\\beta_1x_i \\] このことから、\\(y\\) の予測値（\\(\\hat{y_i} = \\hat{\\beta}_0+\\hat{\\beta}_1x_i\\)）も線形回帰モデルと同様に示すことができる。したがって、係数の解釈について注意が必要ではあるものの、LPMによる推定結果も線形回帰モデル同様の含意を提供する。特に、複数の説明変数を採用することで、他の変数を固定したうえで説明変数の変化に伴う反応確率の変化を捉えることができる。そのため、LPMによって着目する変数が反応確率へ与える影響を検証することができる。 しかしながら、LPMの推定結果には注意も必要である。具体的には、以下の2点について問題が生じる。 誤差項の分散が不均一になる。 予測値が論理的整合性を満たさなくなる。 第一の問題点については、LPMで推定してしまうことで分散均一という、OLS推定量が好ましい性質を持つための仮定を満たさないことにつながる。分散が均一であるとは、誤差項の分散がどの観測個体 \\(i\\) に対しても同じ大きさであることを指す。そのため、特定の主体だけ誤差項の分散が大きい場合や、\\(x\\) の値の変化に伴って誤差項の分散が大きくなるような状態ではこの仮定は満たされず、分散不均一であると言われる。ここで、上記のモデルの誤差項は \\(e_i= y_i - (\\beta_0+\\beta_1x_i)\\) である。ここで、\\(y_i\\) の条件付き期待値を \\(P_i=E(y_i|x_i)=P(y_i = 1|x_i)=\\beta_0+\\beta_1x_i\\) と定義すると、誤差項の分散は \\(Var(e_i|x_i)=P_i(1-P_i)\\) となることが知られている（西山ほか,2019）。\\(P_i\\) の大きさはその定義より各主体によって \\(x_i\\) に依存する形で変化することが伺える。そのため、LPMによる推定では、分散不均一の問題が生じるといえる。しかしながらこの問題は通常、分散不均一に対して頑健な標準誤差（例えば、ホワイトの標準誤差）を用いた分析を用いることで対応される。 第二に予測値の論理的整合性について説明する。LPM推定によって得た \\(y\\) の予測値は、反応確率の予測値を表すため、 \\(y\\) の予測値は 0 から 1 の間に収まらないといけない。しかし、LPMでは予測値が負の値を取ったり、1 を上回ることもある。言い換えると確率の定義に反するような、論理的に整合的ではない予測値を返してしまう。ここで、以下のような簡単な人工データを用いて、LPMによる分析を実行してみる。下記df1の成果変数（Y）はダミー変数であり、説明変数は連続変数だとする。これを線形モデルで回帰し、予測値を出力してみる。すると、1つ目と4つ目の観測は負の値、7つ目と8つ目の観測は1を越える予測値を得たことが伺える。 df1 &lt;- data.frame(Y = c(0, 0, 0, 0, 0, 1, 1, 1, 1), X = c(3.4, 5.22, 7.06, 2.81, 4.11, 10.34, 13.67, 15.99, 9.09)) lpm1 &lt;- lm(Y ~ X, data = df1) pred_lpm1 &lt;- predict(lpm1) pred_lpm1 ## 1 2 3 4 5 6 ## -0.006342894 0.173357681 0.355032986 -0.064597476 0.063760077 0.678888967 ## 7 8 9 ## 1.007681776 1.236750640 0.555468243 図 9.1 は、このデータのプロットとLPMで推定した際の回帰直線との関係の例を示している。これでも、いくつかの観測主体によって確率の範囲を越えた予測値を得ていることが伺える。 Figure 9.1: LPM と回帰直線 そのため、予測を重視する研究を行う場合には、LPMは適さないことが多い。その一方で変数間の関係を検証することを目的とする場合には、大きな問題にはならないと考える場合もある（Angrist and Pischke, 2014）。言い換えると、ある説明変数が選択確率に与える影響を統計的に検証するという目的のもとでは、LPMでも対応可能である。特に、操作変数法や固定効果推定といった発展的な手法が線形モデルでは開発されており、LPMではこれらの手法を応用することができる。そのため、効果の検証に重きを置く分析の場合にはLPMを意識的に選択する場合もある。 9.4.2 プロビットモデルとロジットモデル 任意の \\(k\\) 個の説明変数を用いたLPMの反応確率は、 \\[ P_i=E(y_i|x_{1i},...,x_{ki})=P(y_i = 1|x_{1i},...,x_{ki})=\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki} \\] となる。ここまでの議論において、線形確率モデル（LPM）の問題は、この確率が 0 から 1 の範囲を越えてしまう事にあった。この問題は、\\(y\\)（選択）と \\(x\\)（説明変数）との関係を線形で捉えることが原因となっている。そのため、0 から 1 の範囲を越えないように、何らかの累積分布関数を用いて非線形で分析を行う（回帰直線ではなく、回帰曲線を引く）ことで、この問題を克服する事ができる。 非線形なモデルとして分析する場合、回帰モデルによる反応確率は以下のように示すことができる。 \\[ P(y_i = 1|x_{1i},...,x_{ki})=F\\left(\\beta_0+\\beta_1x_i+...+\\beta_kx_{ki}\\right) \\] ただし、\\(F(\\cdot)\\)は選択（反応確率）と説明変数を非線形な形で結びつけるためのなんらかの関数である。ここで採用する\\(F(\\cdot)\\)によって回帰曲線の形状が決まる。 例えば、図 9.2は図9.1との対比として（説明変数が一つのモデルを）非線形でのモデル化を示したものである。図 9.2のように、非線形の近似曲線で回帰分析を行うことで、確率を意味する予測値が0から1の範囲に収まる。 Figure 9.2: 非線形での近似イメージ このときに用いる関数形として一般的なものが、標準正規分布の累積分布関数である。累積分布関数は、確率密度関数を積分していくことで得ることができ、0 を下限、1 を上限とする分布である。 標準正規分布の累積分布関数を\\(\\Phi\\)（ファイ）で示すと、反応確率は以下のように表すことができる30。 \\[ P(y_i=1|x_{1i},...,x_{ki})=\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})=\\int^{\\beta_0+\\sum^k_{j =1}\\beta_jx_{ji}}_{-\\infty}\\phi(z)~dz \\] 一方で、\\(P(y_i=0|x_{1i},...,x_{ki})\\)については、以下が成り立つ。 \\[ P(y_i=0|x_{1i},...,x_{ki})=1-P(y_i=1|x_{1i},...,x_{ki})=1-\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\] このように設定した選択の有無それぞれに対する分布関数をベルヌーイ確率関数31に代入すると、\\(y_i=1\\) の場合と \\(y_i=0\\) の場合の両方を捉えた、以下のような確率密度関数を考えることができる。 \\[\\begin{equation} P(y_i|x_{1i},...,x_{ki})=\\\\ \\Big(\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\Big)^{y_i}\\times \\Big(1-\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\Big)^{1-y_i} \\tag{9.1} \\end{equation}\\] このように、選択行動に対して標準正規分布の累積分布関数を仮定して定式化するモデルを「プロビットモデル」と呼ぶ。また、選択モデルでは標準正規分布ではなく、ロジスティック分布の累積分布関数を用いた定式化を行う「ロジットモデル」を用いることも多い。ロジットモデルでは、ロジスティック分布の累積分布関数（\\(\\Lambda(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})\\)）を用いて、以下のような密度関数を用いる。 \\[ P(y_i|x_{1i},...,x_{ki}) =\\Lambda(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})=\\frac{\\exp(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})}{1+\\exp(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})} \\] ロジットモデルでは、回帰モデル（\\(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}\\)）の値が大きくなると反応確率が 1 に近づき、小さくなると 0 に近づくという性質を持っており、取りうる区間も 0 から 1 の間に限定されている。ロジットモデルは分析における数値計算がプロビットモデルよりも容易であるという特徴を持っており、これまで広く使われてきたという経緯がある。コンピュータの性能が高まった近年ではプロビットモデルの使用が増えてきたものの、過去の研究との比較やこれまでの研究蓄積、慣習と言った側面を重視し、引き続きロジットモデルが使われることも多い。 9.4.3 潜在変数アプローチによる説明 これまでは、「選択」という点に着目し、ベルヌーイ確率関数を用いてプロビットモデルを導出する方法を紹介した。一方で、選択という離散的な変数の背後に、観察できない連続的な変数（潜在変数: latent variable）が存在するという視点からモデル化を行うことも可能である。このような考え方を潜在変数アプローチと呼ぶ。マーケティング領域の論文やテキストでは、この潜在変数アプローチに基づく選択モデルの記述や紹介も行われるため、本書では、先述のプロビットモデルと潜在変数アプローチにより導出されたモデルが一致することを示す。 ここで、\\(y_i^*\\) という（観察可能な説明変数とは異なる）連続的な潜在変数を考え、以下のような回帰モデルを考える。 \\[ y_i^*=\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}+e_i \\] ただし、この誤差項 \\(e_i\\) は標準正規分布に従い、\\(e_i|x_{1i},...,x_{ki}~\\sim N(0,1)\\)を満たすと考える。 この時、\\(y_i^*\\) がある閾値を越えたならば（観察可能な）被説明変数は 1 をとり、越えない場合は被説明変数が 0 をとる、というような潜在変数と被説明変数との関係を考える。プロビットモデルでは具体的に、以下のような関係を仮定する。 \\[ \\begin{aligned} y_i= \\left\\{ \\begin{array}{ll} 0 &amp; ~\\text{if}~~y_i^*\\leq0 \\\\ 1 &amp; ~\\text{if}~~y_i^*&gt;0 \\end{array} \\right. \\end{aligned} \\] そのため、\\(P(y_i=1|x_{1i},...,x_{ki})=P(y_i^*&gt;0|x_{1i},...,x_{ki})\\)や、\\(P(y_i=0|x_{1i},...,x_{ki})=P(y_i^*\\leq0|x_{1i},...,x_{ki})\\) と示せることがわかる。この性質を利用して、選択をしない条件付き確率 \\(P(y_i=0|x_{1i},...,x_{ki})\\) は以下のように標準正規分布の累積分布関数を用いて表現できる（西山ほか, 2019, p.339）。 \\[ \\begin{align} P(y_i=0|x_{1i},...,x_{ki})&amp;=P(y_i^*\\leq0|x_{1i},...,x_{ki})\\\\ &amp;= P(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}+e_i\\leq0|x_{1i},...,x_{ki})\\\\ &amp;=P(e_i\\leq -\\beta_0-\\beta_1x_{1i}-...-\\beta_kx_{ki}|x_{1i},...,x_{ki})\\\\ &amp;=\\Phi(-\\beta_0-\\beta_1x_{1i}-...-\\beta_kx_{ki}) \\end{align} \\] 標準正規分布はその性質より、0を中心として左右対称な確率密度関数を持つ。そのため、\\(e_i\\) が \\((-\\beta_0-\\beta_1x_{1i}-...-\\beta_kx_{ki})\\) 以下の値を取る確率と、\\((\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})\\) 以上の値を取る確率は等しくなるため、以下を得る。 \\[ P(y_i=0|x_{1i},...,x_{ki})=1-\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\] また、\\(P(y_i=1|x_{1i},...,x_{ki})=1-P(y_i=0|x_{1i},...,x_{ki})\\) であるため、反応確率は以下のように示すことができる。 \\[ P(y_i=1|x_{1i},...,x_{ki})=\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki}) \\] したがって、潜在変数アプローチでも通常のプロビットモデルと一致するモデルを得ることができた。 ただし、\\(\\Phi&#39;(z)=\\phi(z)=\\frac{1}{\\sqrt{2\\pi}}\\exp(\\frac{-z^2}{2})\\)とする。↩︎ \\(f(y_i)=P_i^{y_i}\\times\\big(1-P_i)^{1-y_i}\\)↩︎ "],["プロビットモデルの推定と解釈.html", "9.5 プロビットモデルの推定と解釈", " 9.5 プロビットモデルの推定と解釈 9.5.1 分析の実行と結果のまとめ プロビットモデルを構築できたら、次はモデル内のパラメータに関する推定値（\\(\\hat{\\beta}_0, \\hat{\\beta}_1,...,\\hat{\\beta}_k\\)）を求め、選択確率に関する含意を得る。プロビットモデルのパラメータ推定においては、最小二乗法ではなく、最尤（Maximum Likelihood: ML）法を用いるが、詳細については補足（9.6）を参照してほしい。 プロビットモデルを推定する場合のデータセットには、説明変数に関する列（\\(x_{1i},.., x_{ki}\\)）と、それに対応する個人 \\(i\\) の選択結果 \\(y_i\\)（選択していれば1、選択していなければ 0）が記録されている。choice_data.xlsx は、二つの製品に対する消費者の選択結果を捉えた人工データデットである。データには製品1の価格 \\(p1\\)、製品2の価格 \\(p2\\)（千円）、製品1のクーポン広告を受け取ったかのダミー変数 \\(a1\\)、製品2のクーポン広告ダミー変数 \\(a2\\) と、製品1の選択結果 \\(y1\\)、製品2の選択結果 \\(y2\\) が含まれている。 choice_df &lt;- readxl::read_xlsx(&quot;data/choice_data.xlsx&quot;) head(choice_df) ## # A tibble: 6 × 6 ## y1 y2 p1 p2 a1 a2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 10.7 16.8 0 1 ## 2 0 1 11.6 9.15 1 0 ## 3 1 0 8.97 9.31 1 0 ## 4 1 0 4.62 8.03 0 0 ## 5 1 0 7.81 19.2 1 1 ## 6 1 0 7.49 8.84 0 0 このchoice_data.xlsxを用いて製品1の選択に関するプロビットモデルを推定してみる。Rではglm()関数において、family = binomial(link = probit) という引数を指定することで実行が可能である32。以下では分析の実行と結果の出力を行う。 probit1 &lt;- glm(y1 ~ p1 + a1, family = binomial(link = probit), data = choice_df) summary(probit1) ## ## Call: ## glm(formula = y1 ~ p1 + a1, family = binomial(link = probit), ## data = choice_df) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.04117 0.22350 9.133 &lt; 2e-16 *** ## p1 -0.24720 0.02213 -11.171 &lt; 2e-16 *** ## a1 0.62763 0.08656 7.251 4.13e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1384.5 on 999 degrees of freedom ## Residual deviance: 1205.5 on 997 degrees of freedom ## AIC: 1211.5 ## ## Number of Fisher Scoring iterations: 3 ここまで、我々は線形確率、プロビット、ロジットという3種類のモデルでの推定方法を概観してきた。以下では、これらの異なる推定方法を用いた分析結果を併記する。 library(modelsummary) ols1 &lt;- lm(y1 ~ p1 + a1, data = choice_df) logit1 &lt;- glm(y1 ~ p1 + a1, family = binomial(link = logit), data = choice_df) models &lt;- list() models[[&quot;Linear probability model&quot;]] &lt;- ols1 models[[&quot;Probit model&quot;]] &lt;- probit1 models[[&quot;Logit model&quot;]] &lt;- logit1 modelsummary(models, title = &quot;モデル比較&quot;, notes = &quot;Values in [ ] show robust standard errors&quot;, stars = TRUE, statistic = &quot;std.error&quot;, vcov = &quot;robust&quot;, gof_map = &quot;nobs&quot;) tinytable_gaqpr7btwykogksg6ap5 .table td.tinytable_css_r5yomvtgc0ccv98s0wda, .table th.tinytable_css_r5yomvtgc0ccv98s0wda { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_pti03czc77zcycb1mah4, .table th.tinytable_css_pti03czc77zcycb1mah4 { text-align: left; } .table td.tinytable_css_wpf0v3cvhdrl8ldilwsq, .table th.tinytable_css_wpf0v3cvhdrl8ldilwsq { text-align: center; } .table td.tinytable_css_qlg425fim1mmyp7i5d16, .table th.tinytable_css_qlg425fim1mmyp7i5d16 { text-align: center; } .table td.tinytable_css_trdkg3h5dz2rytbypx92, .table th.tinytable_css_trdkg3h5dz2rytbypx92 { text-align: center; } .table td.tinytable_css_3j91463cphd5lic5uzll, .table th.tinytable_css_3j91463cphd5lic5uzll { border-bottom: solid 0.05em black; } モデル比較 Linear probability model Probit model Logit model + p Values in [ ] show robust standard errors (Intercept) 1.196*** 2.041*** 3.332*** (0.065) (0.212) (0.361) p1 -0.085*** -0.247*** -0.404*** (0.006) (0.021) (0.036) a1 0.221*** 0.628*** 1.035*** (0.029) (0.087) (0.145) Num.Obs. 1000 1000 1000 分析の結果、どのアプローチでも p1 は被説明変数に対して負に、a1 は正に有意な影響を与えることが示されている。しかしながら、詳しくは次節にて説明するが、係数の解釈については注意が必要である。このように、複数の分析アプローチを用いた結果をまとめ、併記することも簡単である。結果の頑健性チェックや、様々な比較検討のために、このような表が用いられることも多い。 9.5.2 限界効果の計算 プロビットモデルの分析には成功したが、非線形モデルで推定された係数の解釈には注意が必要である。特に、プロビットモデルによって推定されたある変数の係数値は、「他の変数の影響をコントロールした場合にその変数が選択確率に与える影響」を意味しない。そのため、OLSの分析結果のように、係数の推定値のみを見て変数の影響の程度を議論することはできず、そのような解釈を行うためには特定の変数が持つ「限界効果」を追加的に分析する必要がある。 プロビットモデルによって得た係数の推定値を直接的に解釈できない点と限界効果について説明する。パラメータの推定結果と、\\(x_{1i},...,x_{ki}\\) の値を得たときに \\(y_i\\) が 1 を取る確率は以下の様に示される。 \\[ \\hat{P}(y_i=1|x_{1i},...,x_{ki})=\\Phi(\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+...+\\hat{\\beta}_kx_{ki}) \\] ここで、説明変数 \\(x_{1i}\\) の限界効果は、\\(x_{1i}\\) が変化したときに反応確率がどのように変化するのかを表す。これは偏微分という計算方法を用いて以下の様に示すことができる。なお、以下で示す内容は \\(x_{1i}\\) 以外の任意の説明変数を用いても成り立つ。 \\[ \\frac{\\partial \\hat{P}(y_i=1|x_{1i},...,x_{ki})}{\\partial x_{1i}}=\\phi(\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+...+\\hat{\\beta}_kx_{ki})\\hat{\\beta}_1 \\] ただし、\\(\\phi\\) は標準正規分布の確率密度関数であり、累積分布関数を微分することで得る。このように、OLSのような線形モデルと異なり、\\(\\hat{\\beta}_1\\) が直接的に限界効果を示しているわけではないことが伺える33。 また同式より、ある変数の変化が反応確率へ与える限界効果は、個人が持つ全ての説明変数（\\(x_{1i},...,x_{ki}\\)）の値によって変化することもわかる。そのため、限界効果の報告においては「平均的な」限界効果を報告することが一般的である。ただし、「何の平均を取るか」という点において、2種類の計算方法が存在する。第1に、個人の個別限界効果を計算し、その平均値求めるという以下のような方法である。 \\[\\begin{equation} \\frac{1}{n}\\sum^n_{i=1}\\Big[\\phi(\\hat{\\beta}_0+\\hat{\\beta}_1x_{1i}+...+\\hat{\\beta}_kx_{ki})\\hat{\\beta}_1 \\Big] \\tag{9.2} \\end{equation}\\] この計算方法は、因果推論における平均処置効果の議論に対応しているという好ましい性質を持っている（西山ほか, 2019）。 第2の方法は、説明変数の値について、それぞれ平均値を計算し、限界効果を求めるという以下のような方法である。 \\[\\begin{equation} \\phi(\\hat{\\beta_0}+\\hat{\\beta}_1\\bar{x}_1+...+\\hat{\\beta}_k\\bar{x}_k)\\hat{\\beta}_1 \\tag{9.3} \\end{equation}\\] これは、それぞれの説明変数について平均値を取るような「平均的個人」における限界効果として解釈可能な計算方法である。 着目する説明変数が上のデータ分析結果における a1 のように離散変数（ダミー変数）の場合、その限界効果についてこれまでのような偏微分の議論は使えない。そのため、ダミー変数が 0 の場合の反応確率と、1 の場合の反応確率の差を取る形で限界効果を捉える。例えば、クーポンを受け取っていない場合（\\(a_1=0\\)）と受け取った場合（\\(a_1=1\\)）の反応確率の差は以下の様に示すことができる。 \\[\\begin{equation} \\Phi(\\hat{\\beta}_0+\\hat{\\beta}_1p_{1i}+\\hat{\\beta}_2) - \\Phi(\\hat{\\beta}_0+\\hat{\\beta}_1p_{1i}) \\tag{9.4} \\end{equation}\\] この限界効果も、他の変数の値（例えば、\\(p_{1i}\\)）によって変化するため、連続変数の場合と同様に平均値を報告することが一般的である。 限界効果の計算には、mfx::probitmfx() を用いる34。必要に応じて、以下のようにパッケージをインストールしてほしい。 install.packages(&quot;mfx&quot;) mfx パッケージを用いた限界効果の計算では、線形モデルやプロビットモデルの推定同様、以下のようにモデルを指定し、分析を行う。平均的な限界効果の計算方法については、atmean という引数で指定する。atmean = FALSE とすることで、個別限界効果の平均値を計算できる。一方で、平均値における限界効果はatmean = TRUE と指示することで計算できる。 library(mfx) #Average Marginal Effects (限界効果の平均) probit1_ame &lt;- probitmfx(y1 ~ p1 + a1, data = choice_df, atmean = FALSE) probit1_ame ## Call: ## probitmfx(formula = y1 ~ p1 + a1, data = choice_df, atmean = FALSE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## p1 -0.0847786 0.0060787 -13.9469 &lt; 2.2e-16 *** ## a1 0.2188866 0.0289816 7.5526 4.266e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;a1&quot; #Marginal Effects at Mean（平均値における限界効果） probit1_mem &lt;- probitmfx(y1 ~ p1 + a1, data = choice_df, atmean = TRUE) probit1_mem ## Call: ## probitmfx(formula = y1 ~ p1 + a1, data = choice_df, atmean = TRUE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## p1 -0.0984235 0.0088074 -11.1750 &lt; 2.2e-16 *** ## a1 0.2446609 0.0324366 7.5427 4.602e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;a1&quot; 分析結果を比較すると、多少計算結果は異なるものの、どちらの計算方法においても、価格は製品の選択確率に負に、クーポンは正に有意な影響を与えることが伺える。また影響の程度として、限界効果の平均値に着目すると、価格が1単位（千円）上昇すると、選択確率が約\\(8.5\\%\\)下がること、クーポンを受け取った人は受け取っていない人と比べて約\\(21.9\\%\\)選択確率が高いことが伺える。また、今回の結果の場合、OLSを用いたLPMの係数パラメータと似た値を得たことも伺える。なお、詳細については省略するが限界効果の標準誤差はデルタ法と呼ばれる方法で計算されている（Fernihough, 2019）。 また、mfx による結果も以下のように整理して出力する事ができる。下表の(1) は限界効果の平均（Average Mean Effect）、(2) は平均的個人における限界効果（Marginal Effect at Mean）をそれぞれあらわしている。 marginal &lt;- c( &quot;p1 marginal&quot; = &quot;p1&quot;, &quot;a1 marginal&quot; = &quot;a1&quot; ) marg_model &lt;- list( &quot;(1) Probit_AME&quot; &lt;- probit1_ame, &quot;(2) Probit_MEM&quot; &lt;- probit1_mem ) marg_sum &lt;- modelsummary( # models to summarize side-by-side marg_model, # S.E. in parentheses statistic = &quot;std.error&quot;, # rename and select the coefficients coef_map = marginal, # significance stars stars = TRUE, # term and component columns are combined shape = term:component ~ model, add_rows = data.frame(&quot;Marginal effect type&quot;, &quot;Average Marginal Effect&quot;, &quot;Marginal Effect at Mean&quot;), title = &quot;限界効果サマリー&quot;, # omit all goodness-of-fit statisitcs except # of observations gof_map = &quot;nobs&quot;) marg_sum tinytable_r0gufhkdq7d7p884uuxq .table td.tinytable_css_so81cqkbrb6spomeju7w, .table th.tinytable_css_so81cqkbrb6spomeju7w { border-bottom: solid 0.1em #d3d8dc; } .table td.tinytable_css_mxx3t938mhhvzv7yt2dj, .table th.tinytable_css_mxx3t938mhhvzv7yt2dj { text-align: left; } .table td.tinytable_css_rhtvur8ch39ypr3r2myu, .table th.tinytable_css_rhtvur8ch39ypr3r2myu { text-align: center; } .table td.tinytable_css_qca7764q24z69x9zhff6, .table th.tinytable_css_qca7764q24z69x9zhff6 { text-align: center; } .table td.tinytable_css_fasm0rkcpgkd85n4on0j, .table th.tinytable_css_fasm0rkcpgkd85n4on0j { border-bottom: solid 0.05em black; } 限界効果サマリー (1) (2) + p p1 -0.085*** -0.098*** (0.006) (0.009) a1 0.219*** 0.245*** (0.029) (0.032) Num.Obs. 1000 1000 Marginal effect type Average Marginal Effect Marginal Effect at Mean 最後に、プロビットモデルのモデル評価指標について紹介する。プロビットモデルを最尤法で推定した場合、OLSにおける決定係数 \\(R^2\\) を用いてモデルの当てはまりの良さを用いることはできない。そこで、通常疑似決定係数（Pseudo \\(R^2\\)）を用いる事が多い35。疑似決定係数は0から1の間の値を取り、自身の立てたモデルの当てはまりが良いほど1に近づくという特徴を持っている。Rにおいては、DescTools::PseudoR2() を用いて計算可能である。install.packages(\"DescTools\") でインストールしてほしい。 DescTools::PseudoR2(probit1) ## McFadden ## 0.1293345 ロジットモデルの場合は、family = binomial(link = logit) で計算可能である。↩︎ \\(\\hat{y}_i=\\hat{\\beta}_0+\\hat{\\beta_1}x_{1i}+...+\\hat{\\beta}_kx_{ki}\\) という線形モデルの限界効果は、以下のようになる。 \\[\\frac{\\partial \\hat{y}_i}{\\partial x_{1i}}=\\hat{\\beta}_1 \\]↩︎ ロジットモデルの場合は、mfx::logitmfx() で計算可能である。↩︎ この指標は、最尤推定で用いる対数尤度関数 (9.5) に実際の観測値と最尤推定値を代入して求まる対数尤度の和（\\(L_{full}\\)）と、定数項だけを含むプロビットモデルを推定したときの対数尤度の和（\\(L_0\\)）を用いて、\\(\\Big(1-(L_{full}/L_0) \\Big)\\) と定義される↩︎ "],["ml.html", "9.6 補足：最尤推定法の紹介", " 9.6 補足：最尤推定法の紹介 最尤法では、あるデータが与えられたときに、そのような情報が得られる確率が最も高くなる（最も尤もらしくなる）ようにパラメータの値を求める方法である。この尤もらしさを尤度と呼び、得られたデータを尤度が最大になる（最もうまく説明できる）ようにパラメータを推定する。 一般的に、パラメータ \\(\\theta\\) を含む確率密度 \\(f(x,\\theta)\\) からの無作為標本\\({x_1,..., x_n}\\) を得た時、これに対する以下のような同時確率密度関数をパラメータ \\(\\theta\\) に関する尤度関数と見做す。 \\[ L(\\theta)=f(x_1,\\theta)\\times f(x_2,\\theta)\\times...\\times f(x_n,\\theta) \\] そして、これを最大にするように \\(\\theta\\) の推定値を求める方法が最尤法であり、ここで得る推定量を最尤推定量（Maximum Likelihood Estimator: MLE）と呼ぶ。 実際の分析においては、尤度関数の自然対数を取った以下のような対数尤度関数を用いる事が多い。 \\[ LL(\\theta)=\\sum^n_{i=1}\\ln f(x_i,\\theta) \\] 対数尤度と尤度を最大にする \\(\\theta\\) は数学的には等しく、対数尤度を用いたほうが、計算が容易であることから、対数尤度が用いられる。 前節で確認したプロビットモデル（(9.1)）の対数尤度関数は、以下のように求まる。 \\[\\begin{equation} LL_i(\\theta)=y_i\\Big(\\ln[\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})] \\Big)+ (1-y_i)\\Big(\\ln[1-\\Phi(\\beta_0+\\beta_1x_{1i}+...+\\beta_kx_{ki})]\\Big) \\tag{9.5} \\end{equation}\\] 式 (9.5) は個人 \\(i\\) に関する対数尤度であるため、データ全体をうまく説明するパラメータを推定するためには、個別対数尤度の和（\\(\\sum^n_{i=1}LL_i\\)）を最大化するような係数パラメータ（\\(\\beta_0,\\beta_1,..., \\beta_k\\)）の推定値（\\(\\hat{\\beta}_0,\\hat{\\beta}_1,..., \\hat{\\beta}_k\\)）を計算する。 "],["離散選択モデルへの合理的消費者像の応用.html", "9.7 離散選択モデルへの合理的消費者像の応用", " 9.7 離散選択モデルへの合理的消費者像の応用 離散選択モデルを用いた消費者需要の推定に関する先駆的な研究としてMcFadden (1974) による交通網に関する研究が挙げられる。この研究から続く離散選択モデルを用いた消費者需要の分析アプローチでは、（実際に消費者が何を考えているかに関わらず）消費者が自身の効用が最大になる選択肢を選んでいると考える。具体的には、「AとBという任意の2つの選択肢があり、ある個人がAを選んだ場合、その個人のAに対する効用はBに対する効用よりも高い」という、顕示選好に関する仮定を置き分析を行っている。そして、この仮定の根拠として妥当性を担保しているのが、10.1節と10.2節で紹介した理論的背景である。 ここで、選択肢 \\(j\\) に対する効用は以下のような関数で捉えるとする36。 \\[ U_{ij}=\\beta_{0}+\\beta_{1}x_{ij}+e_{ij} \\] ただし、\\(x_{ji}\\) は選択肢 \\(j\\)、個人 \\(i\\) における説明変数、\\(e_{ji}\\) は効用のランダム項を表している。 顕示選好の仮定と観察可能な選択結果によってデータに含まれる個人の選好・効用について類推することが可能になる。ここで、ある個人 \\(i\\) が選択肢 A と B の中から Aを選んだ、という状況を考える。選択肢 A の効用を \\(U_{iA}\\)、B の効用を \\(U_{iB}\\) とすると以下のような整理が可能になる。 \\[ U_{iA}&gt;U_{iB}\\\\ \\beta_0+\\beta_1x_{iA}+e_{iA}&gt;\\beta_0+\\beta_1x_{iB}+e_{iB}\\\\ e_{iB}-e_{iA}&lt;(\\beta_0+\\beta_1x_{iA})-(\\beta_0+\\beta_1x_{iB})\\\\ e_{iB}-e_{iA}&lt;\\beta_1(x_{iA}-x_{iB}) \\] ここで、効用のランダム項の差（\\(e_{iB}-e_{iA}=e_i\\)）が標準正規分布に従うことを仮定すると、以下のようにプロビットモデルを用いることができる (Adams, 2021)。 \\[\\begin{equation} P(y_i=A|x_{iA},x_{iB})=P(e_{iB}-e_{iA}&lt;\\beta_1(x_{iA}-x_{iB}))\\\\ = P(e_i&lt;\\beta_1(x_{iA}-x_{iB}))\\\\ =\\Phi(\\beta_1(x_{iA}-x_{iB})) \\tag{9.6} \\end{equation}\\] さらに、このモデルは選択肢のいずれかを選ぶ合理的行動を捉えているため、何も選ばない個人がいないとすれば、製品Bの選択確率は以下のように表現される。 \\[ P(y_i=B|x_{iA},x_{iB})= P(U_{iA}&lt;U_{iB})=1-\\Phi(\\beta_1(x_{iA}-x_{iB})) \\] なお、上記のモデルでは、定数項のパラメータが消される形で定式化されていた。このような効用モデルの定数項を、マーケティング戦略変数を除いたうえでのその選択肢の価値として、製品の「ベースライン価値」や「ブランド価値」として捉えることがある（照井・佐藤, 2022）。選択肢ごとに異なるブランド価値があると想定するモデルを構築する場合、選択肢ごとに固有の定数項（\\(\\beta_{A0}\\)と\\(\\beta_{B0}\\)）を持つモデルに拡張することが好ましい。拡張モデルでは、以下のような効用関数を想定する。 \\[ U_{ij}=\\beta_{0j}+\\beta_{1}x_{ij}+e_{ij} \\] これを、式 (9.6)と同様の定式化を行うことで、以下を得る。 \\[ \\begin{align} P(y_i=A|x_{iA},x_{iB})&amp;=P(e_{iB}-e_{iA}&lt;(\\beta_{0A}-\\beta_{0B})+\\beta_1(x_{iA}-x_{iB}))\\\\ &amp;= P(e_i&lt;\\tilde{\\beta}_{0}+\\beta_1(x_{iA}-x_{iB}))\\\\ &amp;=\\Phi(\\tilde{\\beta}_{0}+\\beta_1(x_{iA}-x_{iB})) \\end{align} \\] ただし、\\(\\tilde{\\beta}_0\\)は、\\(\\beta_{0A}-\\beta_{0B}\\) であり、選択肢間の定数項の差を表している。そのため、このような定式化によって分析されるモデルの定数項は、「選択肢固有の定数項の差」として推定される。以下では、前節でも用いた choice_df を用いて上記の消費者の離散選択モデルを推定する。具体的には、製品1と2の価格差（p_ratio=p1-p2）と製品1のクーポン広告受取り有無（a1）と製品2のクーポン広告（a2）を用いて以下のようなモデルを分析する。 \\[ U_{1i}=\\tilde{\\beta}_{0}+\\beta_1(p_1-p_2)+\\beta_2a_1-\\beta_3a_2+e_{1i} \\] ただし、R上のコードでは、\\(a_2\\) の係数について、負に推定されることを想定しつつ、+ 記号を使って定式化する。 #価格差変数作成 choice_df &lt;- choice_df %&gt;% mutate(p_ratio = p1 -p2) probit2 &lt;- glm(y1 ~ p_ratio + a1 + a2, family = binomial(link = probit), data = choice_df) summary(probit2) ## ## Call: ## glm(formula = y1 ~ p_ratio + a1 + a2, family = binomial(link = probit), ## data = choice_df) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.54634 0.07756 -7.044 1.87e-12 *** ## p_ratio -0.22180 0.01524 -14.559 &lt; 2e-16 *** ## a1 0.66314 0.09162 7.238 4.55e-13 *** ## a2 -0.32291 0.09884 -3.267 0.00109 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1384.5 on 999 degrees of freedom ## Residual deviance: 1066.9 on 996 degrees of freedom ## AIC: 1074.9 ## ## Number of Fisher Scoring iterations: 4 分析の結果、価格差については負に有意であり、p1 が相対的に高くほど選択確率が下がる傾向にあることを示唆している。また、自社のクーポン（a1）は正に、他社のクーポン（a2）は負に有意であることも伺えた。加えて、定数項（(Intercept)）は負に有意であるため、製品2を基準（製品2の定数項を 0 ）とした場合と比べ、製品 1 の定数項（ブランド価値）は低いと解釈する事ができる。本モデルの解釈を行うために、限界効果と擬似決定係数を以下のように分析する。 DescTools::PseudoR2(probit2) ## McFadden ## 0.229384 probit2_ame &lt;- probitmfx(y1 ~ p_ratio + a1 + a2, data = choice_df, atmean = FALSE) probit2_ame ## Call: ## probitmfx(formula = y1 ~ p_ratio + a1 + a2, data = choice_df, ## atmean = FALSE) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## p_ratio -0.0669199 0.0029405 -22.7581 &lt; 2.2e-16 *** ## a1 0.2036848 0.0270475 7.5306 5.049e-14 *** ## a2 -0.0969891 0.0291848 -3.3233 0.0008897 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;a1&quot; &quot;a2&quot; 分析の結果、価格差が1単位（千円）大きくなると約 \\(6.7\\%\\) 製品1の選択確率が下がることが伺える。一方で、製品 1 のクーポンを受け取っている消費者はそうでない消費者に比べて約 \\(20\\%\\) 購買確率が高く、反対に他社クーポンは約 \\(9.7\\%\\) の購買確率低下につながる。 ただし、行列の記法を使えば、同じ議論を \\(x_{1ij},...,x_{kij}\\)の説明変数に応用できる。↩︎ "],["多項選択モデル紹介.html", "9.8 多項選択モデル紹介", " 9.8 多項選択モデル紹介 本章のこれまでの内容では、2 つの選択肢から 1 つを選ぶ 2 項選択モデルに着目していた。しかし、複数の選択肢から 1 つを選ぶという多項選択モデルにもこれまでの議論を拡張することができる。任意の \\(m\\) 個の選択肢の中からある選択肢 \\(j\\) を選ぶ場合、選択肢 \\(j\\) の効用は、\\(j\\) を含む 1 から \\(m\\) までの選択肢の中で最も大きいといえる。 詳細は省略するが、(9.1) でベルヌーイ試行に着目したいたものをカテゴリカル分布に拡張し、選択確率についての確率密度関数を考えることで、多項選択モデルを得る。なお、計算の複雑性から、多項選択の分析では、多項ロジットモデルが用いられることが多い。多項離散選択モデルの実行においては、解釈やデータ準備においていくつか注意が必要である。 多項選択モデルの推定では、mlogit パッケージを用いる。そのため、install.packages(\"mlogit\") によってパッケージをインストールしてほしい。分析においては、5 章でも紹介した、wide型とLong型のデータ構造に注意が必要である。ここで、 mlogit パッケージに含まれる Cracker データを用いて多項ロジットモデルを実行する。このデータは3293件のクラッカーの選択について扱ったデータである。このデータには以下の変数が含まれている。 id: 個人を特定する番号 choice: sunshine, keebler, nabisco, private,のうちどれを選んだか disp.z: ブランドz (各ブランド) が特別な陳列をされていたか feat.z: ブランドz (各ブランド) が新聞広告を掲載していたか price.z: ブランドz (各ブランド) の価格 library(mlogit) data(Cracker, package = &quot;mlogit&quot;) head(Cracker, n=20) ## id disp.sunshine disp.keebler disp.nabisco disp.private feat.sunshine ## 1 1 0 0 0 0 0 ## 2 1 0 0 0 0 0 ## 3 1 1 0 0 0 0 ## 4 1 0 0 0 0 0 ## 5 1 0 0 0 0 0 ## 6 1 0 0 0 0 0 ## 7 1 0 0 1 0 0 ## 8 1 0 0 1 0 0 ## 9 1 0 0 1 0 0 ## 10 1 1 0 1 0 0 ## 11 1 0 0 1 0 0 ## 12 1 0 0 0 0 0 ## 13 1 1 0 0 0 0 ## 14 1 0 1 1 0 0 ## 15 1 0 0 0 0 0 ## 16 1 0 0 1 0 0 ## 17 2 0 0 0 0 0 ## 18 2 1 0 1 0 1 ## 19 2 1 0 0 0 1 ## 20 2 1 0 0 0 0 ## feat.keebler feat.nabisco feat.private price.sunshine price.keebler ## 1 0 0 0 98 88 ## 2 0 0 0 99 109 ## 3 0 0 0 49 109 ## 4 0 0 0 103 109 ## 5 0 0 0 109 109 ## 6 0 0 0 89 109 ## 7 0 0 0 109 109 ## 8 0 0 0 109 119 ## 9 0 0 0 109 121 ## 10 0 0 0 79 121 ## 11 0 0 0 109 113 ## 12 0 0 0 109 121 ## 13 0 0 0 89 121 ## 14 0 0 0 109 109 ## 15 0 0 0 109 109 ## 16 0 0 0 129 104 ## 17 0 0 0 79 99 ## 18 0 0 0 69 105 ## 19 0 0 0 79 125 ## 20 0 0 0 79 125 ## price.nabisco price.private choice ## 1 120 71 nabisco ## 2 99 71 nabisco ## 3 109 78 sunshine ## 4 89 78 nabisco ## 5 119 64 nabisco ## 6 119 84 nabisco ## 7 129 78 sunshine ## 8 129 78 nabisco ## 9 109 78 nabisco ## 10 109 78 nabisco ## 11 109 96 nabisco ## 12 99 86 nabisco ## 13 99 86 nabisco ## 14 129 96 nabisco ## 15 129 79 nabisco ## 16 129 96 nabisco ## 17 69 69 nabisco ## 18 89 65 sunshine ## 19 106 69 sunshine ## 20 106 69 sunshine このデータ構造を、long型に変更する37。 cracker &lt;- mlogit.data(Cracker, choice = &quot;choice&quot;, shape = &quot;wide&quot;, varying=c(2:13)) head(cracker, n=20) ## ~~~~~~~ ## first 20 observations out of 13168 ## ~~~~~~~ ## id choice alt disp feat price chid idx ## 1 1 FALSE keebler 0 0 88 1 1:bler ## 2 1 TRUE nabisco 0 0 120 1 1:isco ## 3 1 FALSE private 0 0 71 1 1:vate ## 4 1 FALSE sunshine 0 0 98 1 1:hine ## 5 1 FALSE keebler 0 0 109 2 2:bler ## 6 1 TRUE nabisco 0 0 99 2 2:isco ## 7 1 FALSE private 0 0 71 2 2:vate ## 8 1 FALSE sunshine 0 0 99 2 2:hine ## 9 1 FALSE keebler 0 0 109 3 3:bler ## 10 1 FALSE nabisco 0 0 109 3 3:isco ## 11 1 FALSE private 0 0 78 3 3:vate ## 12 1 TRUE sunshine 1 0 49 3 3:hine ## 13 1 FALSE keebler 0 0 109 4 4:bler ## 14 1 TRUE nabisco 0 0 89 4 4:isco ## 15 1 FALSE private 0 0 78 4 4:vate ## 16 1 FALSE sunshine 0 0 103 4 4:hine ## 17 1 FALSE keebler 0 0 109 5 5:bler ## 18 1 TRUE nabisco 0 0 119 5 5:isco ## 19 1 FALSE private 0 0 64 5 5:vate ## 20 1 FALSE sunshine 0 0 109 5 5:hine ## ## ~~~ indexes ~~~~ ## chid alt ## 1 1 keebler ## 2 1 nabisco ## 3 1 private ## 4 1 sunshine ## 5 2 keebler ## 6 2 nabisco ## 7 2 private ## 8 2 sunshine ## 9 3 keebler ## 10 3 nabisco ## 11 3 private ## 12 3 sunshine ## 13 4 keebler ## 14 4 nabisco ## 15 4 private ## 16 4 sunshine ## 17 5 keebler ## 18 5 nabisco ## 19 5 private ## 20 5 sunshine ## indexes: 1, 2 変換後のデータセットでは、各選択肢の特徴と、それに対する個人の選択結果（TRUE or FALSE）を含める形で行が構成されている。このようにデータの構造を修正することで、分析を行う。 分析についても mlogitパッケージを利用して実行するのだが、分析結果の解釈を確かなものにするため、多項ロジットモデルについての説明を加える。多項ロジットモデルでは、\\(m\\) 個の選択肢の中からある選択肢 \\(j\\) を選ぶ行為を分析対象としている。分析モデルにおいては、説明変数 \\(x_{1i},...,x_{ki}\\) が与えられたときにある選択肢 \\(j\\) を選ぶ確率は 0 から 1 の間の値を取る必要がある。そのため、各選択肢に対する選択確率の合計が1になるように調整しなければならない。これに対して多項ロジットモデルでは、ある特定の選択肢（仮に選択肢 1 とする）を基準とし、その選択肢に対応する回帰係数パラメータ（\\(\\beta_{10},\\beta_{11},...,\\beta_{1k},\\)）をすべて 0 に固定する38。多項ロジットモデルは以下のように表すことができる。 \\[ \\begin{aligned} P(y_i=j|x_{1i},...,x_{ki})= \\left\\{ \\begin{array}{ll} \\frac{1}{1+\\sum^J_{j=2}[\\exp(\\beta_{j0}+\\beta_{j1}x_{1i}+...+\\beta_{jk}x_{ki})]} &amp; ~(y_i=1) \\\\ \\frac{\\exp(\\beta_{j0}+\\beta_{j1}x_{1i}+...+\\beta_{jk}x_{ki})}{1+\\sum^J_{j=2}[\\exp(\\beta_{j0}+\\beta_{j1}x_{1i}+...+\\beta_{jk}x_{ki})]} &amp; ~(y_i=2,..,m) \\end{array} \\right. \\end{aligned} \\] 多項ロジットの係数の解釈にも注意が必要である。多項ロジットモデルの係数（\\(\\beta\\)）は、対応する説明変数が微小に変化したときの、\\(y_i=j\\) と \\(y_i=1\\)（基準となる選択肢）との相対確率がどのように変化するかを示している。一方で限界効果は、説明変数が変化した時に \\(y_i=j\\) を選ぶ確率がどのように変化するかを示している。そのため、推定される係数そのものと限界効果との意味が大きく異なる点も、多項ロジットモデルの特徴である（西山ほか, 2019）。 また、多項ロジットモデルでは任意の 2 つの選択肢の相対的な選択確率は他の選択肢からは独立であるという、無関係な選択肢からの独立（independence from irrelevant alternatives: IIA）という仮定を置く。この仮定が満たされない場合には、データセット内の選択肢を統合（ひとまとめに）するなどして、IIAの仮定が適切だとみなせる状況を作るよう工夫が必要となる。 これらを踏まえ、多項ロジットモデルを実行する。分析においては mlogit::mlogit() を用いる。多項ロジットモデルの推定では、用いる変数に対する係数をどの細かさで分析するかを分析者が判断することが重要になる。例えば、選択肢ごとに異なる係数を推定すべきなのか、それとも各選択肢に共通で同一の係数が推定されるべきなのか、という点については分析者が指定することになる。Rコードでのモデルの定式化においては、以下の 3 つの変数カテゴリを区別する形で、変数と推定される係数の関係を記述する必要がある（ただし、\\(j\\) はある選択肢を示す）。 データ上選択肢固有の変数であり、複数の選択肢に共通の一般的な係数\\(\\beta\\)を得るための変数：\\(x_{ij}\\)。 データ上個人固有の変数であり、各選択肢ごとに異なる係数 \\(\\gamma_{j}\\) を得るための変数：\\(z_i\\)。 データ上選択肢固有の変数であり、各選択肢ごとに異なる係数 \\(\\delta_j\\)を得るための変数：\\(w_{ij}\\)。 これらの変数と係数のカテゴリを踏まえ、個人 \\(i\\)、選択肢 \\(j\\) に関する効用の確定項 \\(V_{ij}\\)は以下のように示すことができる。 \\[ V_{ij}=\\alpha_j+\\beta x_{ij}+\\gamma_j z_i+\\delta_j w_{ij} \\] そのため、どの変数については選択肢ごとの係数を推定すべきなのかについて、分析者側が、既存の理論やドメイン知識（実務・社会的知識）を用いて決定する必要がある。Rでのコード作成では、formula = choice ~ x|z|w という形でモデルを定義することができる。以下では、価格に対する消費者の反応はブランドごとに異なるものではないが、広告や陳列については各ブランドの具体的な戦術の特徴によって影響の程度が変わるかもしれないと考え、以下のように分析を行う（プロビットモデルを使うときには、probit=TRUEとする）。なお、このような選択肢に関する特徴を説明変数に含む多項ロジットモデルは、条件付きロジットモデル（Conditional logit model）と呼ばれることもある。 ml_cracker &lt;- mlogit(choice ~ price|1|disp + feat, probit = FALSE, data = cracker) summary(ml_cracker) ## ## Call: ## mlogit(formula = choice ~ price | 1 | disp + feat, data = cracker, ## probit = FALSE, method = &quot;nr&quot;) ## ## Frequencies of alternatives:choice ## keebler nabisco private sunshine ## 0.068651 0.544350 0.314399 0.072600 ## ## nr method ## 5 iterations, 0h:0m:0s ## g&#39;(-H)^-1g = 0.000258 ## successive function values within tolerance limits ## ## Coefficients : ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept):nabisco 2.0011226 0.0831336 24.0712 &lt; 2.2e-16 *** ## (Intercept):private 0.3193793 0.1238052 2.5797 0.0098888 ** ## (Intercept):sunshine -0.5435987 0.1139407 -4.7709 1.834e-06 *** ## price -0.0300966 0.0021082 -14.2761 &lt; 2.2e-16 *** ## disp:keebler 0.2999316 0.2070369 1.4487 0.1474251 ## disp:nabisco 0.1011111 0.0773633 1.3070 0.1912249 ## disp:private -0.2244555 0.1495236 -1.5011 0.1333199 ## disp:sunshine 0.4818670 0.1672400 2.8813 0.0039605 ** ## feat:keebler 0.6678542 0.2581732 2.5868 0.0096859 ** ## feat:nabisco 0.6047773 0.1404077 4.3073 1.653e-05 *** ## feat:private 0.1726981 0.2004446 0.8616 0.3889213 ## feat:sunshine 0.8304895 0.2340938 3.5477 0.0003886 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Log-Likelihood: -3333.8 ## McFadden R^2: 0.052798 ## Likelihood ratio test : chisq = 371.66 (p.value = &lt; 2.22e-16) この分析モデルでは、priceはブランドごとに係数が推定されない変数（\\(x_{ij}\\)）、disp と feat はブランドごとに係数が異なる変数（\\(w_{ij}\\)）として定義している。そして、ブランドごとに係数を分析するが、その特徴自体は同個人に共通（同id内では各ブランドに共通の値が与えられる）である変数（\\(z_i\\)）には、1 という値が記入されている。これは、他の変数から独立して、各ブランドが持つ特徴である「定数項」として分析するための指示である。これにより、各説明変数から独立して、消費者が各選択肢に対してどのような選択確率を（相対的に）有しているのかを表している。なお、ここで出力されている選択肢ごとの係数の推定値は基準となる選択肢（keebler）との相対的な選択確率の変化を表していることに、改めて注意が必要である。 なお、mlogit.data() 関数の引数の設定については、元々のデータ構造によって変わるので、[Cranにあるソフトウェア](https://cran.r-project.org/package=mlogit）の説明などを確認し、応用することを進める。↩︎ ただし。\\(\\exp(0)=1\\) 。↩︎ "],["需要の予測と本章のまとめ.html", "9.9 需要の予測と本章のまとめ", " 9.9 需要の予測と本章のまとめ ここまでは、離散選択モデルを用いて消費者の選択に関わるパラメータを推定する方法について紹介してきた。これを用いることで、モデルに基づく各選択肢の選択確率を計算することが可能になる。例えば以下では、先ほど推定した2項選択モデルである probit2 を用いて、推定結果から選択確率の予測値を計算する。 #パラメータの推定値(beta)の抽出 beta &lt;- probit2$coefficients #パラメータ betaと説明変数の線形結合を作成（beta_0は定数項なので1をかける） est_probit2 &lt;- cbind(1, choice_df$p_ratio, choice_df$a1, choice_df$a2) %*% beta #pnormによって標準正規分布の分布関数による計算を実行する。 #製品1(c1) を選ぶ確率の予測値 pred_probit_c1 &lt;- pnorm(est_probit2) mean(pred_probit_c1) ## [1] 0.4789384 #ロジットモデルの場合 logit2 &lt;- glm(y1 ~ p_ratio + a1 + a2, family = binomial(link = logit), data = choice_df) beta_logit &lt;- logit2$coefficients est_logit2 &lt;- cbind(1, choice_df$p_ratio, choice_df$a1, choice_df$a2) %*% beta_logit pred_logit_c1 &lt;- (exp(est_logit2))/(1+exp(est_logit2)) mean(pred_logit_c1) ## [1] 0.479 probit2 モデルにおける製品1の選択確率は約 \\(47.9\\%\\) であり、製品2とほぼ半分ずつ分け合っているが、わずかに製品2の選択確率のほうが高いことが伺える。ここで、製品1の企業が値下げを行ったら、選択確率はどの様に変化するのだろうか。このような架空の状況について、今回の分析結果（パラメータの推定値）を用いて予測してみる。choice_df における p1 の平均はおよそ10（千円）である。仮に、製品2は価格を買えないまま、製品1の価格を平均価格の \\(10\\%\\) である1000円値引き（\\(p1-1\\)）した場合の価格差を考える。 choice_df_v &lt;- choice_df %&gt;% mutate(p1_v = p1 - 1, p_ratio_v = p1_v - p2) est_probit_v &lt;- cbind(1, choice_df_v$p_ratio_v, choice_df_v$a1, choice_df_v$a2) %*% beta #pnormによって標準正規分布の分布関数による計算を実行する。 #製品1(c1) を選ぶ確率の予測値 pred_probit_v &lt;- pnorm(est_probit_v) mean(pred_probit_v) ## [1] 0.5459071 分析の結果、1000円の値下げによって企業1は製品の選択確率を約\\(54.6\\%\\)まで上昇することが示された。この値下げ幅によって生じる損失や費用と、上昇する選択確率による便益をどう捉えるのか、そしてどのような意思決定を行うのかは、企業の意思決定者に委ねるべき問題である。しかしながら、既存の消費者行動データをきちんと蓄積、分析することで、実際に値下げを実行しなくても消費者の選択がどう変わりうるかを予測できることは実務的にも非常に有効な手段であるだろう。 離散選択モデルは、プロビットモデルやロジットモデルを実行するための関数を用いて分析が可能である。しかしながら、引数の設定や出力される結果の解釈、ひいてはモデルそのものに内包されている理論的仮定については注意が必要である。これらの注意点を抑えるためには最低限の理論的知識が必要になる。そのため、本章では理論に関する説明も含め離散選択モデルについての紹介を行った。 離散選択モデルには、ここで紹介した以外にも様々なタイプが存在する。たとえば、被説明変数が離散的であり大小関係を持つような変数（ランキング、アンケート尺度など）の場合、順序プロビット（Ordered probit）や順序ロジットモデルを用いることが多い。また、被説明変数が打ち切りデータである場合、トービットモデルを用いる。打ち切りデータとは、データの上限や下限があるようなデータであり、例えば株の保有額は保有していなければ 0（下限）で保有している際はその評価額をとるため、打ち切りデータだと考えられる。また、打ち切りデータであり、 0 の観測が著しく多い被説明変数の場合には、ポアソンモデルや負の二項分布モデルが用いられる。例えば、ある製品の購買頻度や購買量について（アンケートなどを用いて）広く消費者から情報を得る場合など、多くの消費者においては購買頻度が 0 であると考えられる。このようなデータを被説明変数とする場合にはポアソン分布や負の二項分布を想定したモデルを利用することが多い。紙幅の都合上これらの詳細は割愛するが、関心のある読者においては計量経済学やマーケティング・サイエンスのテキストを参照してほしい。 また、本章の前半に紹介した通り、このアプローチは「自身の好みを理解し、首尾一貫した選択を行う」ような消費者像を前提としている。この前提は多くの状況に当てはまるものだと思われるが、そうではない状況もあるだろう。ここで想定している前提が崩れるような状況や個人的特性に着目する場合、異なる世界観（理論や学術領域）に基づく研究が必要になる。そのため、自身の捉えている問いや状況がどのようなものであるかを明確化し、それに整合的な理論と手法を選んで研究を進めることが重要になる。 "],["参考文献-7.html", "9.10 参考文献", " 9.10 参考文献 神取道宏（2014）「ミクロ経済学の力」，日本評論社. デビッドクレプス（2008）「MBAのためのミクロ経済学入門I価格と市場 中泉真樹他訳」，東洋経済新報社. 照井信彦・佐藤忠彦（2022）「現代マーケティング・リサーチ 新版」, 有斐閣. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Adams, C., P. (2021) “Learning Microeconometrics with R,” CRC Press. Fernihough, A. (2019) “Marginal Effects for Generalized Linear Models: The mfx Package for R.” McFadden, D. (1974) “The Measurement of Urban Travel Demand,” Journal of Public Economics, 3, 303-328. "],["empirics.html", "Chapter 10 観察重視アプローチ ", " Chapter 10 観察重視アプローチ "],["本章の概要-7.html", "10.1 本章の概要", " 10.1 本章の概要 本章では 3 章で紹介した観察重視（EF）アプローチについて紹介する。EFアプローチは、Golder et al. (2023）によって整理されたマーケティングにおける研究アプローチである。本書のこれまでの内容は、理論重視（TF）の研究アプローチに基づき構成されていた。TFアプローチはマーケティングにおける主流な研究法である。しかしながら、近年のデータ入手可能性の拡大や分析手法の洗練化によって、データ分析から探索的にマーケティングに関する知見を得る方法も無視できない重要なアプローチになった。しかしながら、EFアプローチはTFアプローチとは根本的に異なる目的とプロセスによって実施される必要がある。 本章では、（1）EFアプローチの概要、（2）EFアプローチによる研究プロセス、（3）EFアプローチに基づく研究紹介、（4）簡易的なデータ探索例を紹介する。特に、（4）のデータ探索例では、統計的な分析を用いずに顧客に関する洞察を得る方法を紹介する。EFアプローチ研究では珍しいフィールドデータと先端的分析手法とを用いる研究が目立つ。しなしながら、Golder et al. (2023) でも強調されているようにEFアプローチ研究における分析は簡単なデータ整理や分析から始めることが重要である。そこで本書では、Rを用いてデータを整理・処理することで、複雑な統計分析を行わなくてもマーケティング的知見を得られることを、顧客関係管理を例に紹介する。 "],["観察重視アプローチ.html", "10.2 観察重視アプローチ", " 10.2 観察重視アプローチ EFは、現実のマーケティング事象、問題や観測に関するデータの取得と分析により、マーケティングに関するインサイトを得ることを目的とした研究アプローチである。データのアクセス可能性や分析手法の拡張により、現実のデータを入手し、探索的なぶんせきを行うことで、何らかの知見を得ることが可能になった。しかしながら実際にはこのようなアプローチをとっているにも関わらず、通常のTFアプローチ的な構成（イントロダクション \\(\\rightarrow\\)理論・概念フレームワーク\\(\\rightarrow\\)仮説\\(\\rightarrow\\)方法\\(\\rightarrow\\)結果\\(\\rightarrow\\)ディスカッション）で論文を書くことは、疑わしい研究慣行として問題視されている。特に、HARKing（hypothesizing after the results are known）と言われる「結果を得てから仮説を作る」行為には注意が必要である。EFアプローチによって探索的なデータ分析を行った場合には、途中からTFに変更したり、装ったりすることはせず、EFアプローチとして研究を完遂し報告することが求められる。以下の表は、Golder et al. (2023, p.322) に記載されているTFとEFの対比を日本語訳したものである。 Table 10.1: TFとEFアプローチの研究の違い（Golder et al. (2023, p.322）より筆者訳 研究プロセスのステップ TFアプローチ EFアプローチ 理論のテストまたは開発 主要な目的 数ある結果の可能性のうちの一つ 研究の焦点の決定 研究は仮説に基づいた結果を中心に展開される 探索的思考での研究 研究プロセスの認識 直線的 反復的 研究プロセスの混乱への寛容さ 明快で内的一貫性があり、仮説を支持するような結果が期待される 混乱はよくあることで、資産になり得る。また、混乱は十分に活用され報告されるべきである 文献レビュー 検証可能な仮説を支持するためのストーリーラインを提供 先行研究がなくても構わないが、洞察やインスピレーションを与えてくれる文献があればなお良い 原因究明 焦点が絞られている 複数の角度からの説明が推奨される 事前のまとめ より厳密 より拡散的 概念的枠組みの開発 事前に構成要素と関係性を明確に構築する 構成概念とその関係性はゆるくつながれている：途中で概念枠組みが構築される可能性もある データ収集 理論/仮説を検証するための経験的証拠の収集 着目する事象について探求し理解するための経験的観察結果の収集 データ分析 仮説を評価するために統計的な分析を用いる 有意でない結果も含め、経験的な結果を記述・報告する。 頑健性のチェック 代替理論や説明論理を排除することに重点を置く：着目する調査の範囲内で頑健性がチェックされる 複数の説明論理や結果の差異を容認 頑健性が満たされない場合の判断 核心的な主張の信頼性を低下させる 学習機会と見なす 原稿の執筆 標準的なテンプレートが存在する 提案されるテンプレート: 現象に着目することへのモチベーション、さまざまな分析過程を説明し、得られたインサイトを述べる：理論的な示唆があることもあれば、ないこともある。 上記のように、研究に関する様々な側面においてTFとEF間の違いが確認できる。研究プロジェクトやその目的によって適したアプローチは異なるため、各研究にそったアプローチを採用し、自身の用いたアプローチを明確に記述することが求められる。 "],["観察重視アプローチの研究プロセス.html", "10.3 観察重視アプローチの研究プロセス", " 10.3 観察重視アプローチの研究プロセス Golder et al.（2023）は、論文内でEFアプローチに基づく研究プロセスも紹介している。具体的には、EF研究は主に以下の３つの段階を経る形で実施される。 研究機会の特定 地形探索 理解の発展 本節では、これらの要素について説明を行うことで、EFアプローチを実施するための基本的な考え方を共有する。 10.3.1 研究機会の特定 EFアプローチの研究を実行するために重要なことは、研究機会を発見し特定化することである。EF研究では、新興であり未開拓の現実の現象について分析を行うことが求められる。先入観を持たずに、実社会で起きているマーケティングやその他の事象から、既存の理論では説明できない問題や、マーケティングステークホルダー（例えば、消費者、実務家、政策決定者、教育機関など）が直面している問題や不都合を発見して捉えることが重要になる。 研究になりうる問題を捉えたら、その後、新しいデータソースを活用する必要がある。また、EF研究によって得た結果はマーケティングステークホルダーにとって活用可能であり、彼らの行動に直結するような示唆を含むことが求められる。そのため、データソースの選択においては、実行可能（主体が操作可能）な説明変数と結果に値する被説明変数の両方を捉える必要がある。 その後、EFアプローチを採用することの適切さを評価することが求められる。EFアプローチは以下の状況においては特に適当だと考えられる（Golder et al., 2023）。 - 理論の供給が不足している。 - 既存研究の知見が一貫していない。 - 直感的に、複数の相対するが尤もらしい帰結が予想される。 - 実世界やビジネスレポートによる観察が理論的予測と一貫していない。 - 経験的な発見（効果）の発生頻度については未検討である。 - 新しく豊かなデータへのアクセスによって未検討の関係性について探索できる。 これらの基準を満たす場合、EFに基づく研究を進め、次の段階として、データと思考を行き来するプロセスを繰り返すことになる。 10.3.2 地形探索 研究上の問いの設定は、EFアプローチにおいても踏襲すべき最初のステップである。しかしながら、EFアプローチの特徴は、初期に決めた問いからデータの収集及び分析を行った結果得た知見によって再度問いを考え直すことを許容し、推奨している（Golder et al., 2023）。そのため、データと思考の行き来することで、着目する領域についての地形を探索することが求められる。 EFアプローチに基づく研究は多くの場合探索的な問い（「ある領域における重要な要素はなにか？」や「どうなっているか？」）に基づき研究が始まる。そのため研究者が着目する事象についてわからないことも多く、研究の進捗に応じて当初設定した問いが修正されることもある。例えば、データを収集し分析を行うことで実情についての知見を得た結果、より広範で深い問いに直面することもあるだろう。その場合、新しい研究上の問いに着目することでより高次な知見につながるため、研究の範囲や問いは（再）検討されるという特徴を持つ。 データの収集及び分析はEFアプローチ研究における重要なプロセスである。研究者はデータの作成、接合や取得を通じて、自身が着目するユニークなデータセットを構築する必要がある。 データセットを構築した後はそれを分析するのだが、まずはシンプルなデータの確認や分析から始めることが推奨される。モデル化をしていない発見事項（Model-free evidence）によって重要な変数の特徴、変数間の関係について把握し、その後のモデルに基づく分析に移行することが期待される。データの可視化も有力なアプローチであり、テキストマイニング、クラスター分析（11章）や、因子分析（12章）などによるデータ構造の単純化を伴う方法と組み合わせることで、有益な情報提示になりうる。そして、model-free evidence を得た後に、様々な頑健性のチェックや因果関係の検証を行い、より高度な批判に耐えうる分析結果を得ることが求められる。 分析を行った後、EFアプローチでは、得たデータや結果についての検討を行うことが、データ分析プロセス内で求められる。例えば、「新たなデータを利用することで結論が変わりそうか？」のような問題を検討することでデータおよび結果の信頼性について検討することはEFアプローチ研究の結果に関する信頼性を向上することにつながる。また、「発見事項が実務や社会的に洞察に富むものかどうか？」、「検出された効果量（effect size）が意味のあるものか？」ということについての検討も、有効な検討となりうる。その検討の結果、必要であれば問いの（再）検討に移行し、改めてデータ探索プロセスを繰り返す。一方でこれらの検討事項に基づ、追加的な検証が必要ないのではないかと判断された場合、データ探索を終了する。 10.3.3 理解の発展 データを探索することで得られた知見（観察結果、発見事項）に関する一般化可能性を検討することが求められる。EFアプローチによって観察された結果が、理論的、数学的、視覚的手法を用いて単純化された形で、他の文脈や環境においても繰り返される規則性の発見へつながることがある。その場合、EFによる発見物の貢献は大きい。また、その差異、効果の方向性（例えば、正負）だけでなく、効果量に関する規則性についても言及することができると、通常の理論的枠組みでは提供できない知見として、大きな社会的意義を持つことが期待される。 必須ではないとはいえ、EFアプローチによる研究でも新たな概念的、理論的含意を提供することはある。EFアプローチによる研究は現象に関するステークホルダーに関する行動可能な含意を重視するが、抽象化を行うことでより一般化可能なマーケティングに関する知見や理論的基盤の構築につながる。 10.3.4 EFアプローチ研究の報告 EFアプローチ研究により得た結果を論文化する際には、いくつか注意すべき点が存在する。第一に、当該研究がEFアプローチによるものであることを明確に伝え、なぜそのアプローチが適切なのかを説明する必要がある。そのため、EFアプローチによって実行された研究をTFアプローチかのように整理し論文化することは避けるべきである。そのようなアプローチはHARKingとして不適切な研究態度と問題視される可能性がある。 しかし、一気通貫したプロセスではない研究過程を読者に伝えるのは困難である。そのため、EFアプローチの論文であっても、研究者が経験した全ての紆余曲折を記述する必要はない。しかしながら当該研究が持つ探索的な性質については反映した原稿を書く必要がある。例えば、論文の冒頭にフローチャートや論文の構造について明示し、それに沿った節構成や図表を採用することで、読み手の労力を下げるような工夫が求められる。 そのうえで、論文内で伝える内容は発見事項に誠実である必要があり、過度な一般化や根拠のない実務的含意の提供は避けなければならない。当該研究の発見を超えた一般化や実務的含意については、その後に続く別研究により達成されるべきである。また、Appendixなどを使って詳細な経験的発見物を共有することも重要である。このような知見をきちんと構造化して提示することは後続の研究（特に、メタ分析など）にとって有意義な情報となり、学術的貢献を持つ。 "],["efアプローチ研究例.html", "10.4 EFアプローチ研究例", " 10.4 EFアプローチ研究例 ここでは、EFアプローチ研究の例として、Chung et al. (2022) を紹介する。Chung et al. (2022) は、消費者がオンラインホームシェアプラットフォームに着目し、シェアリングエコノミーにホストととして参加する動機にどのようなものがあるか、そして動機の違いがゲストの満足度やホストの生涯価値の変化をどう説明できるかを分析した研究である。著者らは、Airbnbのデータを用いて、13,337人のホストから得たテキスト回答を分析し、ホストの動機を明らかにした。その後、291,746件の予約取引に関する情報を抽出し、動機により結果変数（ゲストの満足度やホストの生涯価値）がどのように変わるかを分析した。 消費者の動機は、非常に素朴かつ重要な情報でありながら、その調査困難性からあまり研究において着目されてこなかった経緯がある。近年では、プラットフォームやマーケットプレイスを通じて消費者が自身の所有物を他者に貸し出すような行動が見られるようになった。このような活動はシェアリングエコノミーをと呼ばれ、その市場規模も拡大している。 シェアリングエコノミーという新しい消費・取引形態の重要性が実務的にも学術的にも増す一方で、消費者の動機に関する未開拓な状況は引き続いていた。消費者による財の供給者（ホストなど）としてのシェアリングエコノミーへの参加については、様々な動機が考えられる。そして消費者がどのような動機でプラットフォームに参加するのかは、重要な問いになりうる。その理由は、参加動機が異なる場合、プラットフォーム上での振る舞い（提供する情報）に違いが生まれる可能性があり、ひいては受容者（ゲストなど）の満足度や供給者の得る成果にも影響を与えるかもしれない。しかしながら、この点について先行研究では十分に検討されてこなかった。 この問題に対して Chung et al. (2022) は、Airbnb という代表的なオンラインシェアリングエコノミープラットフォームであり、物件（部屋や建物）を共有するサービスにおける2種類のデータセットを用いて探索した。1つ目は、Airbnb上で実際のユーザーに対して行った調査データである。具体的には、2013年3月から2014年10月までの期間に渡ってAibnbのホストに対してサイト上で表示された自由回答式設問（なぜホスティングを始めたのですか？“Why did you start hosting?”）への回答（テキストデータ） 13,337件を用いた。2つ目のデータはAirbnb内における 291,746件の予約取引に関するデータ（予約日、宿泊数、人数、価格、ゲストのレビューレーティング、物件に関する詳細な情報）である。 ホストの動機を明らかにするため、著者らは自由回答で得たアンケートデータに対し、機械学習におけるマルチラベル学習（multi-label classification）トピックモデル（topic model）を用いることで複数の動機を抽出した。発見された動機は（1）現金を稼ぐ、（2）美しさを共有する、（3）人々と出会う、（4）状況依存（空室を持つなど）、（5）Paying forward、（6）他者からの紹介、（7）その他、であった。そのため、ホストの動機として金銭的な動機と内的な動機（美しさや出会い）が存在することが大規模なフィールドデータから明らかになった。また、これらの動機は具体的なテキストデータから抽出された頻出ワードからラベリングされたものであるため、これらの概念に対応するワードも同時に確認されている。 2つ目のデータセットにおける分析では、ホストの初期エンゲージメント（Airbnb プラットフォームに参加する時点での、物件についての記載内容における文字や写真の数）を計測した。また、上記の動機の抽出で見つかったワード（美しさの共有や、人々との出会い）を使い、内的な動機をもったホストを特定し、動機の違いによるホストのエンゲージメントに関する違いを分析した。分析においてはまず model-free evidenceとして平均の差の検定を行い、内的な動機を持つホストはエンゲージメントが高い一方で、金銭的な動機を持つホストはエンゲージメントの程度が低いことを明らかにした。この結果をより生地に検証するために筆者らは、様々なコントロール変数を用いた回帰分析を行った。結果として、内的な動機を持つホストはエンゲージメントが高いという結果については一貫した結果を得ている。 また、筆者らはシナリオを用いた消費者実験を行うことでこの結果の妥当性を検証した。実験の参加者にはランダムに異なる動機を含むシナリオを提示され、その後彼/彼女らはある物件をAirbnbで貸し出すための広告文章を書いた。なお、物件に関しては全ての参加者を通じて共通である。つまり、実験的にランダムに操作（提示）された動機よって物件に関する記載が変化するかを確認するデザインになっている。実験により得たデータを分析した結果、金銭的な動機に直面した場合に比べ、美しさの共有や人々との出会いといった動機に直面した被験者はより多くの文字数を用いることが確認された。つまり、動機の違いによるエンゲージメントの程度については実験的な操作を通じても再現される規則性を持った結果である可能性が高い。その後 Chung et al. (2022)では、Airbnb内の予約取引データを用いて、動機 \\(\\rightarrow\\) エンゲージメント\\(\\rightarrow\\) ゲストの満足度（星5評価レビューの比率）という媒介関係を確認した。 また、同論文内ではBayesian laten attribution model を用いて、動機がAirbnb内での活動パターンと生存（掲示物件の残存）へ与える影響も分析している。その結果、人々と会うためという動機を持つホストは、それ以外のホストと比べ、解約傾向が低く予約泊数が多い一方で、一泊あたりの価格は低いということがわかった。美しさの共有という動機を持つホストは予約泊数は低いものの価格は高いという結果であったものの、解約傾向については有意な結果を得られなかった。金銭的な動機を持つホストでは有意に低い価格が設定されるが、予約泊数や解約傾向については有意な結果が得られなかった。予約傾向について高い傾向を持つホストは状況依存による動機を持つであった。 このように Chung et al. (2022) ではリッチなフィールドデータおよび実験調査と様々な先端的分析手法を組み合わせることで、シェアリングエコノミーにおけるホストの動機と、それらの動機のプラットフォーム内での活動や成果との関係について明らかにした。 "],["簡易的なデータ探索法紹介統計的分析を用いない顧客分析.html", "10.5 簡易的なデータ探索法紹介：統計的分析を用いない顧客分析", " 10.5 簡易的なデータ探索法紹介：統計的分析を用いない顧客分析 10.5.1 データ処理と顧客関係管理（CRM） 前節で紹介したように、EFアプローチ研究では珍しいフィールドデータと先端的分析手法とを用いる研究が目立つ。Chung et al. (2022) で扱ったような分析手法は本書の範囲を超えるため扱わない。しなしながら、Golder et al. (2023) でも強調されているようにEFアプローチ研究における分析は簡単なデータ整理や分析から始めることが重要である。 そこで本節では、Rを用いてデータを整理・処理することで、複雑な統計分析を行わなくてもマーケティング的知見を得られることを、顧客関係管理を例に紹介する。 10.5.2 データの構造変化とソート 本節では、学術的に価値を持つような広範な影響力を持つEFアプローチ研究ではなく、特定の企業における主要顧客を発見することを目的とした探索的分析例を紹介する。 現代のマーケティングリサーチでは、顧客の購買データを用いて（統計的な分析を要さず）重要顧客や顧客層を発見することが、小売企業を中心に広く行われている。ここでは基本的に、ID-POSデータを用いたデータベースの正規化と集計2焦点を合わせ、そのための分析手法を紹介する。特に、顧客個人に関する情報を用いながら企業や店舗にとって重要な顧客を特定し、その顧客との関係性を深めた場合を考える。 小売企業や店舗の運営効率から考えると、単に来店客数を増やすだけでなく、より頻繁に、より高額の買い物をする顧客を特定し、その人（達）の購買を促進することが効果的になる。言い換えると、企業や店舗は、ロイヤルカスタマーを特定し、その顧客との関係性を構築したいと考えるのである。そのためにはまず、ロイヤルカスタマーを特定する作業が必要になる。そこで本節では、データから企業にとって価値のある顧客を発見する方法について、データの前処理技術を応用する形で紹介する。 本節では簡単に、単純なデータハンドリングから顧客インサイトを得る方法を考える。特に、データ処理とソーティング（順番の入れ替え）を用いる方法を用いる。本節ではID-POSデータを用いた分析として、デシル分析とRFM分析を紹介する。デシル分析は、支出額をもとに上位から顧客を並べ替え、その順番に基づき顧客を10分割することで、上位の支出額を担うランクに属する顧客を特定する。なお、他の指標で同様の分析を実行することも可能だが、一般的には支出額を用いることが多い。例えば、月当たり5000人の顧客がいるとすると、500人ずつのグループに分け、購買額の大きい順にデシル1〜10 (10〜1の場合もある) とする形でランク分けする方法がこれにあたる。このとき、各顧客の情報がポイントカードやアプリで紐付いているのであれば、最も購買額の多いグループの特徴を整理することで、現在購入額の高い顧客がどんな特徴を持つのか理解できる。 一方、RFM（Recency, Frequency, Monetary）分析は、取引情報から、最近いつ買ったか、どれだけの頻度で買い物するか、どれだけ支出しているかといった情報を総合的に勘案し、どの顧客が最重要かを特定する方法である。これらの指標は、ロイヤルティや再購買確率が高い顧客を判別するのに役立つ3つの指標である。例えば、最終購買日から時間が経っている顧客は離反しているかもしれないし、購買頻度や購買額が高いと、ロイヤルティが高い可能性が高い。また、クーポンや割引利用の有無の情報と紐付けることができれば、当該顧客がチェリーピッカーか否かも判断することができる。 ID-POSデータは、各顧客の会員IDについての情報はありながらも取引ベースで情報が整理されている。このようなデータに対して以下の手順を用いてデータを集計・ソーティングする。 顧客IDごとに、各取引情報を集計する。 顧客ID情報についてまとめたデータベースにおける順番をソートし、重要顧客を識別する。 このようなデータの集計は、データ構造を取引ベースから顧客IDベースに変換することを可能にする。下図は、取引ベースのID-POSデータを、集計作業によって顧客IDベースのデータ構造に変換するイメージを示したものである。ID-POSデータは、顧客ID情報が含まれていながらも、データの行（観測）は各取引を示している。そのため、仮に同じIDの顧客がデータ収集期間に複数回取引を行っている場合、同じIDを含む観測がいくつも見られることになる。一方で下部の顧客IDベースのデータは、ID-POSデータを顧客ID情報によって集計したものであり、一定期間中に特定のIDを持つ顧客がどのような購買行動を示していたかを捉えたデータである。そのため、データの行は各顧客IDを示している。本節では、まずはじめにこのようなデータ構造の変換について説明する。 データ構造変化 ここでは、先程利用した idpos データを用いて作業を進める。改めて、当該データを以下のコマンドで確認する。 idpos &lt;- readr::read_csv(&quot;data/2022idpos.csv&quot;, na = &quot;.&quot;) ## Rows: 3000 Columns: 4 ## ── Column specification ──────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): date ## dbl (3): id, spent, coupon ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(idpos) ## # A tibble: 6 × 4 ## id date spent coupon ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019/9/25 14326 1 ## 2 32 2019/9/10 10232 1 ## 3 30 2019/9/9 6881 1 ## 4 29 2019/9/4 6365 0 ## 5 46 2019/9/10 7595 1 ## 6 44 2019/9/14 7858 0 ここで、date変数を用いて直近で何日前の来店かを示す変数を作成する。このデータは、2019年09月01日 から 2019年10月01日までの一ヶ月間、とある店舗で記録された取引データであると仮定し作成されている。そのため、データ収集終了最新時点（2019-10-2）と来店日時の差を表す変数を作成する (ここでの処理にエラーが出る場合は、idpos$date &lt;- as.Date(idpos$date) というコマンドを事前に試してから変数の定義を行ってほしい)。head関数により出力された結果によって新たな変数（datediff）が追加されたことがわかる。 idpos$datediff&lt;- as.numeric(difftime(&quot;2019-10-02&quot;,idpos$date,units=&quot;days&quot;)) head(idpos) ## # A tibble: 6 × 5 ## id date spent coupon datediff ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12 2019/9/25 14326 1 7 ## 2 32 2019/9/10 10232 1 22 ## 3 30 2019/9/9 6881 1 23 ## 4 29 2019/9/4 6365 0 28 ## 5 46 2019/9/10 7595 1 22 ## 6 44 2019/9/14 7858 0 18 続いて、パイプ演算子を使ったデータ処理によって顧客IDベースの形へ集計する。ここでは特に、group_by() という関数を使い、顧客id (今回は性別情報も残したいので gender も加えている)をグループ化の基準と指定する形で集計を行う。また、CRM分析で使う変数のために、idレベルでの集計という形で以下の変数を作成する。そして、以下の変数を用いて集計した新たなデータセットを “idpos_cust” として定義する。 frequency：各idの出現頻度をn()でカウントする monetary：spentの合計をsum()で計算する cherry (picker)：クーポンの利用回数の合計をsum()で計算する recency：datediffの最小値をmin()で求め、直近でいつ来たかを判別 idpos_cust &lt;- idpos %&gt;% group_by(id) %&gt;% summarize(frequency = n(), monetary = sum(spent), cherry = sum(coupon), recency = min(datediff) ) head(idpos_cust) ## # A tibble: 6 × 5 ## id frequency monetary cherry recency ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 21 173314 8 1 ## 2 2 20 157976 13 2 ## 3 3 21 134673 5 1 ## 4 4 19 154416 10 4 ## 5 5 20 177156 11 3 ## 6 6 19 151853 11 3 上記の操作によって、元々のidposデータから、顧客ベースのデータ構造（idpos_cust）に変換できたはずである。しかしこれだけでは、まだ我々は誰が重要顧客か特定できない。そのため、次に我々はデータの並べかえを行う。具体的には、支出額が高い順に並び替えたあとに上位20人の顧客を表示する。 idpos_cust_m &lt;- idpos_cust %&gt;% arrange(desc(monetary)) ##Customers in the top 20 (Monetary) idpos_cust_m[1:20,] ## # A tibble: 20 × 5 ## id frequency monetary cherry recency ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15 31 305665 13 1 ## 2 35 30 262181 13 1 ## 3 31 31 254804 19 3 ## 4 33 29 251352 14 2 ## 5 22 24 214150 9 4 ## 6 20 21 199197 11 2 ## 7 18 24 196686 13 3 ## 8 49 24 196143 12 4 ## 9 9 24 195788 14 1 ## 10 12 25 194860 15 1 ## 11 42 21 193582 5 1 ## 12 30 26 190687 17 2 ## 13 36 20 189937 13 6 ## 14 14 21 188600 14 1 ## 15 26 22 184657 10 2 ## 16 32 20 184276 12 1 ## 17 17 19 181947 13 1 ## 18 44 23 181397 12 1 ## 19 34 20 179533 8 2 ## 20 37 21 179169 11 2 10.5.3 データ結合 ここまでの結果からは購買額の高い顧客IDを特定することができた。しかしながら、これらの顧客がどのような特徴を持っているのかについては推察できない。そのため、別で管理されていた顧客情報を捉えたデータセットと結合することでこれらの顧客についての属性を把握する。 以下では、今回使用する顧客情報データセットを読み込み、その概要を示している。このデータセットには、3000人分の会員登録済み顧客情報が蓄積されており、以下の変数を含む： id: 顧客ID gender: 性別 age: 年齢 famsize: 世帯人数 id_data &lt;- readr::read_csv(&quot;data/id_data.csv&quot;, na = &quot;.&quot;) str(id_data) ## spc_tbl_ [3,000 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:3000] 1 2 3 4 5 6 7 8 9 10 ... ## $ gender : chr [1:3000] &quot;female&quot; &quot;male&quot; &quot;female&quot; &quot;female&quot; ... ## $ age : num [1:3000] 51 38 41 24 48 46 36 30 26 57 ... ## $ famsize: num [1:3000] 2 2 1 1 5 1 1 1 3 5 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. gender = col_character(), ## .. age = col_double(), ## .. famsize = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; ここで、顧客データと購買データを left_join() を用いて、idpos_cust をメインとする形で id によって結合する。left_join() は左側に指定したデータフレームに存在知するキーの行を返す形でデータの結合を行う。言い換えると、左側のデータセットに存在する行（観測）はすべて残され、そこに新たな変数を加える形でデータフレーム間の結合を行う。 idpos_cust &lt;- left_join(idpos_cust,id_data, by = &quot;id&quot;) head(idpos_cust) ## # A tibble: 6 × 8 ## id frequency monetary cherry recency gender age famsize ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 21 173314 8 1 female 51 2 ## 2 2 20 157976 13 2 male 38 2 ## 3 3 21 134673 5 1 female 41 1 ## 4 4 19 154416 10 4 female 24 1 ## 5 5 20 177156 11 3 female 48 5 ## 6 6 19 151853 11 3 female 46 1 上記の通り、顧客ベースの取引情報に、各顧客の属性情報が追加された事がわかる。これを利用し、以下のように上位20顧客の性別比率を以下のように確認する。これによって、主要顧客に占める性別比率が確認できる。 idpos_cust_m &lt;- idpos_cust %&gt;% arrange(desc(monetary)) #gender ratio in the top 20 table(idpos_cust_m[1:20,]$gender) ## ## female male ## 16 4 続いては先述のデシル分析を実行する。具体的には、idpos_custに対し、cut()関数を使うことで、monetaryの大きさに基づきサンプルを10等分し、新たに “decile_rank” という変数（列）をデータに追加し、その新たなデータセットを “idpos_cust_m”と定義する。なお、次節にてこのデータを改めて使うため、データをprojectのdataディレクトリ内に保存しておいてほしい。 idpos_cust_m$decile_rank &lt;- cut(idpos_cust_m$monetary, quantile(idpos_cust_m$monetary, (0:10)/10,na.rm=TRUE), label=FALSE,include.lowest=TRUE) head(idpos_cust_m) ## # A tibble: 6 × 9 ## id frequency monetary cherry recency gender age famsize decile_rank ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 15 31 305665 13 1 female 19 3 10 ## 2 35 30 262181 13 1 female 22 3 10 ## 3 31 31 254804 19 3 female 55 1 10 ## 4 33 29 251352 14 2 male 54 5 10 ## 5 22 24 214150 9 4 female 53 4 10 ## 6 20 21 199197 11 2 female 50 1 10 readr::write_csv(idpos_cust_m, &quot;data/idpos_customer.csv&quot;) head()関数によって新たな変数の追加を確認したあとは、各デシルの店舗売上への貢献度を確認する。ここでは、decile_rankをグループ化の基準として設定し、summarize() によって集計する方法を用いる。その後、各デシルの売上比率を計算し、高い順に並び替える。集計・分析の結果は、上位20%の顧客で、ID-POSに計上されている売上の57%を締めていることを示した。 decile &lt;- idpos_cust_m %&gt;% group_by(decile_rank) %&gt;% summarize(freq = n(), monetary = sum(monetary)) total &lt;- sum(decile$monetary) decile2 &lt;- decile %&gt;% mutate(percent = monetary/total*100) %&gt;% arrange(desc(decile_rank)) decile2 ## # A tibble: 10 × 4 ## decile_rank freq monetary percent ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 149 11141710 45.1 ## 2 9 149 3042467 12.3 ## 3 8 147 2220019 8.98 ## 4 7 150 1976807 8.00 ## 5 6 148 1687031 6.83 ## 6 5 149 1466609 5.93 ## 7 4 149 1213922 4.91 ## 8 3 148 939338 3.80 ## 9 2 149 670413 2.71 ## 10 1 149 358988 1.45 本節で示したように、高度な統計的分析を実行せずとも重要顧客を特定する事が可能になる。本節では特に、データの集計や処理技術を使った方法を紹介した。このような分析によって回答できる問いは「企業にとっての重要顧客は誰か」というものだろう。この問いは非常に興味深く実務的にも有意義なものであるが、以下の点に注意することが重要である。第一に、何をもって顧客の重要性を定義するかという問題である。本節では特にRFMなどの基準を用いて、観察可能な購買結果をもとに重要顧客を識別する方法を捉えた。しかしながら、例えば購買頻度と購買額では異なる側面を捉えており、どの指標を用いて分析するかによって（通常は）結果が異なる。そのため、研究者自身が「重要性」をどのように定義するのかを注意深く判断し、なおかつそれをレポートやプレゼンテーション内できちんと明示する必要がある。さらに、データでは捉えきれない側面は分析結果に反映されないという点についても注意が必要である。例えば日本には江戸時代から続く小売企業もいくつも存在する。仮に、そのような小売企業と、長期間代々取引を続けている顧客（一族や企業）がいたとして、さらにその顧客が分析による上位顧客に含まれなかったとする。その場合、この顧客を重要顧客でないと切り捨てて良いのでだろうか。災害、国家の統治体制の変化、戦争、などの激動を経てなお取引が続いている顧客は重要でないと言い切れるのだろうか。もちろん、このような顧客を重要でないと捉えることも、経営判断として間違ったものではない。しかしながら、少なくともデータを用いた分析結果を過信しすぎず、データによって何が捉えられており、何が捉えきれていないのかについて研究者・意思決定者のどちらも自覚的になることが必要になる。 第二に、分析を行うだけでマーケティング実務が完結するわけではないという点についても注意が必要である。「重要顧客を特定する」という研究課題の背後には、「CRMを実行して収益性を向上させる」という実務的課題が存在しているはずである。そのため、今回の発見物をもとに、マーケティング活動への示唆を与えていくことが重要になるのだが、誰が重要顧客か、という問いに答えるだけでは具体的な活動指針（アプリを通じた囲い込みや、訪問販売等）を与えるのは難しい。そのため、重要顧客のライフスタイルや価値観などの彼/彼女らの特徴に踏み込んだ調査を行うことも必要になるかもしれない。昨今のロイヤルティプログラム（ポイントシステム）では、モバイルアプリを通じて個人の様々な行動履歴が記録されたり、アンケートへの回答を促されたりすることがあるだろう。これらの情報とID-POSデータをうまく接合できれば、重要顧客を特定しつつ、それらの顧客に適したCRM方策を策定できるかもしれない。 "],["練習問題-6.html", "10.6 練習問題", " 10.6 練習問題 “2022idpos2” と “id_data2” という前節とは別のデータセットを用いて、重要な顧客もしくは顧客グループを判別してみよう。また、その顧客グループの特徴についても整理し、その結果に基づくマーケティング方策案を提示してみよう。 "],["参考文献-8.html", "10.7 参考文献", " 10.7 参考文献 Chung, J., Johar, G. V., Li, Y., Netzer, O., Pearson, M., Inman, J. J., &amp; Winer, R. S. (2022). Mining Consumer Minds: Downstream Consequences of Host Motivations for Home-Sharing Platforms. Journal of Consumer Research, 48(5), 817-838. Golder, P. N., Dekimpe, M. G., An, J. T., van Heerde, H. J., Kim, D. S. U., &amp; Alba, J. W. (2023). Learning from Data: An Empirics-First Approach to Relevant Knowledge Generation. Journal of Marketing, 87(3), 319-336. "],["cluster.html", "Chapter 11 セグメントとクラスター分析 ", " Chapter 11 セグメントとクラスター分析 "],["本章の概要-8.html", "11.1 本章の概要", " 11.1 本章の概要 本書ではこれまで、統計的な推定や検定を用いて仮説を検証する方法を説明してきた。これに対して 10 章では、データを探索的に分析することでマーケティングに関する洞察を得ることへ関心が集まっていることを紹介した。データ分析のアプローチにおいても、検証のようなプロセスを経ずに、データの持つ情報を集約・整理することで探索的に分析結果を得るものが存在する。マーケティング領域で広く使われる探索的なデータ分析手法として、クラスター分析がある。 クラスター分析は、複数の量的変数情報に基づいて、データのサンプルをいくつかのグループに分類する方法である。この手法は、「セグメンテーション」というマーケティング実務的枠組みと関連している。マーケティングの基本的な戦略方針として、セグメンテーション、ターゲティング、ポジショニングがある。これは、市場を構成する消費者を細分化し、標的とするグループを特定化することで、具体的なマーケティング要素をそのグループに合わせて調整することで、市場におけるポジションを確立するという、実務的方針である。しかしながら人間が自身の認知能力によって数多く存在する消費者の中からセグメントを発見・弁別するのは容易ではない。 このような限界を克服するため、消費者に関する情報を集めたデータセットを用いて、消費者間の類似性をもとにグループ分けを行う方法がクラスター分析である。クラスター分析では、回帰分析のように着目する目的変数を用いず、入力されたデータそのものに着目する。このようなアプローチでは、分析者の判断が重要な役割を担う。そのため、本章では、クラスター分析の実行方法に加え、分析手法そのものの概要についても理解することを目的とする。 この目的を達成するために、本章では以下の内容を扱う。 セグメンテーションとマーケティング意思決定についての復習 クラスター分析の概要と階層的クラスター分析 非階層的クラスター分析とKmeans法 Rを用いたクラスター分析の実行 本書では、（1）階層的クラスター分析と（2）非階層的クラスター分析（K-means法）という二つのクラスター分析の手法を紹介する。階層的クラスター分析はデータの中から類似している観測値を段階的にクラスター（観測値の集団）としてまとめていき、最終的にすべてのデータが1つのクラスターになるまでそれを繰り返す方法である。この方法では、観測値同士の類似性（距離）やクラスター同士の類似性に基づき似たものから順にまとめていく。階層的クラスター分析の結果としては主に、デンドログラムと呼ばれる樹形図のような図を出力する。階層的クラスター分析ではこの結果をもとに、研究者がいくつのクラスター数でこのデータをまとめ上げることが良いのかを判断することになる。その判断においては多くの場合、分類結果の「効率性」と「有効性」のバランスが基準になる。効率性は分類によってどれだけ多くの情報を集約し説明できているかを表しており、より少ないクラスター数で多くのデータを説明できたほうが情報の集約による効果が大きいと考えられる。しかしながら、得たクラスター分類が役に立たないと意味がない。そこで、分類結果がどの程度現実的な含意につながるかを捉えたのが有効性である。効率性を意識しすぎて少ないクラスター数による分類を採用しても、あまりに大雑把過ぎる分類だと分析結果が有益にならないため、ある程度クラスター数を増やしたほうが有効な分類になるかもしれない。 非階層的クラスター分析は、階層的クラスター分析によってクラスター数の目ぼしをつけた後に実行することが一般的である。それは、非階層的クラスター分析を実行するためには、研究者が事前にクラスター数を指定することが必要なためである。非階層的クラスター分析では、指定されたクラスター数を所与として、計算・分析が実行される。そのため本書では、非階層的クラスター分析に加え、エルボー法などの手法を併用し、クラスター数を決定した後に非階層的クラスター分析を実行する手段を紹介する。 クラスター分析の概要を説明したあとは、Rを用いた分析手法を紹介する。本書では、吉田秀雄記念事業財団によって2023年に実施され、オンライン上に公開されている消費者調査アンケートデータを用いて消費者クラスターを発見することを試みる。ここでは、クラスター分析の実行に関するコードに加え、プレゼンテーション等で活用できる図示化の方法についても紹介する。 "],["セグメンテーションとマーケティング意思決定.html", "11.2 セグメンテーションとマーケティング意思決定", " 11.2 セグメンテーションとマーケティング意思決定 セグメンテーションは、セグメンテーション・ターゲティング・ポジショニング（STP）に代表される基本的なマーケティング枠組みを構成する重要な要素である。マーケティングにおいて企業は、製品やサービスを通じて顧客に対し価値を提供することを目指すのだが、そのためには、対象となる市場においてどの部分（セグメント）を狙うのかを特定化する必要がある。市場を構成する消費者は、多様な好みや特徴を持っている。その中から似た好みや特徴を持つ消費者グループを見つけ出し特定化することがセグメンテーション（市場細分化）である。 市場におけるセグメントを明らかにすることは、より効果的に標的への価値提供を行うことにつながると期待できる。セグメンテーションを行わない場合、様々な消費者の好みからバランスを取るように、平均的な特徴をもつような製品・サービスを開発することになる。例えば、ホットティーが好きな人もいれば、アイスティーが好きな人もいる。しかしながら、これらの間を取った「ぬるいお茶」を提供しても、誰の好みにも響かない可能性がある。つまり、万人受けを狙って中途半端な製品・サービスを提供することを避けるためにも、市場を細分化して、共通の好みを持つ消費者グループを判別することはとても重要になる。 セグメンテーションを行うための分類基準として、企業はいろいろな情報を活用すると考えられる。理想的には、企業は消費者が求めている便益や好みに基づき市場を細分化したいと考えるだろう。例えば自動車市場において、ある消費者は技術的革新性を、別の消費者は快適性を求めるかもしれない。このような消費者のニーズに基づき市場を細分化することができれば、より効果的な標的市場を決定できる。しかしながら、消費者のニーズに関するデータを直接的に入手できない場合も多い。そのようなときには、（1）消費者属性、（2）心理的情報、（3）行動的情報についてのデータをニーズにつながる代理指標として用いてセグメンテーションを行うことが多い。 消費者属性としては、人口統計的情報や地理的情報が用いられる。人口統計的情報には、年齢、職業、所得、世帯人数、教育水準などが含まれる。これらの情報が消費者のニーズと連動している場合には、人口統計的情報に基づく市場細分化は効果的であると言える。例えば、高品質だが高価格な製品と低価格だが質の低い製品がある場合、所得による市場細分化は効果的になるかもしれない。また、子供がいる世帯といない世帯では、異なるニーズを持っていることも予想できる。 地理的情報として、消費者の居住地域によって異なる好みを表すことがある。例えば、関東と関西のように、食品の味付けに関する好みについて日本国内の地域間で違いがうかがえる。カップうどんやスナック菓子においては、このような味付けに関する地域差を反映させた製品開発を行っており、製品を販売する地域に応じて味付けを変えている。このような人口統計・地理的情報は、測定可能性・容易性が高いという利点を有している。自社内部データや自治体の人口動態情報などによって、自社顧客や特定地域における消費者特性の情報について得ることができる。これらの情報が消費者のニーズや好みと連動している場合には、非常に有力なデータとなるものの、マーケティングの本来的な目的はこれらの情報に基づくセグメントを発見することではないことには、注意が必要である。あくまで、これらの情報の背後にある消費者のニーズの代理指標として利用していることを理解することが求められる。 心理的情報としては、ライフスタイルや価値観、志向といった、特定の商品への評価そのものではない消費者の心理的特性を捉えた指標が用いられる。これらの情報については、アンケート調査などを用いた心理学的手法を用いることによって把握することができる。例えば、低価格志向や環境意識などは製品・サービスに求める価値観と近い心理的特性であるため、例えば自社の既存顧客や店舗を出店しているエリア内において環境問題への関心に基づきセグメントを確認することは、製品開発上重要な情報になりうる。 また、行動につながる心理的な態度（ブランドに対する好意）を捉えた類型も可能になる。これにより、どのようなセグメントが何を好むかを把握することができる。例えば、英国における “comfortable green” というセグメントは、Fairtrade や Green &amp; Black’s を好むが、Asdaやコカ・コーラは嫌っているだと言われている（Jobber and Elis-Chadwick, 2020）。 行動的情報の代表的なものとしては、購買行動に関する情報が挙げられる。例えば、製品やサービスの購買・利用頻度を顧客によるブランドロイヤルティの代理指標として用いることも多い。セグメンテーションを行うためにはまず、「どの情報に基づきセグメンテーションを行うのか」という基準となる情報の選定が求められる。 セグメンテーションは、図11.1 に示されるように、市場に存在する多様な消費者をいくつかのグループに分類することである。セグメンテーションが可能であることの前提には、（1）市場における消費者の好みが異質であることと、（2）共通した好みを持つ消費者のまとまりは見いだせるが、同時に見出したまとまり間での好みの差異も見いだせることである。つまり、異なる好みを持つ消費者の中から、「グループ内類似性」と「グループ間差異」とを基準に複数のグループを見出すことがセグメンテーションであるといえる。 Figure 11.1: セグメンテーション概要 "],["stpその後.html", "11.3 STPその後", " 11.3 STPその後 セグメンテーションが完了すると企業はターゲティングを行う。ターゲティングにおいては、発見したセグメントの中から標的とする特定のセグメントを選ぶ。その際、自社にとって魅力的なセグメントを選ぶのだが、その魅力は主に以下の要因によって規定される（Jobber and Ellis-CHadwick, 2020）： 市場要因：セグメントのサイズ、成長率、収益性 競争要因：競争の状態（数量的情報だけでなく、質的情報も重要）、新規参入者、差別化可能性 政治・社会・環境要因：政治的・社会的傾向、環境問題、技術的変化 基本的に企業にとっては、規模が大きく、成長性も収益性も高いセグメントは魅力的である。しかし、そのようなセグメントに対しては競争も激しくなると予想できる。そこで、競争の状態を観察するのだが、ここでは競合の数だけではなく競合の持っている強みが自社とどのように異なるかを検討することも重要になる。例えば、自動車メーカーにとってアメリカ市場は非常に魅力的なマーケットである。しかしながら、伝統的に欧米の自動車会社が市場を席巻しており、他地域の企業にとっては厳しい競争環境に見えるかもしれない。しかしながら、欧米の自動車会社が持っていた弱点をうまく利用し、競争を優位に進めたのは日本の自動車会社である（Jobber and Ellis-CHadwick, 2020）。つまり、競争の状態に関する質的な側面も含めてセグメントの魅力を評価すべることが重要となる。 政治・社会・環境要因の例として、近年のジェンダーや性役割に関する消費者の認識の変化が挙げられる。このような変化は、新たなセグメントの成長性へ影響を与えるかもしれない。伝統的な価値観のもとでは、女性が育児の中心を担うと思われる傾向があったが、それとは異なる価値観が台頭することによって、育児に関する製品群において男性も魅力的なセグメントになるかもしれない。例えば、自転車小売業者のサイクルベースあさひは、直線的で無骨なデザインながら、チャイルドシートや荷台を追加しやすい機能を有した「88サイクル（ハチハチサイクル）」という製品を販売している。あさひはこの製品を「パパチャリ」としてフレーミングし、家庭内での役割を果たす男性のに向けた自転車としての市場を捉えようと試みている。 セグメントの選択においては、セグメントの魅力だけでなく、自社の能力も考慮する必要がある。どれだけ魅力的なセグメントがあったとしても、そのセグメントのニーズに応えるための能力がなければ、そのセグメントを狙うことは適切ではない。また、自社能力を評価するときには、既存もしくは潜在的な競合との比較のもとで相対的に能力を評価することが大切になる。例えば、自社として製品品質に自信があったとしても、同程度の価格帯でより品質の高い製品を製造できる競合がいた場合には、相対的に能力は低いことになる。 ターゲットを決めた後企業は、製品や提供物を決定し、市場において特有のポジションを専有しようと試みる。この段階をポジショニングという。ポジショニング段階にある企業は、競争的な優位性を得ることに焦点を合わせるのだが、この時消費者に自社製品が優れていると認識されるような情報伝達も必要になる。ポジショニングと消費者の認識との関係については 12 章にて説明を行う。 次節では、定量的なデータを用いたセグメントの探索・発見方法としてのクラスター分析を紹介する。なお、本資料においてはクラスター分析の概要とRを用いた基礎的な分析方法に着目する。そのため、理論的背景や発展的手法については他の資料や著書を参照してほしい。 "],["セグメンテーションの実行とクラスター分析.html", "11.4 セグメンテーションの実行とクラスター分析", " 11.4 セグメンテーションの実行とクラスター分析 セグメンテーションでは、基準となる情報を選定し、その情報を用いて消費者の類型を発見する。その発見の方法として照井・佐藤（2023）では、（1） 経験、（2）クラスタリング、（3）潜在クラス、の3つの方法を紹介している。経験によるセグメンテーションでは、マーケティング担当者の経験や既存のリサーチによる知見を参考に消費者を類型する。この方法は、十分な知識や経験が蓄積されており、基準となる情報が限定的かつニーズと関連的である場合には有効になる。例えば、消費者の年齢がニーズと関連していることがわかっている場合、年齢に基づくセグメンテーションは特別な分析を介さずとも有効な手法となりうる。しかしながら、経験的な知見が不足している場合には、セグメンテーションの信頼性を損なうことに加えて、考慮すべき情報が複数ある場合には、情報処理が複雑になり、類型化が困難になる。 クラスタリングによるセグメンテーションは、クラスター分析を用いたセグメンテーションである。基準となる情報を（多くの場合複数）選択し、その情報をもとに各観測の類似性を求めることで、類型化を行う。この方法は、多くの情報をセグメンテーションの基準として用いることができる点や、分析方法についての資料やソフトウェアが充実しているため、分析の実現可能性が高いという利点を持つ。本資料ではこのクラスター分析を中心に議論を進める。第三の潜在クラスによるセグメンテーションでは、潜在クラスモデルと呼ばれる統計モデルを用いた方法である。これは発展的な手法であり、セグメントに関する統計的推測も行えるという利点を持っている。また、この方法では消費者が確率的に複数のグループに属することも許容するため、より現実的な手法とも評価されている（照井・佐藤,2023）。しかしながら、この方法には相対的に多くのデータを必要とし、発展的な統計的知識が必要になる。本講義においては潜在クラスモデルを用いた手法は扱わない。 本資料では、消費者や顧客のセグメントを発見するための方法としてクラスター分析を紹介する。クラスター分析は、2つ以上の基準となる情報（変数）に基づいて、対象または人を相互に排他的で網羅的なグループに分類するために使用される統計的手法である。クラスター分析にはいくつかのアプローチが存在するが、それらに共通するのはサンプル間の類似性を確認し、グループとして分割していくというプロセスを有しているということである。ここでは主に階層的クラスター分析と、被階層的クラスター分析を紹介する。階層的クラスター分析は類似する観測同士を段階的にまとめていき、グループ（クラスター）を形成していく方法である。一方で非階層的クラスター分析は、分析者の定めた前提のもと、非階層的にクラスターを形成する方法である。本資料ではおもに、これら2つのアプローチについて紹介する。 11.4.1 階層的クラスター分析 本節では、階層的クラスター分析について説明する。階層的クラスター分析では、データの中から類似している観測値を段階的にクラスターとしてまとめていき、最終的にすべてのデータが1つのクラスターになるまでそれを繰り返す方法である。 このプロセスにおいて類似度は観測値同士の距離として測られる。距離の測定方法には色々とあるが、ここではユークリッド距離とマンハッタン距離を紹介する。ユークリッド距離は、観測値同士の各座標の差の二乗和の平方根であり、マンハッタン距離は観測値同士の各座標の差の絶対値の合計であるといえる。例えば、以下の図11.2 における2点をつなぐ赤い線がユークリッド距離、青い線がマンハッタン距離だといえる。以降では、実際の分析においても頻繁に用いられるユークリッド距離に主に焦点を合わせ、手法を紹介する。 Figure 11.2: 距離定義概要 階層的クラスター分析では、図11.4 で示されているデンドログラム（樹形図）を得ることで、各観測が段階的に集約されていく様子が可視化される。例えば、図 11.4の左（1）では、6個の観測が3つのクラスターにまとめ・分類されている一方で、右（2）では、2つのクラスターに集約されている事がわかる。これらの図を見ると、aはbと最も近く、cはdと最も近いことがうかがえる。また、aとbで構成されたクラスターは、cdクラスターとは近いが、efとは遠いことがうかがえる。デンドログラムの解釈方法については後述するが、まずはデンドログラムを得るプロセスについて説明する。 ここでは、以下の表11.1 で示されている、2つの変数（x, y）に関する6個のデータが与えられた場合を考える。この場合の各観測値は図 11.3のように示される。 Table 11.1: データ例 x y a 1 2 b 2 3 c 2 5 d 3 5 e 7 2 f 6 3 Figure 11.3: 階層的クラスター分析 この時、各データ同士の類似度をユークリッド距離で測るとする。例えば、a と最も近いデータは b である。a と b の距離は、以下のように求まる。 \\[ \\sqrt{(2-3)^2+(1-2)^2}=\\sqrt{2}=1.414 \\] 同様に、c（e）と最も近いデータは d（f）であり、その類似度もユークリッド距離で求めることができる。これによって、各観測点から最も近い観測点同士を結びつける形で、3つのクラスター（図11.4 左に対応）が初期段階のものとして形成される。次に、まとめた3つのクラスターと、他の観測点もしくは他クラスターとの距離を計算し、より大きな（多くの観測点が含まれる）クラスターを形成する。例えば、クラスター（a, b）は、（e, f）よりも（c, d）のほうに近いため、図11.3 に示されているように、（a, b, c, d）というクラスターとしてまとめる（合併する）ことができる（図11.4 右に対応）。このように段階的にデータをまとめていくと、最終的にはすべてのデータを一つのクラスターとしてまとめることができる。これが、階層的クラスター分析の直感的プロセスである。 Figure 11.4: デンドログラム 図 11.4 のようなデンドログラムでは、一番下に全観測値が表示される。横軸と平行の線は、クラスタとしての併合を意味しており、下でのつながりほど初期に併合されたクラスタであることを示す。そのため、最終的には（一番上では）すべてが一つのクラスタにまとまっていることがうかがえる。この時、デンドログラムの高さ（縦軸）は距離を示している。したがってデンドログラムは、どの程度の離れ具合を許容するかによって何組でデータをまとめられるかが変わることを表している。例えば図11.4 では、高さを2に定めれば3つのクラスターにまとめることができ、4を設定すれば2つのクラスターにまとめることができる。 階層的クラスター分析ではクラスター同士を段階的に合併させていくのだが、クラスター同士の類似性を測るための方法もいくつか存在する。ここでは、代表的なものをいくつか紹介する。クラスター同士の距離の決め方として、図 11.5 に示すように、最短距離法（Single linkage method）、再遠距離法（Complete linkage method）群平均法（Group average method）がある。最短（遠）距離法はクラスタ間の最も近い（遠い）観測の組み合わせの距離を測るものである。一方で群平均法は、クラスタ間のすべての観察の組についての距離を計算する方法である。 また、非常に頻繁に使われる手法としてウォード法も存在する。ウォード法は最小分散法とも呼ばれ、クラスター内の平均までの二乗距離を最小化する方法である。 Figure 11.5: クラスター類似性測定手法 ウォード方では、異なるクラスター間の類似性について、まずそれらを構成する観測値を一括にして捉えた仮のクラスターを形成し、そのまとめられたクラスター内の観測値同士の距離が近い（分散が小さい）ものから併合される。図 11.6ではその直感的な概略図を示している。ウォード法ではこのようにクラスター間の類似性を分散の形式で測定することで段階的に併合するグループを決定していく。 Figure 11.6: クラスター類似性測定手法 階層的クラスター分析におけるクラスター数の決定は、分析者の判断に依存し、絶対的な基準は存在しない。しかしながら、多くの場合、分類の「効率性」と「有効性」のバランスから効果的なクラスター数が決定される。効率性は分類によってどれだけ多くの情報を説明できているかを表しており、より少ないクラスター数で多くのデータを説明できたほうが情報の集約による効果が大きいと考えられる。例えば、表 11.1 で示されているような6つのデータを6つのクラスターで説明しても、クラスター分析としては不適切だといえる。一方で、得たクラスター分類がどの程度現実的に含意のある分類をできているか、を捉えたのが有効性である。効率性を意識しすぎて少ないクラスター数による分類を採用しても、あまりに大雑把過ぎる分類だと分析結果が有益にならない。例えば、図 11.3 において、1つのクラスターで6個のデータを説明するよりも、2つか3つのクラスターで説明したほうがより直感的かつ実務的な含意を得ることができるかもしれない。このように、階層的クラスター分析は、クラスターがどのように形成されていくかの段階を示すことによって、おおよそどのようなクラスター分類を行うことが良さそうかを判断することにつながる。 "],["k-means.html", "11.5 非階層的クラスター分析：K-means法", " 11.5 非階層的クラスター分析：K-means法 非階層的クラスター分析では、段階的なクラスターの分類ではなく、事前に決められたクラスター数に基づき、データを分類する。本資料では代表的な非階層的クラスタリング法である、K-means法について紹介する。K-means法の分析では、分析者が事前にクラスター数を決める必要がある。その仮定の下、Sequential threshold method と言われる以下のようなプロセスによってクラスターが決定される。 初期クラスターの形成 クラスターセンターのアップデートと更新 アップデートの収束と最終クラスターの決定 図 11.7と11.8 はK-means法のプロセスに関する直感的説明を段階的に図示化したものである。ここでは、図 11.7 (1) のようなデータに対し、3つのクラスター数を仮定した場合を考える（このプロセスは任意の \\(k\\) 個のクラスターに対して適応可能）。 Figure 11.7: K-means法直感１ まず初めに図11.7 (2) に着目してほしい。ここでは、与えられたデータの空間に対して3つのクラスターセンター初期値（初期シードとも言う）をランダムにとる。次に各観測個体をそれぞれの最も近い初期センターに割り当てる形で3つのクラスターを作成する（図11.7 (3)）。ここで、作成された3つのクラスター（の観測値）と、初期値とを比較し、適切に中心を表しているかを検討する（図11.7 (4)）。例えば、図11.7 (4) における赤いクラスターについてはもう少し右上の方向にセンターを移動したほうがいいかもしれない。このように、各クラスターの観測値に基づき、クラスターセンターが計算され、アップデートされる。 Figure 11.8: K-means法直感２ しかしながら、クラスターセンターがアップデートされると、新たなセンターから各観測個体までの距離も変化する。例えば、図11.8 (5) を見ると、これまで赤クラスターに属していた個体のひとつが、クラスターセンターのアップデートに伴って赤のセンターよりも青のセンターから近くなっている。そのため、この個体は青クラスターに分類される（図11.8 (6)）。各観測個体のクラスターへの分類が変更されたため、クラスターセンターも当然修正されるべきであり、新たな分類に基づくクラスターセンターが再度計算される（図11.8 (7)）。K-means方においては、このようなプロセスをクラスターセンターが動かなくなるまで繰り返すことで、観測個体を分類する。 K-means法によるクラスターセンターの計算等については、ソフトウェアが行ってくれる。しかしながら、分析者は分析実行前にクラスター数の決定を行わなければならない。クラスター数の決定においては、11.4.1節でも述べた通り、分析者の恣意性を含む判断によって決定される。クラスター数の決定においては、伝統的には（1）ハーティガンルールと、（2）エルボー法という方法のいずれかもしくはその両方を用いるｋとが多い。 ハーティガンルールとは、\\(k\\) 個と \\(k+1\\) 個のクラスターにおける内部平方和（クラスター内部の分散）を比較する変量を計算し、その値が10を越えた場合は、\\(k+1\\)個のクラスター数を採用したほうが良いという考え方である。この値が10を越える状況は、\\(k+1\\)個の場合に比べ\\(k\\)個のクラスターを採用したほうが内部平方和（クラスター内部の分散）が大きくなることを意味している。 一方でエルボー法とは、\\(k\\) 個のクラスター数それぞれに対応する内部平方和をプロットすることで、好ましいクラスター数を解釈する方法である。図11.9に示すように、クラスター数の変化に伴い、内部平方和（クラスター内部の分散）がどのように変化するかをチェックすることができる。この図を用いた判断では、変化量（傾き）の変化に着目することも多い。変化量が小さい、つまりクラスター数が増えてもあまり内部平方和が減らない場合、効率性と有効性の観点からより少ないクラスター数を選ぶことが判断されやすい。例えば図11.9 の場合、4つのクラスターまではクラスター数の増加とともに内部平方和が下がっているものの、4を越えてからは傾きがほぼ水平になっていることがうかがえる。そのため、このような結果の場合には、4クラスターを仮定した非階層的クラスター分析を実行することが多い。 Figure 11.9: エルボー法例 ここで、セグメントの発見を目的とするような探索的なクラスター分析に関する手順を紹介する。非階層的クラスター分析は、階層的クラスター分析との組み合わせる形で実行されることも多い。顧客セグメントを探索するような目的で実行されるクラスター分析として、以下のような手順を紹介する： 階層クラスター分析、ハーティガン、エルボーの確認 クラスター数 (k) を決定 k個のクラスター数に対する K-means クラスター分析の実行 \\(\\rightarrow\\) クラスターの図示化と記述による検討 クラスターのプロファイル情報を整理し、各クラスターを評価 次節では、この4つのプロセスに基づき、Rを使ったクラスター分析方法を紹介する。 "],["rによるクラスター分析の実行.html", "11.6 Rによるクラスター分析の実行", " 11.6 Rによるクラスター分析の実行 ここからは、Rを用いたクラスター分析の実行方法について紹介する。なお、本節では、吉田秀雄記念事業財団によって2023年に実施された消費者調査アンケートデータ、https://www.yhmf.jp/aid/data/data_aid_2023_later.html のうち、（簡単化のために）特定の変数と回答者（兵庫と東京在住者のみ）を抽出して利用する39。ここでは、以下のリストにある変数を活用する。 q12_4（ブランドロイヤリティ性向）：「たとえ多くのブランドを利用できる状況にあっても、何時も同じブランドを選ぶ。」 q13_3（価格感度）：「大抵、一番安いものを買う。」 性別 年齢 結婚有無 q5: 職業 q7_2: 世帯年収 なお、q12_4とq13_3はどちらも5点リッカート尺度で回答を得ている。その他消費者属性情報の回答項目の詳細はエクセルファイルを参照してほしい。上記の条件に沿うデータは以下のように抽出することができ、その結果、1973件の回答を得た。 df_cons &lt;- readxl::read_xlsx(&quot;data/回答データ【消費者調査2023年度下期調査】.xlsx&quot;, sheet = &quot;回答データ【共通調査2023年度下期】&quot;,na = &quot; &quot;) library(tidyverse) #東京と兵庫の県番号リスト作成 list &lt;- c(13, 28) #回答者と項目を抽出 df_cons &lt;- df_cons %&gt;% select(県番号, q12_4, q13_3, 性別, 年齢, 結婚有無, q5, q7_2) %&gt;% filter(県番号 %in% list, q12_4 != 999, q13_3 != 999) %&gt;% mutate(Pref = ifelse(県番号 == 13, &quot;Tokyo&quot;, &quot;Hyogo&quot;), Gender = case_when(性別 == 1 ~ &quot;Male&quot;, 性別 == 2 ~ &quot;Female&quot;, TRUE ~ &quot;Others&quot;), MaritalSt. = case_when(結婚有無 == 1 ~ &quot;Married&quot;, 結婚有無 == 2 ~ &quot;Not Married&quot;, TRUE ~ &quot;Others&quot;)) 本節でのクラスター分析の実行にあたっては、以下のパッケージをインストールし利用する。なお、クラスター分析自体は cluster パッケージで実行可能だが、factoextra を用いるともう少し洗練された可視化が可能になるため、こちらの紹介も行う。 install.packages(c(&quot;cluster&quot;, &quot;factoextra&quot;,&quot;ggrepel&quot;, &quot;useful&quot;)) library(cluster) library(factoextra) library(ggrepel) library(useful) 本節で我々はdf_consを用いて分析を行いたいのだが、クラスタリングの関数はデータセットに含まれているすべての変量間の類似性・相関を計算してしまうため、必要な変数のみを抽出し、分析に利用する。なお、クラスタリングの関数は文字列にも対応していないため、もしデータセットにそのような変数が含まれている場合には、この段階で取り除く必要がある。 ここではブランドスイッチ（q12_4）と価格志向（q13_3）に関する変数の抽出と、クラスター分析の実行を行うために、以下の手順を経る： 階層的クラスター分析を実行し、デンドログラムを確認 デンドログラムとエルボー法によりクラスター数を決定 2.で決定したクラスター数に従い、K-means法を実行 クラスター情報と元データ（df_cons）を結合し、プロファイル情報の整理と検討 まずは以下の通り、階層的クラスター分析を実行する。なお、階層的クラスター分析の実行においては、agnes() 関数を用いる。その際に用いる距離の計測方法は metric = という引数で設定できる。その後、階層的クラスター分析の結果を用いて pltree() 関数を実行することで、デンドログラムが出力される。 clus_cons &lt;- df_cons %&gt;% select(q12_4, q13_3) Hier1 &lt;- agnes(clus_cons, metric = &quot;euclidian&quot;, method = &quot;ward&quot;, stand = TRUE) pltree(Hier1) Figure 11.10: 消費者デンドログラム 図11.10 の通り、本データは1973件の観測個体を有するため、デンドログラムも下部については識別が難しくなっている。しかしながら、例えば高さを40ぐらいに設定した場合、クラスターは4つか5つに分かれることがうかがえる。例えば4クラスターの場合、どのような分け方になるかについて、直感的に示すために、factoextra パッケージを用いると図11.11のようにデンドログラムを出力できる。この時、fviz_dend() 関数における k = 4 という引数で、ここでのクラスター数を定義している。 alt_Hier &lt;- clus_cons %&gt;% dist(&quot;euclidian&quot;) %&gt;% hclust(&quot;ward.D&quot;) alt_Hier %&gt;% fviz_dend(k = 4, rect = TRUE, rect_border = TRUE) Figure 11.11: 4クラスターデンドログラム 続いて、上記の結果を踏まえ、エルボー法を実施する。 fviz_nbclust(clus_cons, kmeans, method = &quot;wss&quot;) Figure 11.12: エルボー法結果 図 11.12 を確認すると、4もしくは5クラスターで傾きが小さくなっているように見える。ちなみに、5クラスターを採用したケースは、デンドログラムでは図11.13のように示すことができる。 alt_Hier %&gt;% fviz_dend(k = 5, rect = TRUE, rect_border = TRUE) Figure 11.13: 5クラスターデンドログラム 4クラスターモデルと5クラスターモデルのどちらがいいのかについての判断は難しいが、ここでは便宜的に5つのクラスターを仮定する。K-means法の実施には、clusterパッケージの kmeans()関数を用いる。なお、初期クラスターセンターの割り振りはランダムで行うため、set.seed() を用いる。set.seed()のカッコの中に数値を入力することで、その数値を使う場合には常に同じ乱数を発生させることが可能になり、初期値の固定と再現性の確保ができる。 set.seed(343) K_shopping &lt;- kmeans(clus_cons,5) K_shopping ## K-means clustering with 5 clusters of sizes 80, 298, 760, 592, 243 ## ## Cluster means: ## q12_4 q13_3 ## 1 5.000000 2.150000 ## 2 3.261745 1.781879 ## 3 2.582895 3.207895 ## 4 3.447635 4.123311 ## 5 1.699588 1.777778 ## ## Clustering vector: ## [1] 5 5 5 4 3 3 3 4 2 3 3 1 3 3 3 3 4 3 3 4 2 4 5 3 2 4 3 4 5 4 3 3 3 4 3 4 3 ## [38] 4 3 3 4 5 4 3 3 4 3 3 4 2 4 2 2 4 4 2 3 3 3 3 4 3 4 3 2 1 2 1 3 4 3 4 2 4 ## [75] 2 3 5 3 4 4 4 3 4 3 2 4 3 2 3 3 3 3 4 2 3 3 3 4 4 3 3 3 4 3 3 3 4 4 3 1 3 ## [112] 3 3 3 4 4 3 4 5 4 4 5 3 3 4 3 4 4 4 3 3 3 4 1 2 4 3 2 3 1 4 3 2 4 3 3 4 4 ## [149] 3 2 5 2 4 2 3 3 3 4 3 3 4 3 4 4 3 4 4 3 3 4 4 3 2 4 5 3 2 4 3 4 3 3 3 3 3 ## [186] 4 3 3 3 3 3 2 4 3 3 2 4 4 4 3 2 3 3 4 4 1 3 3 2 4 3 5 3 3 3 4 4 5 3 2 1 4 ## [223] 5 3 5 3 4 4 5 3 4 5 3 4 3 3 4 4 5 3 3 3 5 4 3 4 2 3 2 4 3 4 3 3 4 5 4 2 3 ## [260] 3 3 3 5 2 5 4 3 2 3 5 1 4 2 3 4 3 4 3 4 3 2 1 3 3 3 2 3 4 4 2 3 4 3 4 4 3 ## [297] 3 3 2 4 3 5 4 2 2 4 4 4 4 5 3 1 3 4 4 4 3 3 5 4 5 4 4 3 2 2 4 4 5 4 3 4 5 ## [334] 3 3 4 4 3 3 3 2 1 4 3 4 3 4 3 5 5 2 3 1 2 1 3 3 3 4 2 3 4 4 2 3 3 2 2 3 3 ## [371] 4 3 3 3 3 3 3 5 3 3 4 4 3 4 3 2 4 3 4 5 3 2 4 2 3 3 4 3 3 3 3 4 4 4 1 4 3 ## [408] 4 3 2 3 3 3 3 3 4 3 1 2 4 2 5 2 3 2 3 2 4 2 3 2 3 4 3 3 3 3 3 2 3 4 3 3 3 ## [445] 3 3 3 5 3 3 3 1 3 3 4 4 3 2 3 5 3 4 1 5 4 4 3 2 3 3 4 3 5 4 3 4 4 3 3 4 5 ## [482] 4 3 4 2 3 5 4 4 4 3 4 3 4 3 3 4 5 5 3 3 3 5 3 3 1 5 3 4 3 4 5 3 2 2 5 5 3 ## [519] 2 3 3 4 2 4 3 3 3 2 5 2 4 5 5 3 3 5 3 4 2 3 3 3 3 3 3 4 4 1 4 3 3 4 2 5 4 ## [556] 3 4 3 3 2 5 4 5 3 3 4 5 4 3 2 2 4 4 3 2 4 2 3 2 3 3 2 3 3 3 4 5 3 4 3 3 4 ## [593] 2 4 3 3 4 3 4 3 4 3 3 4 3 2 2 3 3 3 2 4 3 4 2 5 5 3 5 3 3 5 3 4 3 1 3 2 3 ## [630] 3 5 1 4 3 5 3 2 3 2 4 5 2 3 1 4 4 3 2 4 3 4 3 4 3 2 4 3 4 4 3 4 3 5 4 3 3 ## [667] 3 3 3 5 2 3 2 3 4 4 3 5 3 3 4 1 3 4 1 3 5 3 5 4 5 3 3 4 2 3 3 5 2 3 3 3 4 ## [704] 3 3 4 3 3 3 3 3 5 4 1 2 3 2 4 2 2 3 5 3 3 3 5 3 4 3 5 3 2 3 4 4 5 4 3 1 3 ## [741] 4 3 5 4 3 1 3 3 4 4 3 5 4 3 4 4 4 2 3 4 3 5 3 3 3 4 4 3 1 3 5 2 4 1 2 3 2 ## [778] 3 2 3 4 3 5 5 4 3 4 4 3 2 3 3 3 1 3 3 3 3 2 4 4 3 3 2 3 4 4 3 3 4 3 5 4 3 ## [815] 4 3 4 1 4 1 4 3 4 5 2 4 2 3 4 3 2 5 4 3 4 4 3 4 2 5 3 2 2 4 3 4 4 4 4 3 5 ## [852] 3 2 4 5 4 4 5 2 2 3 1 3 4 5 5 3 2 3 3 5 2 4 3 3 2 4 2 3 1 3 2 2 2 3 2 5 1 ## [889] 4 4 5 5 5 4 2 5 2 3 4 1 4 3 5 5 3 2 4 4 4 3 2 2 4 3 3 4 3 3 3 3 3 2 3 2 3 ## [926] 5 5 4 3 4 3 4 4 3 1 2 4 5 4 3 5 1 2 4 5 4 4 5 3 3 4 3 4 4 3 4 2 4 2 4 5 4 ## [963] 3 2 4 5 4 2 4 2 3 4 3 4 3 2 4 2 3 2 3 4 2 4 2 3 3 3 2 2 4 1 3 4 1 4 3 4 4 ## [1000] 3 5 4 2 5 4 5 2 3 3 3 4 3 4 3 3 4 3 3 4 4 2 3 3 5 3 2 5 3 3 3 3 4 3 4 4 5 ## [1037] 2 4 3 4 2 3 2 2 4 3 3 3 4 4 2 5 3 3 3 4 3 3 5 4 4 4 2 3 4 5 1 4 3 1 2 5 4 ## [1074] 4 3 4 5 4 4 3 4 3 3 4 4 4 3 3 3 4 2 4 2 4 3 2 4 4 3 3 2 3 5 5 4 3 3 5 3 4 ## [1111] 2 2 3 4 5 3 4 5 4 4 3 3 3 3 3 3 4 4 3 3 4 4 3 3 4 3 2 4 5 5 5 5 2 4 3 4 3 ## [1148] 4 4 2 5 4 3 2 2 3 2 4 3 3 4 4 5 3 2 2 3 4 4 2 4 5 3 5 1 4 3 2 5 3 4 4 3 3 ## [1185] 4 1 5 2 4 5 3 3 4 4 3 4 3 5 5 2 3 2 3 4 4 3 2 4 4 3 2 4 3 3 3 2 5 4 1 3 5 ## [1222] 4 4 3 3 3 3 4 4 2 3 4 2 5 5 4 5 4 5 2 5 2 5 4 3 3 4 3 5 3 4 4 3 4 4 4 3 3 ## [1259] 2 4 2 3 4 4 4 3 4 2 4 2 4 3 4 3 2 2 4 5 5 4 3 3 3 4 4 2 3 3 4 4 3 3 5 1 4 ## [1296] 3 4 3 2 3 4 3 3 4 5 3 3 4 3 4 3 2 4 4 3 3 3 5 3 4 3 5 4 4 3 4 3 5 1 4 3 1 ## [1333] 2 4 4 5 3 4 4 3 3 3 4 3 3 5 4 3 1 3 3 5 3 3 3 4 2 3 5 3 2 4 3 4 1 3 4 1 5 ## [1370] 3 4 3 3 3 4 3 3 5 4 5 4 3 4 2 4 5 2 4 3 3 5 5 3 2 4 5 4 4 3 3 2 3 2 2 3 3 ## [1407] 4 4 5 3 4 2 3 2 3 1 2 1 2 3 2 3 3 3 4 4 1 4 2 5 3 5 2 4 4 3 4 4 4 5 3 2 3 ## [1444] 1 5 3 4 3 3 3 2 4 3 4 4 3 4 3 2 4 3 4 2 4 3 3 5 4 3 5 4 2 5 4 4 2 3 2 3 3 ## [1481] 3 2 5 5 5 4 4 1 3 4 3 4 3 2 5 3 4 3 3 5 1 4 4 4 3 3 3 3 4 3 4 3 4 2 3 3 3 ## [1518] 3 5 4 5 5 3 2 4 3 4 4 3 3 4 3 4 3 3 2 4 4 4 5 2 4 4 1 3 3 3 5 3 3 5 3 3 3 ## [1555] 3 5 4 3 2 2 4 3 3 2 3 3 3 3 3 4 4 5 5 4 2 4 3 2 3 4 4 3 1 4 2 1 5 4 2 4 3 ## [1592] 3 2 3 3 4 5 3 2 2 4 1 3 2 5 3 3 3 2 4 5 5 5 4 3 5 5 4 2 4 3 5 3 1 4 5 3 2 ## [1629] 4 5 5 3 2 3 3 4 1 3 5 2 3 3 2 4 3 2 3 2 3 5 3 2 2 5 3 2 4 4 4 3 4 2 4 2 3 ## [1666] 2 4 2 5 5 1 3 4 2 3 5 3 3 5 5 3 4 4 2 2 4 2 4 3 4 4 4 3 3 2 2 2 4 5 2 3 3 ## [1703] 5 1 3 4 5 2 3 5 5 3 2 2 4 3 3 5 3 4 4 3 3 5 3 4 4 4 3 3 4 4 5 4 4 3 3 3 3 ## [1740] 4 3 3 4 2 2 5 3 4 4 2 3 4 4 2 4 3 3 2 3 2 4 2 4 4 4 4 5 2 2 3 4 3 2 4 2 4 ## [1777] 4 4 4 4 3 2 4 4 3 4 4 5 4 5 2 2 2 2 3 5 4 5 2 2 1 4 4 2 4 1 3 5 2 4 5 3 1 ## [1814] 3 5 4 2 3 3 3 3 2 4 3 5 3 5 3 4 1 5 5 2 3 4 2 5 4 3 5 4 5 3 4 3 4 5 3 4 4 ## [1851] 4 3 3 5 1 2 5 3 3 1 2 4 3 4 3 3 4 4 1 5 3 3 3 4 1 2 3 5 5 2 4 3 2 4 4 2 3 ## [1888] 2 4 5 4 3 2 3 5 3 1 3 2 3 1 2 3 2 4 3 3 3 3 4 4 3 4 4 3 4 2 4 2 3 3 5 3 4 ## [1925] 5 3 4 3 4 4 5 4 1 4 3 3 3 2 2 5 1 4 4 3 3 3 4 4 4 4 2 4 3 4 1 4 5 2 4 3 5 ## [1962] 1 3 3 3 4 5 2 3 3 4 3 3 ## ## Within cluster sum of squares by cluster: ## [1] 54.2000 108.4060 401.9303 622.3750 127.0700 ## (between_SS / total_SS = 66.7 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; 分析の結果表示される Cluster means:では、分析に用いた変数に関する各クラスターごとの平均値が出力される。例えば、クラスター1は、ブランドスイッチ（q12_4）については高いが、価格志向（q13_3）については低いことがうかがえる。結果の下部で表示される、Within cluster sum of squares by cluster:には、各クラスターの内部平方和（分散）が出力されている。その下の between_SS / total_SS は、全体の平方和に占めるクラスター間平方和の比率であり、全体の分散の何%を5つのクラスターが説明しているかを示している。 以下では、上述の結果について、factoextra::fviz_cluster()によって図示化する。図11.14 では、ブランドロイヤリティと価格志向のどちらも高い（低い）グループや、どちらか一方のみ高いグループに加え、価格志向は低いがブランドスイッチについては平均的というグループも確認できた。このように図示化することで、クラスター分析の結果についての解釈が容易になる。なお、4クラスターを採用した場合の結果については、11.15で示されている。また、4クラスターでの分析を実行すると、between_SS / total_SS = 65.6 % という結果を得るはずなので、興味のある読者は自身で実行してみてほしい。 fviz_cluster(K_shopping, data = clus_cons, geom = &quot;point&quot;) + labs(title = &quot;k = 5&quot;, x = &quot;Brand switching (Std. score)&quot;, y = &quot;Price orientation (Std. score)&quot;) Figure 11.14: 消費者価値観クラスター Figure 11.15: 4クラスターモデル図 続いて、各クラスターに属する消費者についての情報を整理、検討する。ここでは、5クラスターモデルの結果に基づき、クラスター情報と元データとを結合する。結合した結果として以下の表 11.2 に、元データにクラスター番号に関する変数 cluster_id が追加されていることがわかる。 df_cons$cluster_id &lt;- factor(K_shopping$cluster) knitr::kable(head(df_cons),caption = &quot;結合データ&quot;) Table 11.2: 結合データ 県番号 q12_4 q13_3 性別 年齢 結婚有無 q5 q7_2 Pref Gender MaritalSt. cluster_id 13 2 2 2 38 2 8 4 Tokyo Female Not Married 5 13 2 1 1 42 2 2 9 Tokyo Male Not Married 5 13 1 2 1 30 1 3 NA Tokyo Male Married 5 13 4 5 1 33 1 3 NA Tokyo Male Married 4 28 3 3 1 25 1 3 10 Hyogo Male Married 3 28 3 3 1 32 1 3 NA Hyogo Male Married 3 以下では、各クラスターの平均的な消費者像について理解するために、個人属性情報をまとめる。その作業方法と結果は、以下のコードと表11.3 のとおりである。ここでは、表11.3に基づき、いくつかのクラスターに絞り、それらの特徴を整理し解釈を行う。なお、本データでは東京在住者の観測数（1465）が兵庫在住者の観測数（508）を大きく上回っているため、居住エリアの比率については本データの比率（\\(1465/1973\\approx 0.742\\)）を基準に、この比率と同等の東京在住者がいれば1を、それより多ければ1より大きい値を取るような比率で示している。 表11.3におけるクラスター１は最も人数が少なく、価格志向は高くスイッチはやや低いことがうかがえる。そのため、このグループに属する消費者は、価格に基づいて選んだブランドを買い続ける傾向があるのかもしれない。また、このクラスターは他のクラスターよりも東京在住者の比率が高いこともうかがえる。クラスター6は、価格志向もスイッチもどちらも低いことがうかがえる。そのため、このクラスターに属する消費者は価格以外の属性に基づいて選んだブランドを買い続ける傾向があるかもしれない。また、このクラスターは既婚者率が高く、このような属性の特徴も価値観に影響しているのかもしれない。他のクラスターに関する解釈はここでは割愛するが、ぜひ読者においてもそれぞれのクラスターについての解釈を展開してみてほしい。 clus_summary &lt;- df_cons %&gt;% group_by(cluster_id) %&gt;% summarize(N = n(), Loyalty_m = mean(q12_4), Price_m = mean(q13_3), Age_m = mean(年齢), Male_r = sum(Gender == &quot;Male&quot;)/n(), Tokyo_r = (sum(Pref == &quot;Tokyo&quot;)/n())/(1465/1973), Married_r = sum(MaritalSt. == &quot;Married&quot;)/n()) knitr::kable(clus_summary, caption = &quot;クラスターサマリー&quot;) Table 11.3: クラスターサマリー cluster_id N Loyalty_m Price_m Age_m Male_r Tokyo_r Married_r 1 80 5.000000 2.150000 42.61250 0.5125000 0.8753925 0.5250000 2 298 3.261745 1.781879 38.56040 0.5134228 0.9400188 0.6107383 3 760 2.582895 3.207895 41.88816 0.5065789 1.0100683 0.5618421 4 592 3.447635 4.123311 43.79730 0.4645270 1.0191680 0.4712838 5 243 1.699588 1.777778 34.91770 0.5102881 1.0363938 0.6213992 本節では、実際の消費者アンケートデータを用いてクラスター分析の実行手順を紹介した。クラスター分析を用いて探索的にセグメントを発見するためには、階層的クラスター分析と非階層的クラスター分析を組み合わせることが重要となる。ただし、クラスター分析ではクラスター数の決定や結果の解釈などにおいて、分析者の恣意性に依存することになる。しかしながら、このような限界も理解した上でうまく利用すれば、有益なセグメントを発見することにもつながりうる。 本節では観測数2000件弱のデータを利用したが、実際に我々がこれだけのデータを目視し、セグメントを発見することは困難である。クラスター分析は人間では処理困難な情報量を集約し、解釈可能にしてくれるという強みを持つ。もちろんもっと多量のデータを用いてクラスター分析を実行することも可能であるし、機械学習への応用や潜在クラスモデルの利用など、より発展的な手法も展開されているため、興味・関心のある学生においてはさらなる学習を進めてほしい。 なお、データの読み込みエラーを防ぐために、自由回答設問項目は削除している。↩︎ "],["練習問題-7.html", "11.7 練習問題", " 11.7 練習問題 前節で活用した 回答データ【消費者調査2023年度下期調査】.xlsx に含まれる別の変数を活用し、データの探索およびクラスター分析を実行しよう。そして、（1）データ・変数の概要、（2）実行した分析アプローチ、（3）結果（図を含む）、（4）結果に基づく実務的助言、をまとめた短いサマリーを書いてみよう。その際、どんな分析を実施し、どんな結果を得たのか、明確かつ簡潔に書くことをこころがけよう。 "],["参考文献-9.html", "11.8 参考文献", " 11.8 参考文献 Jobber, D., &amp; Ellis-Chadwick (2020). Principles and Practices of Marketing 9th Edition, McGraw-Hill Education. 照井伸彦・佐藤忠彦（2022）「現代マーケティング・リサーチ[新版]」,有斐閣. "],["factor.html", "Chapter 12 探索的因子分析と知覚マップ ", " Chapter 12 探索的因子分析と知覚マップ "],["本章の概要-9.html", "12.1 本章の概要", " 12.1 本章の概要 マーケティングや消費者行動研究領域では、心理尺度を用いて消費者の状態や反応を調査することが重要である。4 章では、心理尺度に関する基本的な考え方や尺度例について紹介した。しかしながら、調査を行う立場に立てば、自身が採用する心理尺度がきちんと想定している概念を捉えているのか、一つの概念を捉えるために用意された複数の項目はきちんとまとまるのか、といった疑問についてデータを用いて確認したいと考えることもあるだろう。このような目的に対応する手法が因子分析である。 本章では、探索的因子分析と呼ばれる分析手法について紹介する40。そのうえで、ここでは主に二種類の因子分析の利用方法について紹介する。第一に、因子分析の最も基本的かつ一般的な方法で、心理尺度の構成やまとまりを検討することを目的とした方法である。第二は、マーケティングにおけるポジショニングについて検討するための因子分析の応用について説明する。この方法は実務的なマーケティングリサーチ法の一環で紹介される方法である。 心理尺度を構成する際には、各項目がその背後に想定している概念をきちんと捉えているかを検討することが重要である。その検討においては、概念的な議論に加え、統計的な分析により確認することが求められる。因子分析は、尺度の全体的な構造を統計的に検討するための手法として使われる。本章は、このような因子分析の基本的特徴を踏まえつつ、マーケティングへの応用についても紹介することを目的とする。この目的を達成するために本章では以下の内容を扱う。 因子分析の概要 二因子モデル ポジショニングと知覚マップ概要 Rによる因子分析実行法紹介 知覚マップ作成 第一に、因子分析の基本的な考え方と統計学的な概要について説明する。因子分析は、観測された変数の背後に潜在的な共通員が存在することを仮定する。そして、潜在因子から観測変数の影響を捉えた構造のモデルを定式化することで、観測された変数同士のまとまりを説明する。本書では、基本的なモデルの構造と因子分析の考え方を紹介する。 第二に、複数の因子が存在する場合の因子分析法について、二因子モデルを例に取り説明する。ここでは複数の因子を想定する際に特に注意が必要となる因子軸の回転と、因子数の決定について説明する。因子分析では、解釈が容易になるよう（単純構造化するために）軸を回転させることが可能になる。ここでいう単純構造とは各変数が1つの因子だけから強い影響を受け、他の因子からの影響が0に近くなるように見える構造を意味している。軸の回転においては直交回転と斜交回転という2つのアプローチが存在する。本章では、これらのアプローチについて紹介する。 因子数の決定においては、各因子の固有値を用いた判断基準を紹介する。因子分析を用いた研究では経験的に、固有値が1以上の因子をモデルに含めるという基準を採用する事が多い。固有値は直感的には、項目何個分の情報量を因子が有しているかを意味する。これに加え、因子数と固有値との関係を図示化したスクリープロットも活用することが一般的である。これにより、因子数を増やすことによる固有値の変化量にも着目し、著しく固有値が下がっている項目については除外するなどの判断に活用される。 第三に、因子分析の応用先として本章が着目するポジショニングと知覚マップを説明する。ポジショニングとは、セグメンテーションとターゲティングの後、企業の製品や提供物を決定し、市場において特有のポジションを専有するための過程である。しかしながら自社の戦略とそれに準ずる製品属性を決定したからと言って、それによって自動的に企業のポジションが確立するわけではない。ポジションの確立には、消費者がどのように知覚するかが重要になる。そのため、消費者の知覚や評価に基づき、ポジショニングを理解する事が重要になる。 その後、演習用の架空のデータセットを用いてRを使った因子分析の実行方法について説明する。ここでは、斜交回転を用いた分析の実行例を紹介する。その後、抽出された因子のスコアを用いて回答者のクラスターを抽出する。これにより、因子分析の実行と、その結果の応用について考察する機会を提供する。 第五に、消費者に回答してもらったアンケートデータをもとに因子分析を行い知覚マップを作成する方法を紹介する。多くのテキスト等で紹介、共有されている知覚マップの作成方法やそこで利用されているデータは、すでに企業レベルで集計されている事が多い。しかしながら、各消費者が回答したアンケートデータセットと、企業レベルでのポジションを示すための知覚マップ用のデータセットではデータの構造が異なる。本章では、このようなデータ構造の変換も含めて、実践的な分析手法について紹介する。 また、本章の最後には、因子分析と比較されることの多い主成分分析について、因子分析と主成分分析と比較する形で説明する。 因子分析には、確認的因子分析（Confirmatory Factor Analysis）という手法も存在する。確認的因子分析は、探索的因子分析や先行研究などに基づき、因子の数と因子負荷量について仮説的な構造を想定し、その構造をデータに基づき検証する方法である。本書では省略するが、学術的な研究では広く採用されている手法であるため、川端ほか（2019）などを参照してほしい↩︎ "],["因子分析概要.html", "12.2 因子分析概要", " 12.2 因子分析概要 心理尺度の構成において、各項目が概念をきちんと捉えているか検討することが重要だと述べた。尺度の構成においては概念的な検討に加え統計的な分析の両面から確認することが求められる。特に、尺度の全体的な構造を統計的に検討するための手法として探索的因子分析を紹介する。因子分析は、項目間の相関関係に基づき、項目同士に共通している成分（因子）を見つけるための手法である（南風原, 2002）。そのため、この因子は4.4.1 で説明した概念に該当し、探索的因子分析は、観測された項目の相関関係は何個のどのような因子を用いることで説明できるかをデータから推測する方法である（南風原, 2002）。より具体的には、探索的因子分析においては主に（1） 因子数と（2）各因子に含まれる項目、という二側面から尺度の構成を検討する（小塩, 2024）。 12.2.1 因子分析の概要とモデルの構造 因子分析は、観測された変数の背後に潜在的な共通因子（common factor）が存在することを仮定する。そして、「潜在因子から観測変数の影響」を捉えた構造のモデルを定式化することで、観測された変数同士のまとまり（相関）を説明する方法である。因子分析は心理学分野で発展した手法であり、人々の心理的特徴（例えば、価格志向やコスモポリタニズム）を多面的に捉えることや、情報の複雑さを削減することができる。 因子分析には大きく分けて1つのアプローチが存在する。第1に、探索的因子分析である。これは、複数の観測変数間の相関関係から、その背後にいくつ潜在的な因子を導入すれば観測変数間の関係をうまく説明できるのかを探索的に調査・分析する方法である。第2に、確認的因子分析である。これは、先行研究などに基づき、因子の数と因子負荷量について仮説的な構造を想定し、その構造をデータに基づき検証する方法である。この手法は、共分散構造分析と呼ばれる分析手法を応用したものである。これら2つのうち本講義では、探索的因子分析に焦点を合わせる。 因子分析では、観測変数の値を規定するような、共通する潜在因子が存在すると考える。図12.1 は、3つの観測変数と1つの潜在因子との関係を表す因子モデルを図示したものである。因子分析では観測変数間の相関を捉えており、観測変数間に高い相関があるということは、その背後に共通する因子があると考える。図12.1 における \\(f\\) は潜在因子、 \\(x_j~(j=1,2,3)\\) は観測変数を表している。また、因子と変数の関連性は \\(a_j\\) という因子負荷量で表現され、 \\(e_j\\) は因子では説明できない変数のばらつきを表す独自因子と呼ばれる。図示化においては、一般的に観測変数は四角形、潜在因子と独自因子は円や楕円で表現される事が多い。 Figure 12.1: 因子分析モデル 図12.1のパス図は、以下のような式で表すことができる。 \\[ x_{1i}=a_1f_i+e_{1i}\\\\ x_{2i}=a_2f_i+e_{2i}\\\\ x_{3i}=a_3f_i+e_{3i}, \\] ただし、\\(i\\) は \\(1,..,n\\) の個人を指す。そのため、因子分析のモデルは、各観測変数それぞれを被説名変数とし、潜在因子を説明変数とした回帰モデルのような形で捉えることができる。このことからも、潜在因子が観測変数を規定しているという考えのもとモデルが構築されていることがうかがえる。因子分析においては、上記の式をもとに、因子負荷量を求める41のだが、ここで潜在因子を式に含めていることの問題が浮上する。説名変数として用いている潜在変数は、本来観察できないものであるため、単位や基準点が存在せず、式が変形可能になってしまい、因子負荷量についての解が定まらない（分寺、2022）。そこで、因子分析の実行においては以下のいずれかの制約をおく： 因子の分散が1、平均が0と仮定する。 1つ目の観測変数の因子負荷量を1に固定する。 このような仮定の下、因子負荷量を計算するのだが、因子負荷量の計算方法はいくつか存在する。詳細についてはぜひ分寺（2022）を参照してほしい。ここでは、分析後に算出されるいくつかの重要な指標について簡単に説明する。まずは、想定した因子モデルがどの程度うまく観測変数のばらつきを説明できているのかを表す共通性と独自性について説明する。 ここで、\\(j\\) 番目の観測変数（\\(j=1,...,J\\)）に対応する以下のような因子モデルを考える。 \\[ x_{ji}=a_jf_i+e_{ji} \\] この時、\\(\\hat{x}_{ji}\\) は \\(x_{ji}\\) の予測値、\\(\\hat{e}_{ji}\\) は 残差とすると、以下のような式を得ることができる。 \\[ x_{ij}=\\hat{x}_{ji}+\\hat{e}_{ji}=\\hat{a}_jf+\\hat{x}_{ji} \\] 上記の式において説明変数に潜在因子が使われていることには注意が必要であるが、\\(x\\) の分散と予測値や残差との関係について、回帰分析の場合と同様に、以下のように捉えることができる。 \\[ \\sum_{i=1}^n(x_{ji}-\\bar{x}_j)^2=\\sum(\\hat{x}_{ji}-\\bar{x}_j)+\\sum(x_{ji}-\\hat{x}_{ji}) \\] この時、\\(\\sum(x_{ji}-\\bar{x}_j)^2\\) をSST（Total Sum of Squares）、\\(\\sum(\\hat{x}_{ji}-\\bar{x}_j)\\) をSSE（Sum of Squares Explained）、\\(\\sum(x_{ji}-\\hat{x}_{ji}\\) をSSR（Residual Sum of Squares）と定義すると、以下のように変形できる。 \\[ 1=\\frac{SSE}{SST}+\\frac{SSR}{SST} \\] この時、右辺第一項は共通性（観測変数全体の変動のうち、因子で説明できる部分）、第二項は独自性（観測変数全体の変動のうち、因子で説明できない部分）と呼ばれる。 これまでの因子モデルにおいて \\(a_j\\)として示された因子負荷量は、因子がそれぞれの観測変数にどの程度影響を与えているかを表しており、因子負荷量が高い程、因子と観測変数の関連が強いことを示す。また、因子分析を実行すると、因子寄与率（分散比率）が算出される。これは、観測項目全体の分散を因子によってどの程度説明しているかを示している。 詳細は、南風原（2002）を参照。↩︎ "],["因子モデル.html", "12.3 2因子モデル", " 12.3 2因子モデル ここまでの説明では、1つの因子を想定した1因子モデルを紹介したが、因子分析は複数の因子を想定したモデルも採用できる。この場合も、基本的には1因子モデルと同じように計算が可能だが、複数因子モデルにおいては、（1）軸の回転と、（2）因子数の決定、という2点について追加的に考える必要が出てくる。複数因子モデルでは、因子\\(\\times\\)因子負荷量の解の座標の取り方が一意定まらないという性質を持っている。例えば、観測変数を \\(J\\)個、潜在因子を2個含むモデルを行列表記を用いて以下のように示す。 \\[ \\boldsymbol{x}=\\boldsymbol{Af}+\\boldsymbol{e} \\] ここで、上記の式は、任意の（\\(p\\times p\\)）の正則行列（逆行列が存在する正方行列）\\(\\boldsymbol{T}\\)を用いて以下のように示すこともできる。 \\[ \\boldsymbol{x}=\\boldsymbol{Af}(\\boldsymbol{TT}^{-1})+\\boldsymbol{e} \\\\ = \\boldsymbol{A}^*\\boldsymbol{f}^*+\\boldsymbol{e} \\] この\\(\\boldsymbol{A^*}\\)と\\(\\boldsymbol{f^*}\\)もまた2因子モデルの解である。そのため、座標軸の変換についての不定性がうかがえる。因子分析の実行においては、この特徴を逆手にとり、解釈が容易になるよう（単純構造化した）軸を回転させることが可能になる。ここでいう単純構造とは各変数が1つの因子だけから強い影響を受け、他の因子からの影響が0に近くなるように見える構造を意味している。 12.3.1 因子軸の回転 軸の回転の方法としては主に、直交回転と斜交回転という2つのアプローチが存在する。直交回転とは、因子負荷量行列に直交行列をかけた解のことである。この方法では、因子間に相関がないことを仮定している。直交回転法代表例がバリマックス回転である。一方で斜交回転は、直交ではない回転を表しており、因子間の相関を認める方法である。斜交回転法の代表例はプロマックス回転である。図12.2 は、川端ほか（2019 p.180）を参照した因子軸の回転について直感的に示した概要図である。回転なしの図では、すべての観測変数が \\(f2\\) に同じような負荷を持っていることがうかがえる。一方で、直交回転では、直交である条件は守ったままではあるが、\\(1\\sim 4\\)はf1に高いがf2には低い値を取っていることがうかがえる。さらに斜交回転においては、軸の角度を自由に取ることができ、より単純構造化されていることがうかがえる。 Figure 12.2: 因子軸の回転概要 12.3.2 因子数の決定 因子分析では、モデルで採用する因子の数を自由に決定する事ができる。因子負荷量の行列は、データの相関行列を固有ベクトルと固有値（を持つ行列）にそれぞれ分解（固有値分解）することで計算されている（分寺、2022）。そして因子の数はこのプロセスに求められた固有値の値を用いて検討される。固有値分解の詳細について本書では扱わない。基本的には、より少ない因子数で全体を説明できることが好ましいのだが、因子数を少なくしすぎて説明力が下がるのは好ましくない。そのため、11 章での説明と同様、効率性と有効性のバランスを探ることになる。以下では、因子数を決めるために用いられている基準をいくつか紹介する。 第1に、各因子の固有値（eigen value）に基づく基準である。項目間の相関行列の次元（固有値の数）について、固有値分解によって求めた固有値を確認することで検討する。固有値の数は項目の数だけ計算可能であり、固有値は通常正の値を取ることが想定される（小塩, 2024）。因子数判断の基準では、固有値の値を確認し、その固有値に対応する因子を残すかどうかを判断することになる。固有値の値は直感的には、項目何個分の情報量を有しているかを意味する。そのため、1を越えていない因子についてはモデルに含めないと判断する事が多い。このような判断基準はガットマン基準やカイザー基準と呼ばれる。この時計算される各因子の固有値はその因子の寄与率にも対応しており、固有値が1ということは、（直感的には）観測変数1項目分の分散を説明していると解釈することも可能である。そのため、固有値が1以上の因子のみをモデルに用いるという方法が慣習的に広く用いられている。しかしながら、この基準に対する批判も存在していることに注意が必要である。 第2の因子数判断基準にスクリープロットの活用がある。これは、因子数を横軸にとり、それに対応する固有値を縦にプロットしたものである。図12.3 はスクリープロットの例である。スクリープロットでは、固有値そのものに加え、因子数を増やすことによる固有値の変化量（傾き）の変化にも着目する。例えば図12.3 については、因子数が2から3への変化では傾きが急であるが、3以上になった点からプロットの傾きが緩やかになっている。この場合、3因子目は説明力が低く、それ以降の因子についても説明力が高くない事が伺える。そのため、12.3 の結果では、2因子モデルを採用することが有力となる。 Figure 12.3: スクリープロット 固有値とスクリープロットによる因子数の決定は伝統的に広く用いられている基準であるが、この他にも、乱数を用いて計算された固有値との比較（平行分析）などがあるので、関心がある人はぜひ学習してみてほしい。 12.3.3 因子スコア 因子分析ではその結果に基づき、各個体が因子に対してどれだけの特性値を持っているのか、因子の得点を割り振ることができる。このように割り振られた因子についての値を因子スコア（因子得点）という。これは、抽出された因子についての予測値として解釈でき、最も基本的なスコアの求め方は回帰推定法と呼ばれる計算方法である。この方法では、個人 \\(i\\) における因子 \\(f\\) に対して（簡単化のために1因子モデルを考える）\\(J\\) 個の観測変数と、係数 \\(b\\) を想定すると、因子スコアの理論モデルは以下のように示すことができる。 \\[ f_i = b_1x_{i1}+ b_2x_{i2}+...+ b_Jx_{iJ}+u_{ij}\\\\ = \\sum^J_{j = 1}b_{j}x_{ij} + u_{i} \\] ただし、\\(u_{i}\\) は、誤差項である。この因子の予測値を \\(\\hat{f}_i\\) とし、以下の \\(G1\\) を最小化するような \\(b\\) を求めるのが、回帰推定法である。 \\[ G1 = \\sum^n_{i = 1} \\left(f_i-\\hat{f}_i\\right) \\] 12.3.4 小括 本節では、因子分析の概要についての説明を行った。因子分析の概念的な理解のためには、観測できる変数の背後に潜在因子というものが存在することを想定する必要がある。しかしながら観察できない潜在因子を測定するためには、観測変数間の共分散を用いる。ある変数同士が似ているということは、その背後に共通する因子が存在し、その因子から影響を受けているのではないかと考えているわけである。分析の結果、複数の変数を一纏めに捉えることができる因子が発見された場合、事後的に、「それはどのような因子か」を解釈し、名前をつける。このように、データに基づく結果を事後的に解釈することで因子を発見しようとするアプローチを「探索的因子分析」という。次節では、Rを用いた因子分析の実行方法を紹介しながら、探索的因子分析の手順や結果の解釈について説明する。 "],["ポジショニングと知覚マップ概要.html", "12.4 ポジショニングと知覚マップ概要", " 12.4 ポジショニングと知覚マップ概要 企業におけるマーケティング諸活動は、差別的優位性を獲得するための戦略に従って一貫性を保って提供される必要がある。そのための具体的な枠組みがSTPである。11章では、主にセグメンテーションとターゲティングに焦点を合わせ、クラスター分析のやり方を紹介した。しかしながら、企業は自社の定めた標的顧客に評価され、競合よりも優位で差別的な地位を獲得するために、具体的なマーケティング活動を調整・遂行する。4Ps（Product, Price, Place, Promotion）に代表されるマーケティングの諸活動は、このSTPの枠組みに基づくマーケティング戦略の実行手段となる。したがって、STPによるマーケティング戦略の策定は、4Psのようなマーケティング活動間の一貫性に貢献する。ここからは主にポジショニングに着目し、消費者の知覚に基づく企業（または製品）ポジションの可視化について紹介する。 ポジショニングとは、セグメンテーションとターゲティングの後、企業の製品や提供物を決定し、市場において特有のポジションを専有するための過程である。なお、この段階においては、製品やサービスを開発するだけではなく、自社が消費者のニーズを満たし、独自性を有するものだということを消費者に伝える必要がある。しかしながら自社の戦略とそれに準ずる製品属性を決定したからと言って、それによって自動的に企業のポジションが確立するわけではない。製品が消費者にどのように知覚されるかが問題になるため、消費者の知覚や評価に基づき、ポジションを理解する事が重要になる。 製品やサービスの特徴について、消費者知覚に焦点を合わせたポジショニングツールを「知覚マップ」という。知覚マップは市場に存在する企業・製品に対する消費者の認識の視覚的表現である。知覚マップの作成は、ある製品に関する（1）競合ブランドを特定する、（2）重要な製品属性を特定化する、（3）消費者の製品属性に関する評価をアンケート調査等から得る、（4）統計的分析を実施しプロットを描く、という段階を経る。例えば、製品の価格と品質の2点に関する消費者からの評価を製品 A, B, C, D について集計しプロットすると、知覚マップは、図12.4 のようなものとして示すことができる。このとき、AからBの各円は消費者による評価の集計結果を示しているとする。また、企業Aの狙いが点線で囲まれた黄色い楕円であったとする。この場合、A社の想定よりも品質が低く評価されていると言える。このような、企業自身想定と消費者からの評価とが一致しない場合もあるため、消費者からの評価について理解する必要がある。 Figure 12.4: 知覚マップ例 では、我々消費者はどのように製品やサービスを知覚・評価しているのだろうか。消費者がある製品を購買する際、通常いくつかのブランドを購買候補として着目し比較を行う。その際に消費者は、それらの各ブランドがどのような特徴を持つものかについて知覚を構成する。例えば、我々が製品を語るとき、「コスパ（コストパフォーマンス）が良い」や「高級感がある」といった漠然とした印象を用いることが多い。しかし、これらの製品に対する印象は通常、製品に関する複数の異なる要素を観察することで形成される。例えば、「コスパが良いノートパソコン」という印象の背後で、消費者は製品のCPU、ディスプレイ画質、操作性、デザインなど、様々な要素を複合的に評価しているはずである。これらの製品に関する要素は属性と呼ばれ、製品は複数の属性の束として考えられる。製品開発を行う企業においても、企業やブランドレベルでのマーケティング戦略や目標を実現するために具体的な製品属性を決定していくことが、マーケティングにおける意思決定の原則である。 複数の製品属性について知覚マップを形成しようとすると、情報が複雑になり、解釈も難しくなる。そのため、各属性の特徴の背後には抽象的な概念（上述では印象という言葉を使った）が存在すると考え、その概念を捉える形で各属性についての情報を集約し、解釈を行うために「因子分析」を用いる。例えば、製品の高級感という印象は抽象的な概念として捉えられ、この概念自体は観察できない潜在的なものである。しかしこの潜在的な概念は、製品の価格や品質、外観といった様々な観察可能な属性に影響しているはずである（図12.5）。このように、製品の各属性に対する評価をある漠然とした概念として集約することに役立つ分析手法が因子分析である。本章では、因子分析の概要とその応用としての知覚マップの作成に焦点を合わせることで、企業が自社のポジションを把握するためのリサーチ手法を紹介する。 Figure 12.5: 概念と属性例（高級感） "],["faex.html", "12.5 Rによる因子分析の実行", " 12.5 Rによる因子分析の実行 本節では、スマートフォンに関する個人の価値観について捉えた架空のデータセットを用いて、Rでの因子分析実行方法を紹介する。因子分析の実行においては、以下の手順を経ることが多い： データの特徴を確認 因子数の決定（固有値とスクリープロット確認） 因子分析の実行 抽出された因子の解釈 （必要ならば）因子スコアの計算と利用 本節で使用するデータセットでは、「スマートフォンを買う際に、あなた自身がの重視していることについて教えて下さい。」という質問に対応する以下の項目を含んでいる。なお、すべての項目が7点のリッカート尺度で構成されている。 V1: 複数のアプリを同時に快適に使える製品を買うことが大切である。 V2: 見た目が洗練されている製品が好きだ。 V3: スマートフォンは、他の機器との無線通信が快適に使用できるべきである。 V4: 人目を引くデザインをしている製品が好きだ。 V5: 操作の容易性はスマートフォンにおいて重要な要素ではない（逆転項目） V6: スマートフォンを買う上で最も重視することは、形状を含むデザインである。 ただし、V5は値が高いほど重要性が低いことを意味する項目になっていることに注意が必要である。このような質問項目を、逆転項目（Reverse-coded item）と呼ばれ、分析の際に調整する必要がある。 ここからは実際にRを用いて分析を行っていくが、そのために psychとGPArotation というパッケージを新たに使うため、以下の要領でインストールしてほしい。 install.packages(&quot;psych&quot;) install.packages(&quot;GPArotation&quot;) まずは、データとパッケージの読み込みを行う。なお、今回因子分析で用いるpsych::fa() という関数は、指定されているデータセットすべての変数を参照し分析を行ってしまう。そのため、このタイミングで、分析の対象となる変数のみを取り出したデータセットを作成、定義する。そのうえで、以下のコードではV1からV6までの変数間の相関関係を確認している。表12.1 を見ると、V1はV3、V5と正の相関が高く、V2はV4、V6と正の相関が高いことが伺える。 factor_exdata &lt;- readxl::read_excel(&quot;data/factor_ex.xlsx&quot;, na = &quot;.&quot;) library(psych) library(GPArotation) library(tidyverse) factor_exdata$V5 &lt;- 8 - factor_exdata$V5 rownames(factor_exdata) &lt;- factor_exdata$ID factor_exdata2 &lt;- factor_exdata %&gt;% select(-ID) knitr::kable(summary(factor_exdata2)) V1 V2 V3 V4 V5 V6 Min. :1.000 Min. :2.0 Min. :1.0 Min. :2.0 Min. :1.0 Min. :2.000 1st Qu.:2.000 1st Qu.:3.0 1st Qu.:2.0 1st Qu.:3.0 1st Qu.:3.0 1st Qu.:3.000 Median :4.000 Median :4.0 Median :4.0 Median :4.0 Median :4.5 Median :4.000 Mean :3.933 Mean :3.9 Mean :4.1 Mean :4.1 Mean :4.5 Mean :4.167 3rd Qu.:6.000 3rd Qu.:5.0 3rd Qu.:6.0 3rd Qu.:5.0 3rd Qu.:6.0 3rd Qu.:4.750 Max. :7.000 Max. :7.0 Max. :7.0 Max. :7.0 Max. :7.0 Max. :7.000 knitr::kable(cor(factor_exdata2), caption=&quot;相関行列&quot;) Table 12.1: 相関行列 V1 V2 V3 V4 V5 V6 V1 1.0000000 -0.0532178 0.8730902 -0.0861622 0.8576366 0.0041681 V2 -0.0532178 1.0000000 -0.1550200 0.5722121 -0.0197456 0.6404649 V3 0.8730902 -0.1550200 1.0000000 -0.2477879 0.7778480 -0.0180688 V4 -0.0861622 0.5722121 -0.2477879 1.0000000 0.0065819 0.6404649 V5 0.8576366 -0.0197456 0.7778480 0.0065819 1.0000000 0.1364029 V6 0.0041681 0.6404649 -0.0180688 0.6404649 0.1364029 1.0000000 続いて、スクリープロットを描画し、固有値を確認する。図12.6 を確認すると、因子数が2から3へ増える際に大きく固有値が低下しており、3つめの因子の固有値も1を下回っている。そのため、前節で確認した判断基準から、2因子モデルを採用し、因子分析を実行する。 cor.exdata&lt;-cor(factor_exdata2) VSS.scree(cor.exdata) Figure 12.6: スマホデータスクリープロット 因子分析では、psych::fa() という関数を用いるのだが、この関数では引数として因子数（nfactors =）、回転（rotate =）とモデルの推定方法（fm =）を指定する。以下では、斜交（プロマックス）回転を用いた2因子モデルを最尤法（ml）で分析している。 fa&lt;-fa(r = factor_exdata2, nfactors=2, rotate = &quot;promax&quot;,fm=&quot;ml&quot;) fa ## Factor Analysis using method = ml ## Call: fa(r = factor_exdata2, nfactors = 2, rotate = &quot;promax&quot;, fm = &quot;ml&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## ML1 ML2 h2 u2 com ## V1 0.97 0.00 0.94 0.063 1 ## V2 -0.02 0.75 0.56 0.437 1 ## V3 0.89 -0.12 0.83 0.174 1 ## V4 -0.06 0.78 0.62 0.378 1 ## V5 0.89 0.11 0.79 0.205 1 ## V6 0.08 0.83 0.69 0.309 1 ## ## ML1 ML2 ## SS loadings 2.54 1.89 ## Proportion Var 0.42 0.32 ## Cumulative Var 0.42 0.74 ## Proportion Explained 0.57 0.43 ## Cumulative Proportion 0.57 1.00 ## ## With factor correlations of ## ML1 ML2 ## ML1 1.00 -0.06 ## ML2 -0.06 1.00 ## ## Mean item complexity = 1 ## Test of the hypothesis that 2 factors are sufficient. ## ## df null model = 15 with the objective function = 4.25 with Chi Square = 111.31 ## df of the model are 4 and the objective function was 0.21 ## ## The root mean square of the residuals (RMSR) is 0.03 ## The df corrected root mean square of the residuals is 0.05 ## ## The harmonic n.obs is 30 with the empirical chi square 0.65 with prob &lt; 0.96 ## The total n.obs was 30 with Likelihood Chi Square = 5.21 with prob &lt; 0.27 ## ## Tucker Lewis Index of factoring reliability = 0.95 ## RMSEA index = 0.095 and the 90 % confidence intervals are 0 0.314 ## BIC = -8.39 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## ML1 ML2 ## Correlation of (regression) scores with factors 0.98 0.92 ## Multiple R square of scores with factors 0.96 0.84 ## Minimum correlation of possible factor scores 0.92 0.68 分析を実行すると、いくつかの数値が表示される。Standardized loadingsは、各変数に対する標準化された因子負荷量を表示している。 ML1（ML2）は、最尤法によって算出された因子1（因子2）の因子負荷量を示している。そのうえでML1（因子1） はV1, 3, 5に対して因子負荷量が大きくかつそれらが正である。また、M2（因子2）はV2, 4, 6に対して因子負荷量が大きくかつそれらが正であることが示されている。また、その下のSS loadings は因子負荷の二乗和、Propotion Var は各因子の寄与率（説明された分散比率）、Cumulative Var は累積寄与率、Proportion Explained は潜在因子で説明されている分散のうち、各因子が占める分散の比率を表している。これらを踏まえると、因子1は「マルチタスクの快適性」、「無線通信」、「操作容易性」に、因子2は「洗練された見た目」、「人目を引くデザイン」、「デザイン」に影響を与えるものであり、2つの因子で全体の約74%を説明していることになる。この結果を踏まえ、これら2つの因子が何を表しているかを解釈することになる。例えば、因子1は「使いやすさ」、因子2は「デザインの良さ」という名前をつけるかたちで、上記の結果を解釈する事ができる。 Rを使うと簡単に、因子、観測変数と、因子負荷量との関係を図示化することもできる。fa.diagram()という関数をつかうのだが、これを用いると、高い因子負荷量を持つ因子からのみ矢印を引くように設定されている。simple = FALSE という引数を設定することで、複数の因子から線を引くことも可能であるが、図が複雑になり見にくくなる。そのため、デフォルトの設定で図示化を行うと、図12.7 の通りの結果を得る。 fa.diagram(fa) Figure 12.7: 図示化された因子モデル 因子スコアは、因子分析の結果から抽出できる。fa でストアされた結果の場合、fa$scores と指示することで、因子スコアにアクセスできる。ここでは、因子スコアを元データに結合してみる。 fs &lt;- data.frame(fa$scores) factor_exdata2$rowname &lt;- rownames(factor_exdata2) fs$rowname &lt;- rownames(fs) factor_exdata2 &lt;- left_join(factor_exdata2, fs, by = &quot;rowname&quot;) knitr::kable(head(factor_exdata2),caption = &quot;結合後データ&quot;) Table 12.2: 結合後データ V1 V2 V3 V4 V5 V6 rowname ML1 ML2 7 3 6 4 6 4 1 1.3106155 -0.2896470 1 3 2 4 3 4 2 -1.2878038 -0.2073067 6 2 7 4 7 3 3 1.1828544 -0.7996332 4 5 4 6 6 5 4 0.1474225 1.0035837 1 2 2 3 2 2 5 -1.3907544 -1.3067260 6 3 6 4 6 4 6 0.9928111 -0.2875982 せっかくなので、今結合したデータを用いて、抽出された因子に基づくクラスター分析を実行してみる。まず、階層的クラスター分析を実施し、図12.8 を参照し、便宜的に3クラスターモデルを仮定する（エルボー法などは省略）。 library(&quot;cluster&quot;) library(&quot;factoextra&quot;) library(&quot;useful&quot;) library(&quot;ggrepel&quot;) ex_cluster &lt;- factor_exdata2 %&gt;% select(ML1, ML2) Hier1 &lt;- agnes(ex_cluster, metric = &quot;euclidian&quot;, stand = TRUE) pltree(Hier1) Figure 12.8: 2因子に基づくデンドログラム ここでは3クラスターを仮定し、K-means法でクラスター分析を実施し、その結果を図示化する。なお、以下では図のみを載せるが、関心のある学生はクラスター分析の結果も出力してみてほしい。図12.9 を見ると、デザインと使いやすさのどちらも求めている顧客層はおらず、使いやすさを重視している人たちはあまりデザインを重視していない事が伺える。一方でデザインと使いやすさの両方で低いスコアを有しているグループもあり、このグループの人たちはデザインと使いやすさの両方に対するスコアが低い。 cl_1&lt;- kmeans(ex_cluster,3) clus_fa &lt;- data.frame(cl_1$cluster) clus_fa$rowname &lt;- rownames(clus_fa) factor_exdata2 &lt;- left_join(factor_exdata2, clus_fa, by = &quot;rowname&quot;) factor_exdata2$cl_1.cluster &lt;- factor(factor_exdata2$cl_1.cluster) ##Visualizing the clusters with 2 factors p1 &lt;- ggplot(data = factor_exdata2, mapping = aes(x = ML1, y = ML2, color = cl_1.cluster)) p1 + geom_point() + geom_text_repel(mapping = aes(label = rownames(factor_exdata2))) + labs(x = &quot;Ease of use&quot;, y = &quot;Design&quot;) Figure 12.9: 2因子に基づくクラスター 以上のように因子分析は、（1）データの確認、（2）因子数の決定、（3）因子分析の実行、（4）解釈と名前付け、（5）事後的な分析、というステップを経て実行される事が多い。特にマーケティングや消費者行動論の分野では、因子分析による因子の解釈で完結するのではなく、その後の分析に因子を活用する事が多い。また、本節では因子スコアを用いた事後分析を紹介したが、マーケティング領域においては高い因子負荷量を持つ項目のみを用いた「尺度得点」を用いることも多い。例えば、「使いやすさ（EoU）」を表す変数を、\\(EoU_i=(V1_i+V3_i+V5_i)/3\\) という対応する変数の平均値によって定義する方法が。同様に、「デザインの良さ（Des）」は 、\\(Des_i=(V2_i+V4_i+V6_i)/3\\) によって捉えられる。 このような変数作成を行うと、平均値算出で用いた変数以外（例えば、使いやすさに対するV2, V4, V6）は影響を与えないと仮定してよいのか、という疑問が残るだろう。実際に、それはとても強い仮定である。このような疑問に対して統計的な判断を与えるのが、確認的因子分析というアプローチである。因子分析において、モデルに含まれていない変数の因子負荷量がゼロであるという仮定のもと、その仮定に基づくモデルとデータとの適合度を調べるような方法を確認的因子分析と呼ぶ。この方法については発展的な内容になるため本講義では割愛するが、2010年代前半ごろまではマーケティング領域で広く用いられていた手法である。 "],["アンケートデータと知覚マップ.html", "12.6 アンケートデータと知覚マップ", " 12.6 アンケートデータと知覚マップ 本節では、消費者の製品に対する知覚を図示化する形で企業のポジショニングを可視化する知覚マップの作成プロセスを紹介する。消費者の知覚について理解するためには、消費者に関する（主にアンケートによる）データが必要である。しかしながら、企業のポジションは企業同士の相対的な位置を表している。そのため、消費者レベルで収集したデータを企業レベルに集計し、企業に関する結果として図示化する必要がある。本節で紹介する知覚マップの作成は以下の手順で構成される。 複数の製品属性を捉えた、いくつかの企業に対する評価アンケートを実施する（ただし、アンケート収集プロセスは本資料では省略）。 消費者をサンプル（行）とするデータセットを、企業レベルの集計データ構造に変換する。 各企業をサンプルとする因子分析を実行し、いくつかの因子を抽出する。 抽出された各因子の因子に関する得点（因子スコアや尺度得点）をサンプル（企業）に割り当てる。 2つの因子に関する得点の関係をプロットする。 ここでは、7つのカフェチェーンに対する5つの項目について関して11人の消費者から回答を得たと想定する演習用データを用いる。ここでは、以下の5つの各項目に対して5点リッカート尺度を用いて回答を得ている。 提供されている飲み物の品質が高い 提供されている食べ物の品質が高い この企業の店舗は気持ちよく過ごせる環境である この企業の店内は楽しい雰囲気に包まれている この企業の店内は魅力的である 12.6.1 データ構造の変更 多くのテキスト等で紹介、共有されている知覚マップの作成方法やそこで利用されているデータは、すでに企業レベルで集計されている事が多い。しかしながら、消費者から複数の企業に対する複数属性についてのアンケートを取ると、多くの場合表12.3 のような構造を持つだろう。表12.3では、“y1_1” と “y2_1” はそれぞれ企業1への質問1、企業1への質問2を表している。そのため、例えば、“y1_2” は企業2への質問1を表している。なお、照井・佐藤（2022）のように、新聞等ですでに公表されている企業レベルで集計された調査結果を利用する場合、本節で紹介するデータの変換作業は不要である。 df_cafe &lt;- readxl::read_excel(&quot;data/2021_cafeSurvey.xlsx&quot;) knitr::kable(head(df_cafe), caption = &quot;カフェデータ概要&quot;) Table 12.3: カフェデータ概要 ID y1_1 y2_1 y3_1 y4_1 y5_1 y1_2 y2_2 y3_2 y4_2 y5_2 y1_3 y2_3 y3_3 y4_3 y5_3 y1_4 y2_4 y3_4 y4_4 y5_4 y1_5 y2_5 y3_5 y4_5 y5_5 y1_6 y2_6 y3_6 y4_6 y5_6 y1_7 y2_7 y3_7 y4_7 y5_7 1 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 3 3 5 5 2 2 2 3 4 3 2 1 4 5 5 3 5 2 4 4 5 3 4 3 4 2 2 2 4 3 4 2 3 3 4 3 2 3 4 4 4 2 4 2 4 4 2 4 4 4 4 2 4 3 4 4 3 5 5 2 2 3 3 4 2 2 3 3 3 2 4 3 3 3 3 3 3 3 3 2 4 2 4 2 4 4 5 4 5 4 4 4 3 4 4 4 4 4 4 5 3 3 2 3 2 3 3 5 3 4 2 2 4 5 3 5 3 3 3 4 3 4 4 4 4 5 4 4 3 4 3 3 3 4 2 3 3 3 2 2 3 4 4 4 2 3 3 4 2 2 2 3 5 3 2 4 3 4 3 3 3 6 4 3 4 3 4 4 3 4 4 4 2 3 3 3 3 3 3 2 3 2 4 3 1 2 2 4 5 4 4 4 4 3 3 3 3 本資料では、知覚マップの作成についてデータ構造の変更から説明する。慣習としては、消費者についての平均値や合計をとって、製品\\(\\times\\)項目の構造を持ったデータに修正する。本節で紹介する方法は、一般的に 「wide型データ」から「long型データ」への変換と呼ばれる作業である。以下では、合計を使って集計する形でデータ構造の変換作業を行っている。 #IDについての情報は含まない形で、データをlong型に変える。 #&quot;names_to&quot; と &quot;values_to&quot; によって列名が定義される。 data_reshaped &lt;- df_cafe %&gt;% pivot_longer(-ID, names_to = &quot;company&quot;, values_to = &quot;y&quot;) #企業番号と変数をそれぞれ &quot;company&quot; と &quot;k&quot; として割り振る。 ##&quot;str_extract&quot;によって正規表現を抽出 ##&quot;parse_number&quot;によって特定の値を抽出 data_reshaped &lt;- data_reshaped %&gt;% mutate(k = str_extract(company, &quot;y[0-9]&quot;), company = str_extract(company, &quot;_[0-9]&quot;) %&gt;% parse_number()) #&quot;k&quot;をwide型に変換し、変数の列を作成する。 data_reshaped &lt;- data_reshaped %&gt;% pivot_wider(names_from = &quot;k&quot;, values_from = &quot;y&quot;) #企業名の割振りと、企業レベルデータへの集計 data_renamed &lt;- data_reshaped %&gt;% mutate(company = case_when( company ==1 ~ &quot;A&quot;, company ==2 ~ &quot;B&quot;, company ==3 ~ &quot;C&quot;, company ==4 ~ &quot;D&quot;, company ==5 ~ &quot;E&quot;, company ==6 ~ &quot;F&quot;, company ==7 ~ &quot;G&quot;, TRUE~&quot;else&quot; )) brand_based &lt;- data_renamed %&gt;% group_by(company) %&gt;% summarize(q1_sum = sum(y1), q2_sum = sum(y2), q3_sum = sum(y3), q4_sum = sum(y4), q5_sum = sum(y5)) %&gt;% tibble::column_to_rownames(var = &quot;company&quot;) knitr::kable(brand_based, caption = &quot;変換後データ&quot;) Table 12.4: 変換後データ q1_sum q2_sum q3_sum q4_sum q5_sum A 45 41 46 45 45 B 35 34 37 37 38 C 32 33 38 37 34 D 38 40 39 32 36 E 39 39 36 31 35 F 37 44 39 35 39 G 43 45 44 35 44 12.6.2 因子分析の実行 前述の通りデータ構造が企業レベルに変換できたら次は因子分析と図示化テクニックを使って知覚マップを作成する。以下では、簡単にそのプロセスを紹介する。まずはスクリープロットと固有値によって因子数を検討する。図12.10 によると、傾きの変化、固有値どちらの観点からも2因子を採用することにする。 cor_b &lt;- cor(brand_based) VSS.scree(cor_b) Figure 12.10: カフェデータスクリープロット 続いて、2因子モデルで因子分析を実行し、因子を解釈する。分析の結果、1つ目の因子は問1、2, 3, 5に高い因子負荷量を持っている。一方で、2つ目の因子は、問4に高い因子負荷量を持っている。また、因子1よりは低いものの、問3についても高い因子負荷量を有している。そこで、因子1を製品と環境、因子2を店舗の雰囲気と名付けることとする。また着目すべきは、因子1は問4（この企業の店内は楽しい雰囲気に包まれている）に、因子2は問2（提供されている食べ物の品質が高い）に対して負の因子負荷量を持っていることである。そのため、因子1と2は、因子の値が高まると、これらそれぞれの観測変数が低くなると考えられる。消費者の知覚・評価の中で、これらの因子や項目の間になんらかのトレードオフ構造があるかもしれない。 Perc_cafe &lt;- fa(r = brand_based, nfactors = 2, rotate = &quot;promax&quot;) Perc_cafe ## Factor Analysis using method = minres ## Call: fa(r = brand_based, nfactors = 2, rotate = &quot;promax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## MR1 MR2 h2 u2 com ## q1_sum 0.84 0.12 0.82 0.1825 1.0 ## q2_sum 1.04 -0.38 0.83 0.1694 1.3 ## q3_sum 0.58 0.52 0.91 0.0892 2.0 ## q4_sum -0.25 1.10 1.00 0.0023 1.1 ## q5_sum 0.67 0.44 0.93 0.0654 1.7 ## ## MR1 MR2 ## SS loadings 2.64 1.86 ## Proportion Var 0.53 0.37 ## Cumulative Var 0.53 0.90 ## Proportion Explained 0.59 0.41 ## Cumulative Proportion 0.59 1.00 ## ## With factor correlations of ## MR1 MR2 ## MR1 1.0 0.5 ## MR2 0.5 1.0 ## ## Mean item complexity = 1.4 ## Test of the hypothesis that 2 factors are sufficient. ## ## df null model = 10 with the objective function = 5.54 with Chi Square = 19.39 ## df of the model are 1 and the objective function was 0.02 ## ## The root mean square of the residuals (RMSR) is 0 ## The df corrected root mean square of the residuals is 0.01 ## ## The harmonic n.obs is 7 with the empirical chi square 0 with prob &lt; 0.97 ## The total n.obs was 7 with Likelihood Chi Square = 0.04 with prob &lt; 0.84 ## ## Tucker Lewis Index of factoring reliability = 5.793 ## RMSEA index = 0 and the 90 % confidence intervals are 0 0.618 ## BIC = -1.91 ## Fit based upon off diagonal values = 1 ## Measures of factor score adequacy ## MR1 MR2 ## Correlation of (regression) scores with factors 0.98 1.00 ## Multiple R square of scores with factors 0.96 1.00 ## Minimum correlation of possible factor scores 0.92 0.99 上記の因子を反映した知覚マップは図12.11 で表されている。これを見ると、企業Aはどちらの因子についても高い評価を得ている事がわかる。一方で企業Cは、製品や環境については標準的な評価を得ているものの、雰囲気については評価がとても低くなっている。反対に、企業DやEのように、製品や環境について悪い評価を得ている企業も存在する。本節では簡単に図12.11 についての紹介を行っているが、実際にレポートを書く際には、このような結果から得られる含意や実務的示唆について詳しく論じる必要があることに注意してほしい。 fs2 &lt;- data.frame(Perc_cafe$scores) brand_based &lt;- brand_based %&gt;% rownames_to_column(var = &quot;rowname&quot;) fs2 &lt;- fs2 %&gt;% rownames_to_column(var = &quot;rowname&quot;) brand_based2 &lt;- left_join(brand_based, fs2, by = &quot;rowname&quot;) p1 &lt;- ggplot(data = brand_based2, mapping = aes(x = MR1, y = MR2)) p1 + geom_point() + geom_text_repel(mapping = aes(label = rowname)) + labs(x = &quot;Atmosphere&quot;, y = &quot;Product+environment&quot;) Figure 12.11: カフェ知覚マップ 本節では、データ構造の変換も含む知覚マップの作成手順について紹介した。このような方法により、「消費者が企業をどのように評価しているか」という観点から、自社や競合のポジションを把握する事ができる。また、この企業レベルでのデータに対して因子分析とクラスター分析を組み合わせることによって、図12.12 のような図を描画できる。この演習データの場合には少し意義が見出しづらいかもしれないが、図12.12のような可視化はより明確な実務的含意につながりうる。具体的には、2つの因子間において類似する企業を特定することができ、市場の競争環境（特に誰が競合となりうるかなど）について可視化できると考えられる。 cafe_cluster &lt;- brand_based2 %&gt;% column_to_rownames(var = &quot;rowname&quot;) %&gt;% select(MR1, MR2) #事前の階層的クラスター分析等は省略 #便宜的に3クラスターを想定している cl_cafe&lt;- kmeans(cafe_cluster,3) clus_fa_cafe &lt;- data.frame(cl_cafe$cluster) clus_fa_cafe$rowname &lt;- rownames(clus_fa_cafe) brand_based2 &lt;- left_join(brand_based2, clus_fa_cafe, by = &quot;rowname&quot;) brand_based2$cl_cafe.cluster &lt;- factor(brand_based2$cl_cafe.cluster) ##Visualizing the clusters with 2 factors p1 &lt;- ggplot(data = brand_based2, mapping = aes(x = MR1, y = MR2, color = cl_cafe.cluster)) p1 + geom_point() + geom_text_repel(mapping = aes(label = rownames(brand_based2))) + labs(x = &quot;Atmosphere&quot;, y = &quot;Product+environment&quot;, color = &quot;Cluster&quot;,shape = &quot;Cluster&quot;) Figure 12.12: 知覚マップ &amp; クラスター 因子分析は、複数の観測変数の背後にある潜在的な因子を捉えようとする手法である。マーケティングや消費者行動では、この潜在的な因子に基づく議論も多く、因子分析は非常に重要な手法の1つである。特に、因子分析を行って因子を特定するだけではなく、それを知覚マップとして図示化したり、クラスター分析と組み合わせた分析を行うことで実務的含意を含む発見につながることが期待される。 "],["pcafa.html", "12.7 （おまけ）主成分分析との違い", " 12.7 （おまけ）主成分分析との違い 因子分析とよく対比される手法として主成分分析（Principal component analysis）がある。主成分分析は、複数の変数間の類似性を分析し、これらの変数をできるだけ少ない数の合成変数（主成分）にまとめることを目的とした分析手法である。そのため、この主成分はできるだけ高い寄与率を持つことが求められる。主成分分析は、変数が多いとき情報の損失を最小限に押さえながらできるだけ少ない（理想的にはひとつの）主成分に縮約する方法であるといえる。一方で因子分析は、観測変数を「メリハリのある分かれ方（単純構造）」にするように因子数の決定や負荷量の計算が行われる（分寺, 2022, p.55）。主成分の計算方法は本資料では省略するが、興味のある人は田中・脇本（1998）や、渡辺（2017）を参考にしてほしい。 主成分分析のモデルは、図12.13 のように示される。因子分析のパス図（12.1）と比べると、矢印の向きに違いがあることがわかる。 Figure 12.13: 主成分分析モデル 同様に、主成分モデルを式で表すと以下のようになる： \\[ z_i=b_1x_{1i}+b_2x_{2i}+b_3x_{3i} \\] 主成分分析と因子分析とではモデル化のコンセプトが異なる。主成分分析では、変数から主成分を導き出すという想定でモデル化されていることが伺える。一方で因子分析はあくまで背後にある潜在因子が観測変数を規定すると想定している。そのため因子分析を用いる場合には、因子と変数の前後関係が想定できるような現象を捉えたデータにのみ有効である。また、誤差項（独自因子）を含まないということも主成分分析モデルの特徴である。 主成分分析は計算方法においても因子分析とは異なる点を有する。結論から述べると、主成分分析は因子分析の特殊形と考えられる（渡辺, 2017 参照）。計算上では、主成分分析は因子分析における主因子法というアプローチの特殊な場合と位置づけられる。主因子法とは、ある制約のもとで、各因子が観測変数の分散を説明する寄与率を最大にするように因子を求める計算方法である。この方法では、共通性を最初に計算し、因子負荷量を算出、因子数を決定した後再度共通性を計算する。そして新しく得られた共通性をもとに再度因子負荷量を算出するというプロセスを繰り返す。計算における共通性の初期値は1や、重相関係数の2乗値、相関行列の行の相関の最大絶対値などが使われる。 これに対して主成分分析は、共通性初期値を1とし、繰り返し計算を行わない主因子法といえる。より細かく述べれば、主成分分析によって求められる主成分係数（上式の \\(b_1, b_2, b_3\\)）は項目間分散共分散行列の固有ベクトルとして計算される一方で、因子負荷量は対角要素に共通性を代入した項目間相関行列の固有ベクトルに固有値の平方根を乗じたものだといえる（渡辺, 2017）。そのため、標準化された項目を用いた主成分分析によって得る主成分係数（項目間相関行列の固有ベクトル）に固有値の平方根を乗じると、対角要素を1とした項目間相関行列の因子負荷量（ただし、繰り返し計算は行わない）と等しくなる。 Rで主成分分析を行う際は、psych::pca() を用いる。ここで、12.5 節で用いた演習用データを用いて、主成分分析を実行し、それが、「項目間相関行列の固有ベクトルに固有値の平方根を乗じたもの」と等しくなるかを検討する。まず主成分分析を実行するが、デフォルトでは軸の回転を行ってしまうので、ここでは rotate = \"none\"とする。 factor_exdata3 &lt;- factor_exdata %&gt;% select(-ID) pca_res &lt;- pca(factor_exdata3, nfactors = 2, rotate = &quot;none&quot;) pca_res ## Principal Components Analysis ## Call: principal(r = r, nfactors = nfactors, residuals = residuals, ## rotate = rotate, n.obs = n.obs, covar = covar, scores = scores, ## missing = missing, impute = impute, oblique.scores = oblique.scores, ## method = method, use = use, cor = cor, correct = 0.5, weight = NULL) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 h2 u2 com ## V1 0.93 0.25 0.93 0.074 1.1 ## V2 -0.30 0.80 0.72 0.277 1.3 ## V3 0.94 0.13 0.89 0.106 1.0 ## V4 -0.34 0.79 0.74 0.261 1.4 ## V5 0.87 0.35 0.88 0.122 1.3 ## V6 -0.18 0.87 0.79 0.210 1.1 ## ## PC1 PC2 ## SS loadings 2.73 2.22 ## Proportion Var 0.46 0.37 ## Cumulative Var 0.46 0.82 ## Proportion Explained 0.55 0.45 ## Cumulative Proportion 0.55 1.00 ## ## Mean item complexity = 1.2 ## Test of the hypothesis that 2 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.07 ## with the empirical chi square 3.94 with prob &lt; 0.41 ## ## Fit based upon off diagonal values = 0.98 続いて、項目間相関係数の固有ベクトルを計算し、それに固有値の平方根を乗じる。その結果、表12.5 は（PC2については符号は異なるものの）上記の主成分分析の結果と一致している事がわかる。 #データの相関行列から固有値と固有ベクトルを抽出 cor_ex &lt;- cor(factor_exdata3) eigen_list &lt;- eigen(cor_ex) #固有ベクトルについての情報をオブジェクトとして定義 pc1_eig &lt;- eigen_list$vectors[, 1] pc2_eig &lt;- eigen_list$vectors[, 2] #固有ベクトルに、固有値の平方根を乗じる PC1 &lt;- pc1_eig* sqrt(eigen_list$values[1]) PC2 &lt;- pc2_eig* sqrt(eigen_list$values[2]) knitr::kable(data.frame(cbind(PC1,PC2)), caption=&quot;計算結果&quot;) Table 12.5: 計算結果 PC1 PC2 0.9283425 -0.2532285 -0.3005297 -0.7952496 0.9361812 -0.1308894 -0.3415817 -0.7889663 0.8687553 -0.3507939 -0.1766389 -0.8711581 "],["練習問題-8.html", "12.8 練習問題", " 12.8 練習問題 回答データ【消費者調査2023年度下期調査】.xlsx を活用し、以下の分析を実行しよう。 データセットから質問13, 16群を抽出する。 妥当な因子数の特定する。 2で特定した因子数を使った因子分析を回転ありで実施する。 結果を解釈する。 発見した因子を活用し、クラスター分析を実行する。 "],["参考文献-10.html", "12.9 参考文献", " 12.9 参考文献 川端一光, 岩間徳兼, 鈴木雅之（2019）Rによる多変量解析入門, オーム社. 田中豊・脇本和昌（1998）多変量統計解析法，現代数学社. 照井伸彦・佐藤忠彦（2022）現代マーケティング・リサーチ[新版],有斐閣. 南風原朝和（2002）心理統計学の基礎（有斐閣アルマ），有斐閣. 分寺杏介（2022）「Chapter 6 因子分析」，統計的方法論特殊研究(多 変量解析)講義資料. 渡辺利夫（2017）Rで多変量解析, ナカニシヤ出版. "],["psm.html", "Chapter 13 価格感度測定 (PSM) ", " Chapter 13 価格感度測定 (PSM) "],["本章の概要-10.html", "13.1 本章の概要", " 13.1 本章の概要 価格は、マーケティング意思決定者が調整する重要なマーケティング戦術要因（4Ps）のひとつである。価格は4Psの中で唯一企業にとっての収入を生む要因であり、製品の需要量やイメージにも関連する重要な意思決定項目である。消費者の視点にたてば、価格は自身の予算制約のもとその製品が購入に値するかを判断する定量的かつ認知的な情報であると同時に、品質や高級感を推測するための心理的影響を持つ情報としても機能する。マーケティング意思決定者は、製品のマーケティング戦略や目標と一貫するように価格を設定する必要があるが、どれぐらいの価格で販売すれば、消費者が安い/高いと感じるのかを事前に把握したいと考えるだろう。そこで本章では、「消費者に受け入れられる価格」について洞察を得るための方法として、価格感度測定（Price Sensitivity Meter; PSM）という手法を説明する。 PSMは、マーケティング戦術決定に役立つ調査・分析手法でありながら、高度な分析手法を用いる必要がない。この手法はアンケート調査による簡便なアプローチとして広く実務的に用いられている。本章では、PSM の方法と結果の解釈に加えて、PSM に関する学術的背景についても説明することで、「PSM が何を明らかにしようとしているのか」を直感的に理解できるようになることを目的とする。そのために本章では、以下の順でPSMについての説明を行う。第一に、PSMでどのような情報を得ることができるのかを概観するため、PSMで用いる調査方法と、Rでの分析方法を紹介する。続いて、質問の背景に存在する価格概念について説明した後、PSMの背景に存在する学術的議論について簡単に触れる。 また、本章の最後にはPSMを用いるうえでの注意点も紹介する。実務的には、本章で紹介する方法とは異なる、一般的には誤用と考えられるアプローチを活用しているケースも散見される。本書では、そのようなアプローチがなぜ誤用だと言えるのか、そしてなぜそれが広まったのかという経緯についても説明を加える。 "],["psmrunning.html", "13.2 PSMの実行と結果", " 13.2 PSMの実行と結果 価格は、マーケティング意思決定者が調整する重要なマーケティング戦術要因（4Ps）のひとつである。価格は4Psの中で唯一企業にとっての収入を生む要因であり、製品の需要量やイメージにも関連する重要な意思決定項目である。消費者の視点にたてば、価格は自身の予算制約のもとその製品が購入に値するかを判断する定量的かつ認知的な情報であると同時に、品質や高級感を推測するための心理的影響を持つ情報としても機能する。マーケティング意思決定者は、製品のマーケティング戦略や目標と一貫するように価格を設定する必要があるが、どれぐらいの価格で販売すれば、消費者が安い/高いと感じるのかを事前に把握したいと考えるだろう。このような背景のもと、PSMは、消費者の心理的側面を踏まえた価格に関する含意の提示を目的としている。 PSM では、4つの質問項目を用いて消費者の価格に対する認識を捉える。なお、質問においては、消費者の価格に関する知覚について知りたい対象（製品やサービス）を特定化する必要がある。例えば、具体的な設問の前に、製品の画像や情報を提示することで、回答者が参照する製品やサービスを特定化することも有効になる。その上で、PSMでは以下の4つの質問によって調査を行う。 （ある製品に対して）安すぎて買わない価格 例：「（この製品を買う［サービスを利用する]のに）価格がいくらより低いと、買うには安すぎて品質に不安を感じますか？; （回答）〇〇円」 安いと感じる価格 例：「（この製品を買う［サービスを利用する]のに）価格がいくら以下だと、安いと感じますか？; （回答）〇〇円」 高いと感じる価格 例：「（この製品を買う［サービスを利用する]のに）価格がいくら以上だと、高いと感じますか？; （回答）〇〇円」 高すぎて買わない価格 例：「（この製品を買う［サービスを利用する]のに）価格がいくら以上だと、買うには高すぎると感じますか？; （回答）〇〇円」 なお、これらの回答の回答によって得た４つ価格は、\\(4 &gt; 3 &gt; 2 &gt; 1\\) という関係が成り立っている必要がある。これらの質問への回答により得たデータは、以下の図のように回答者ベースの構造をもつはずである。 アンケート元データ構造 PSMでは、上記のデータから各価格レンジ（以下の例では、x円以下形式）に対して、安すぎる、安い、高い、高すぎる、と感じる人がどの程度いるかを集計する。ある価格水準に対し、安い（高い）や安すぎる（高すぎる）と感じる回答者がどれだけいるのかという（相対）頻度を求めることで分析を行う。以下の表はPSMでの集計を簡単に示したデータ構造である。 PSM用データ構造 Rを用いた分析においては、自身でデータ構造の変換を行う必要はない。Rでは、“pricesensitivitymeter” というパッケージの psm_analysis()という関数を用いることで分析を実行できる。なお、PSM用データ構造への集計作業もこのパッケージにおいて行ってくれる。 ここからは、実際にRを用いたPSMの実行手順を紹介する。まず分析の実行に際し、このパッケージをインストールし、起動する。 install.packages(&quot;pricesensitivitymeter&quot;) library(pricesensitivitymeter) library(tidyverse) 続いて、“psm_ex.csv” という演習用データを読み込み、psm_ex として定義する。 psm_ex &lt;- read.csv(&quot;data/psm_ex.csv&quot;, na = &quot;.&quot;) head(psm_ex) ## tch ch ex tex ## 1 963 1016 1242 1318 ## 2 866 1082 1272 1303 ## 3 795 824 1290 1297 ## 4 812 933 1211 1335 ## 5 1026 818 1228 1289 ## 6 912 987 1195 1395 このデータは、ある製品への価格について、安すぎる（tch）、安い（ch）、高い（ex）、高すぎる（tex）という上記4つの価格認識について、250人の消費者から回答を得た（と仮定する）人工データである。分析の実行においては以下のように、psm_analysis() という関数において、toocheap、cheap、expensive、tooexpensiveという引数を用いて、データセットにおけるどの変数がPSMで用いる価格情報に対応するのかを指示する必要がある。 output_psm &lt;- psm_analysis( toocheap = &quot;tch&quot;, cheap = &quot;ch&quot;, expensive = &quot;ex&quot;, tooexpensive = &quot;tex&quot;, data = psm_ex ) summary(output_psm) ## Van Westendorp Price Sensitivity Meter Analysis ## ## Accepted Price Range: 923 - 1253 ## Indifference Price Point: 1101 ## Optimal Price Point: 1058 ## ## --- ## 157 cases with individual price preferences were analyzed (unweighted data). ## Total data set consists of 250 cases. Analysis was limited to cases with transitive price preferences. ## (Removed: n = 93 / 37% of data) psm_analysis() の実行結果として、Accepted price range（受容価格域）、Indifference price point（無差別価格）、Optimal price point（最適価格）が提示される。今回の分析の結果を見れば、受容価格域は 923-1253、無差別価格は 1101、最適価格は 1058 である。受容価格域とは、安すぎると感じる人と高すぎると感じる人が少ない価格域を表している。 また無差別価格とは、高いと感じる人と安いと感じる人がつりあっていて、高くもなく安くもない価格である。一方で最適価格とは、高すぎたり安すぎたりして買わないという人が最も少ない（受容する人が最も多い）価格である。 また、PSMでは分析結果を図示化する慣習もあるが、これも psm_plot() という関数を用いて、ggplot 形式のコマンドで簡単に出力できる。 psm_plot(output_psm) + labs( x = &quot;Price&quot;, y = &quot;Share of Respondents (0-1)&quot;, title = &quot;Price Sensitivity Meter Plot&quot;, caption = &quot;Shaded area: range of acceptable prices\\nData: Randomly generated&quot;) + theme_minimal() Figure 13.1: PSM Plot 上記の図では、受容価格価格域、無差別価格、最適価格というそれぞれの結果が、“too cheap”（安すぎる）曲線、“too expensive”（高すぎる）曲線、“not cheap”（安いと感じない）曲線、“not expensive”（高いと感じない）曲線の交点によって導かれていることがうかがえる。次節以降では、これらの結果の背景にはどのような理論があるのかについて詳しく述べる。 "],["price_concept.html", "13.3 マーケティングにおける価格概念", " 13.3 マーケティングにおける価格概念 本節では、PSMの結果への理解を深めるために、マーケティングにおけるいくつかの価格概念を紹介する。PSMは消費者の内的な価格に対する認識や基準を集計する方法である。消費者の購買に関わる最も基本的な価格概念に、支払い意思額（Willingness to Pay: WTP） がある。WTPは、消費者が特定の製品に対して「最大いくらまで払ってもいい」と考えているかを捉えた価格である。単純化された理論的世界観のもとでは、消費者はある製品の価格が、自身の WTP を下回る場合には、その製品を購入すると考えられる。 しかしながら、消費者の心理的側面を考慮すると、価格が安ければいいというわけでもない。我々の日常的な買い物においては、ある製品が自身のWTPより高いので買えないということに加えて、安すぎる価格によって、（製品品質などに）不安を抱き買いたくないと考えることもあるだろう。製品やサービスの品質が事前にわからない場合、人々はなんらかの情報に基づいて品質を類推するが、価格がその手がかりになると考えられる。このように、製品の価格が安すぎても、高すぎても買ってもらえず、どうやら消費者にとって、ちょうどいい価格幅がありそうだといえる。そして、「安すぎて不安だから買いたくない」という価格から「高くて買えない」という価格で示される、消費者が受容する（製品を買う）価格帯のことを、受容価格帯（Accepted price range）という。 もう一つの心理的側面を捉えた価格概念に、内的参照価格がある。我々の日常的な買い物を振り返ると、自身が買える価格の範囲内であっても、その価格が高いもしくは安いと感じることはあるだろう。内的参照価格は、買い手がその価格が妥当かどうか判断する際の基準となる、個人の価値観を反映した価格である。つまり内的参照価格は、「高すぎて買えない」や「安すぎて買いたくない」というほど極端な価格ではないが、自身の持っている基準から消費者自身が「これは安いな」とか「これは高いな」と感じることを捉えた価格概念である。ただし、内的参照価格は対象となる製品やサービスの価値だけではなく、消費者個人の経験や他の関連製品の価格にも影響を受けることに注意が必要である。PSM では主に、受容価格と内的参照価格を中心に分析・図示化を行う。 "],["psmに関する既存研究.html", "13.4 PSMに関する既存研究", " 13.4 PSMに関する既存研究 13.4.1 受容価格に関する知見 本節ではまず、PSMのもとになっている、受容価格帯に基づく価格分析として Gabor and Grager (1966) について紹介する。PSMは主にマーケティング実務上での活用を前提提示された手法であり、学術研究分野ではあまり言及されることはない。しかしながら、PSMは前節で述べた受容価格帯と内的参照価格に関する学術的知見に基づき提唱されたという経緯がある。 受容価格帯に関する知見としては、Gabor and Granger (1966) による購買反応曲線（Buy Response Curve；以下、BRC）の知見が活かされている。本節では、この論文の知見を簡単に紹介する。詳細について関心がある場合にはぜひ論文を読んで見てほしい。逆に、PSMに関する理論的背景に関心がない場合には、「実務的に広く使われているPSMにも、学術的な蓄積がある」ということさえ頭に留めてくれれば、そのまま次節へ進んでもらっても構わない。 Gabor and Granger (1966)では価格に対する需要量の変化ではなく、ある価格においてその製品が購買される可能性（Plausibility）について捉えた議論を展開している。この研究では受容価格の上限と下限に着目し、ある価格Pにおいて、消費者がそれを安すぎると感じる確率を\\(L(P)\\)、価格Pにおいて、消費者がそれを高すぎると感じる確率を \\(H(P)\\) とした（ただし、\\(0\\leq L(P)\\leq1\\)かつ\\(0\\leq H(P)\\leq1\\)）。これらの確率は、下図で示されるように、\\(H(P)\\)が単調増加である一方で、\\(L(P)\\) は単調減少であり、\\(L(P)\\) ではなく \\(1-L(P)\\) が累積分布関数として捉えられる。 Figure 13.2: L(P)とH(P) これらを用いて、価格Pが消費者の受容価格範囲に入っている確率を \\(B(P) = 1- L(P) - H(P)\\) と定義した。ただし、\\(H(P)\\) と \\(L(P)\\) はすべての \\(P\\) に対して微分可能であり、\\(l(P)=dL(P)/dP\\)、\\(h(P)=dH(P)/dP\\)とする。\\(B(P)\\)は、ある価格Pを消費者が高すぎるとも安すぎるとも感じない確率と言い換えることができ、これを購買反応曲線（BRC）と呼んだ。BRCの形状は以下のように示すことができる。この形状から、\\(P&#39;\\) という価格水準を基準とし、それより高いないし低い価格において、購買される可能性が低くなることが予想される。 Figure 13.3: 購買反応曲線 Gabor and Granger (1966)は思いつきや恣意的に曲線の形状を決めたのではなく、いくつかの仮説に基づき、BRCの形状について議論を発展させた。これが、彼らの研究の一番の理論的貢献である。特に、ここで想定されている仮説を確認することで、BRCの議論においてどのような消費者像が想定されているのかについて理解が可能になる。 例えば論文内では、１つ目の仮説（H1）として、先述の \\(l(P)\\) と \\(h(P)\\) をそれぞれ、低すぎ頻度関数と高すぎ頻度関数と呼び、これらが対数正規の密度関数に従うという仮説を述べた。つまり、\\(p = logP\\) としたとき、\\(l(p)\\) と \\(h(p)\\) は正規分布に従うと考えられる。ここでは、Weber-Fechner law という、「ある刺激とそれに対する個人の知覚の関係は対数的である」とする理論に基づき、対数線形を用いた形状を仮定している。言い換えると、Gabor and Granger (1966) によるBRCは、価格の増加という刺激を感覚（お得感や割高感等）として知覚する消費者像を想定したモデルだと解釈できる。 仮説2（H2）では、ある価格と消費者グループにおいては、\\(l(p)\\) と \\(h(p)\\) の標準偏差は近似的にほぼ等しいと述べた。詳細は割愛するが、H1とH2を含む計4つの仮説と整合的な結果を得ることで、BRCの形状についての知見を獲得した。\\(1-L(p)\\) と \\(H(p)\\) が正規の確率分布関数に従うとするならば、\\(B(p) = 1- L(p) - H(p)\\) の理論的な形状も推察できる。ただし、\\(B(p)\\) 自体は確率密度関数ではない。Gabor and Granger (1966) では、\\(m_1\\) と \\(m_2\\) をそれぞれ、\\(l(p)\\) と \\(h(p)\\) の期待値とし、\\(b(p)=B(p)/(m_2-m_1)\\) を確率密度関数と捉えた。単純化のために H2 を仮定すると、\\(b(p)\\) は \\((m_1+m_2)/2\\) を中心に左右対称である。これらの仮説および議論のもと、\\(l(p)\\)、\\(h(p)\\)と、\\(b(p)\\) は以下のように図示できる。このように、Gabor and Granger (1966)は、いくつかの仮定を満たすような状況では、消費者の価格に対する反応を数量的に分析することが可能になることを示した。 Figure 13.4: 購買反応密度関数 13.4.2 内的参照価格に関する知見 受容価格に関するBRCに加え、内的参照価格についての知見もPSMには重要である。詳しくは兼子（2014）を参照してほしいが、内的参照価格にも幅が存在すると考えられる。つまり、受容価格帯内において安いか高いかを知覚する価格は特定の水準だけでなく、価格幅として構成されると考えられる。ある製品の価格が消費者にとって高いと感じる価格帯よりも低く、安いと感じる価格帯よりも高い価格幅に含まれている場合、その価格は「安くも高くもない」価格として知覚される。言い換えると、この安くも高くもない価格幅に基づき、その上限よりも高い価格については高いと感じ、下限よりも低い価格については安いと感じると考えられる。以下では、兼子（2014）を参考に、受容価格帯（\\(P_4-P_1\\)）と内的参照価格帯（\\(P_3-P_2\\)）を図示する。 Figure 13.5: 内的参照価格帯 上図における、\\(P_1\\) を安すぎる価格の上限、\\(P_2\\) を安いと感じる価格の上限、\\(P_3\\)を高いと感じる価格の下限、\\(P_4\\) を高すぎる価格の下限と考える。そのため、\\(P_1\\) より低い価格や、\\(P_4\\) より高い価格の場合には消費者は購買しないと仮定する。一方で、\\(P_2-P_1\\) の範囲では、安いと感じるが買うと考えられるし、\\(P_4-P_3\\) の範囲では高いと感じるが買うと考えられる。このように、受容価格帯と内的参照価格帯の関係を捉えることで、「買う買わない」に加えた消費者の価格に対する感じ方も捉えた含意を得ることができる。 "],["受容価格帯と内的参照価格帯の応用とpsm.html", "13.5 受容価格帯と内的参照価格帯の応用とPSM", " 13.5 受容価格帯と内的参照価格帯の応用とPSM PSM は、受容価格帯と内的参照価格帯に関わる知見に基づき提案されている価格決定手法である。PSMの特徴は、「安すぎる価格」、「安い価格」、「高い価格」と「高すぎる価格」の4つの質問に対する個人の価格水準を回答してもらう点にある。PSMでは特定の価格を提示しその価格に対する消費者の評価を得るという方法ではないことにも注意が必要である。つまり、「この製品が〇〇円だったらどのように感じますか？：1. 安い,…,7.高い」のような質問形態を取らない。 PSMでは、上記の4つの質問から得た回答をもとに、価格の変化に対する安い（または高い）と感じる人と安すぎる（高すぎる）と感じる人の累積比率を計算し図示化していくのだが、もう一点注意するべき点がある。それは、「高いと感じる人」の比率と「安いと感じる人」の比率を１（または100%）から引き、「高いと感じない人」と「安いと感じない人」についての比率を示す曲線を描画するということである。これらの高い（安い）と感じない人の比率を示した曲線は高い（安い）と感じる人の比率を反転させたもの（兼子，2014）や余事象（岸など，1999）と呼ばれている。以下の図は、「高い（安い）と感じない」という形に反転させた状態でのPSM分析結果例を表現した比率曲線である。下図のうち、“Too expensive” （青い実線）と “Too cheap”（赤い実線） 曲線はそれぞれ高すぎると安すぎる（と感じる人の）比率を表しており、“Not expensive” （青い破線）と “Not cheap”（赤い破線）曲線は高いと感じないと安いと感じない人の比率を示している。 Figure 13.6: PSM結果例 PSMでは、上図内の各曲線の交点から、最適価格（\\(P^*\\)）、無差別価格（\\(\\tilde{P}\\)）、最高価格（\\(\\bar{P}\\)）、最低価格（\\(\\underline{P}\\)）を算出する。\\(P^*\\) は、受容価格帯に基づく価格点である。\\(P^*\\)は高すぎると感じる人と安すぎると感じる人の比率が最も少なくなり、受容不可能な消費者が最小となる価格である。\\(\\tilde{P}\\) は内的参照価格に基づく価格点である。\\(\\tilde{P}\\)は、「安いと感じない」曲線と「高いと感じない」曲線の交点であり、安いとも高いとも感じない消費者が最多となる価格帯であり、前節 13.5 の図における \\(P_3-P_2\\)の範囲内だと感じている消費者が最も多い価格だと解釈できる。 それ以外の価格点の解釈は比率の余事象（例、高いと感じない、安いと感じない）を求めることで容易になっている。\\(\\bar{P}\\) は、高すぎると高いと感じないの曲線の交点である。この点は、「高いとは思うが高すぎるわけではない」と感じる消費者が多い点である。この点よりも低い価格では高いと感じる人が少ない（高くないと感じる人が多い）が、この点よりも高い価格では高すぎると感じる人が多くなる。言い換えると、前節における \\(P_4-P_3\\) の範囲内だと感じている消費者が多い価格だと解釈できる。一方で \\(\\underline{P}\\) は安すぎると安いと感じないという二つの曲線の交点である。この点は、「安いとは思うが安すぎるわけではない」と感じる消費者が多い点であり、前節における \\(P_2-P_1\\) の範囲内だと感じている消費者が多い価格だと解釈できる。 本節で述べた通り、13.2 節でも確認した PSM の分析結果では受容価格と内的参照価格に基づいて提示される価格水準や価格帯である。PSMは調査方法も（Rによる）分析方法も簡便であるが、それだけに結果の解釈については注意が必要である。例えば、PSMの分析結果において「最適価格」という言葉が用いられるが、この価格水準は、受容する人が最も多いことを示しており、他の文脈で用いられうる「最適」を意味しないことに注意が必要である。同様に、その他の価格帯・水準についても受容価格帯と内的参照価格帯との関連から解釈することが可能になる。PSMを利用する際には、その前提となる知識に基づき、結果に対する適切な解釈を行うことが重要になる。 "],["集計方法に関する注意点.html", "13.6 集計方法に関する注意点", " 13.6 集計方法に関する注意点 これまで紹介したPSMアプローチは、Westendorp (1976) によって提唱された方式であるが、、Newton et al. (1993) によって提唱された別のアプローチも広く採用されている。Newton et al. (1993) による方式（NMS型）でも、これまで説明した方式と同様の4つの質問を用いて最適価格、無差別価格、最高価格、最低価格を提示する。しかしながら、NMS型においては「高いと感じる」と「安いと感じる」消費者の比率の余事象は取らずに分析を行う。NMS型は広く用いられているものの、Newton et al. (1993) が誤った方法を紹介したことを起点に広まったとされている（兼子，2014）。 NMS型の PSM を用いると、最適価格と無差別価格は Westendorp 型の方式と等しい結果を得るが、最低価格と最高価格については異なる結果を得る。どちらの数値が正しいのかという点については議論があるものの、NMSは各曲線の交点についての論理的意味が見出しにくいという欠点を有している（兼子，2014）。 NMS型の考え方を簡易的に図示化する。安いおよび高いと感じる曲線を 1 (もしくは100%) から引く形で反転させない場合、最低価格（\\(\\underline{P}\\)）は「高い」曲線（青い破線）と「安すぎる」曲線（赤い実線）との交点で求められる。これは、先述の図13.5 で示されている \\(P_4-P_2\\) の範囲に感じている回答者が多くなる価格を表している。 同様に、最高価格（\\(\\bar{P}\\)）は安い曲線（赤い破線）と高すぎる曲線（青い実線）の交点で求められる。これは、\\(P_3-P_1\\) の範囲に感じている回答者が多くなる価格を表している。これらのことから、NMS型では、最高価格と最低価格の解釈が困難になると考えられる（兼子，2014）。NMS型でも前節までに説明したものと似た結果を得ることはできるのだが、その結果が既存の価格概念と整合的かつ直感的かという点に関して、問題を有している。 Figure 13.7: NMS型概要図 オンライン上では、NMS型のPSMを紹介している記事も散見される。実務的にもNMS型の方法を用いる場合もあるが、PSMの背後にある受容価格と内的参照価格に関する理論的基盤を鑑みると、Westendorp (1976) 方式のPSMアプローチのほうが、より実務的な含意を得やすい明示的な方法であると考えられる。PSMは使用の容易さから広く用いられているアプローチではあるが、本資料を読んだ学生においては、その背後に想定されている「受容価格」と「内的参照価格」という価格概念を理解したうえで活用してほしい。 "],["練習問題-9.html", "13.7 練習問題", " 13.7 練習問題 “MktRes_PSM.xlsx”データを用いて、価格感度測定を実行してみよう。このデータは、東京と神戸エリアの学生を対象に、一般的なラーメン店を想定して、価格についての価値観を回答してもらったという架空のデータセットである。このデータセットには、以下の情報が含まれている： Q1_1: 「安すぎる」価格 Q1_2: 「安い」価格 Q1_3: 「高すぎる」価格 Q1_4: 「高い」価格 Q1-5: 回答者の性別（1:女性、2: 男性、3: その他、4: 無回答） Q1-6: 回答者の年齢 価格以外の情報も使いながら、色々なインサイトを得るようデータを探索してみよう。 "],["参考文献-11.html", "13.8 参考文献", " 13.8 参考文献 Gabor, A., &amp; Granger, C. W. J. (1966). Price as an Indicator of Quality: Report on an Enquiry. Economica, 33(129) 43-70. 兼子良久 (2014). 「Westendorpのprice sensitivity Meterに関する考察-活用上の注意点と粗問題-」, 鹿児島経済論集, 54(1-3), 29-60. 岸邦宏・内田賢悦・佐藤馨一 (1999). 「航空運賃に対する利用者の価格感度に関する研究」, 土木計画学研究・論文集, 16, 187-194. Van Westendorp, P (1976) NSS-Price Sensitivity Meter (PSM) – A new approach to study consumer perception of price. Proceedings of the 29th ESOMAR Congress, 139-167. Online available at https://www.researchworld.com/a-new-approach-to-study-consumer-perception-of-price/. Newton, D, Miller, J, Smith, P, (1993) A market acceptance extension to traditional price sensitivity measurement. Proceedings of the American Marketing Association Advanced Research Techniques Forum. "],["consumer.html", "Chapter 14 コラム1（消費者行動研究の系譜概観）", " Chapter 14 コラム1（消費者行動研究の系譜概観） 市場を構成する重要なプレイヤーである消費者がどのような行動を取るのかという点もマーケティングにおいて重要な問題である。そのため、マーケティングリサーチでは、消費者を対象にした調査を行うことも多い。また、このような消費者の行動を捉えることに集中した研究領域を消費者行動（論）という。 ここでは、消費者行動研究の3つの主要な系譜を紹介する。第1に1950年代に盛んに行われた消費者の購買動機の探求である。これは、消費者がなぜ製品を購買するのかという疑問から出発した研究アプローチである。なぜ購買したのかという点は消費者の理解のためにとても重要だが、直接的な観測や測定が困難である。そのためこの系譜では、モチベーションリサーチと言われる、消費者の前意識・潜在意識や無意識といった深層心理へ接近するための精神分析学的手法による調査研究が行われてきた。例えば、深層面接法というある事柄についての深層心理を探り出すための個別面接の技法や、投影法と呼ばれる、表面化しにくい消費者の観念や感情を回答に投影させる技法が用いられてきた。しかしながら、方法論的な問題点が指摘されることであまりこのアプローチが採用されることは少なくなった。最も大きな問題は、分析の結果が調査者の能力や直感に依存してしまうことである。これは分析に内包する恣意性を排除できないという大きな問題として指摘された。 また、調査にかかる時間と費用が非常に高いことも実践上の問題であった。アンケート調査などに比べ、各回答者に対する調査コストが高く、調査回答者数の確保が困難であることも本アプローチの障害となった。 次のアプローチは、消費者行動の測定と分析、予測である。このアプローチでは、消費者の観察可能な行動に着目し、消費者の選択行動などを定量的に分析しその結果を用いた行動の予測を行う。このような枠組みは刺激に対する反応に注目するため、消費者行動研究領域では、S(Stimulus)-R(Response)アプローチと言われる。このアプローチの最初期には、調査会社や新聞社によって設置された買い物日記式の消費者パネルデータを用いて分析が行われていた。まず、調査に継続参加する消費者に、いつ、どこで、何を買ったかという購買データを記録させる。そして日記データを製品カテゴリー別に購買履歴データとして加工することで、選択に関する分析が可能であった。このアプローチは1960年代から特に盛んになり、その後ブランド選択行動を確率モデルによって定式化するアプローチが隆盛した。特に、選択行動の分析結果から製品の選択確率（選択のサレやすさ）や市場シェアを予測することも可能になったことや、消費者の個別取引などのデータが普及したことから様々な計量モデルへと発展した。なぜ買うかという頭のなかで起きていることは研究者にとっては観察不可であるため、このアプローチでは分析対象を「観察可能」な顕在的行動に限定し、定量的に研究を行おうと試みている。また、このアプローチの発展においては経済学、オペレーションズ・リサーチ（OR）分野の影響が強く、近年のマーケティング・サイエンス分野への発展と応用につながっている。なお本アプローチの詳細や理論的前提については 9 章にて説明する。 顕在的行動に着目したアプローチが発展する一方で、1970年代以降、心理学の発展を受け、行動の前提となる「態度」などの消費者の内的なプロセスの解明のためのアプローチも盛んになった。これは、消費者の顕在的な行動の前提となる原因が、（頭の中の）心理的プロセスとして検証可能であるとするアプローチであり、S-Rアプローチが心理的・内的なプロセスをブラックボックスとして扱っていたことを問題視する形で、心理学分野における学習理論、認知理論や探索理論の発展に伴い、有力な理論枠組みとして扱われるようになった。この枠組みはS-O (Organism)-Rアプローチと言われ、ここではどうして購買に至るのかという消費者の情報処理プロセスに注目している。特に、このアプローチでは、人間（消費者）を１つの情報処理システムとして捉え、情報の探索、取得、解釈、統合する内的な情報処理プロセスに焦点を当てている。このアプローチは、認知心理学の影響を強く受けており、近年の消費者行動研究の中心的なアプローチである。本書の中では、アンケート設計（4）や因子分析（12）、価格感度測定（13）などで、心理学的な知識を用いた議論や手法を紹介する。 "],["参考文献-12.html", "14.1 参考文献", " 14.1 参考文献 青木幸弘（2010）『消費者行動の知識』, 日本経済新聞出版社. 池尾恭一, 青木幸弘, 南知惠子, 井上哲浩（2010）『マーケティング』, 有斐閣. "],["causation1.html", "Chapter 15 コラム2-1（マーケティングにおける因果推論導入）", " Chapter 15 コラム2-1（マーケティングにおける因果推論導入） 近年の学術的なマーケティング研究では、因果効果を識別することの重要性が高まり、採用されるアプローチについても大きな変化が生じている。2000年代から2010年代前半までのマーケティング研究では、質問紙調査によって収集したデータを、共分散構造分析（Structural Equation Model; SEM）によって分析するという方法が多く取られていた。SEMでは一般的に、観測可能な変数（質問尺度）の背景にある構成概念同士の関係をパス図として示し、その相関関係を計測することでそのパス図の尤もらしさをチェックするという方法が取られた。しかしながら、2010年代後半以降、トップジャーナルに掲載される研究を中心に、特定の変数間の関係についてその因果関係を識別するという目的のもと分析を行うアプローチが主流になってきている。 実務的にも、因果関係の着目は重要になりつつある。マーケティング活動(プロモーション等)の効果を特定するのは意外と難しい。例え、「プロモーションを実施した年度の売上と、その前年度の売上を比較する」といった分析を行うことでプロモーション効果を捉えようとするかもしれない。しかし、前年比ではきちんとプロモーションの効果を抽出できていないかもしれない。この点を理解するために、相関関係と因果関係を区別することが重要になる。 相関関係とは、2 つの変数 \\(X\\) と \\(Y\\) の間に、比例や反比例といった関係があることを指す。一方で因果関係ではどちらが先行要因であるかという前後関係が重要視される。具体的には、要因\\(X\\)を変化させることで要因\\(Y\\)が変化する時、\\(X\\)を原因で\\(Y\\)を結果とする因果関係があるという（立森, 2016）。つまり、\\(X\\) が変化した「から」 \\(Y\\) が変化したという関係が因果関係であると言える。 相関関係を因果関係とを区別して認識するためには、主に以下の 2 つの可能性に注意が必要である（中室・津川, 2017）。ここで、仮に企業の年間広告支出額と年間売上との間に正の相関関係が確認できたとする。その際に気をつけるべき1つめの可能性は「逆方向の因果」である。これは、売上が高いから広告支出ができているという因果関係である。この場合、想定とは逆の因果が成立しており、成果が高く余力がある企業が広告に投資できる状態だと解釈でき、広告によって売上が上がるという状況ではない。2 つめの可能性は交絡因子の存在による「疑似相関」である。第3の要素が因果関係を構成する要素の両方に影響を与える場合、その第3の要素を「交絡因子」と言う。例えば、広告と売上の両方に影響を与える別の要素（例えば企業の志向など）が存在している場合、そのような志向を持つ企業が高い広告支出と売上を観測しており、広告と売上との間に因果関係があるかのように見せているかもしれない。このように、本当は因果関係がないのに交絡因子によって観察される相関関係のことを疑似相関と呼ぶ。 ただし、「真の因果効果」なるものは我々の頭の中にしか存在せず、特定することは不可能であるということに注意が必要である。ある主体（例、企業）が選択肢B（プロモーションしない）ではなく、A（プロモーションする）を選ぶことの因果効果は、その人がある選択肢Aを選び、成果（売上）を確認した後、時間を戻し選択肢Bを選んだ場合の成果を改めて確認することで初めて確認することができる。 しかし、そのような方法は現実的ではないので、実際には「平均的な効果としての因果関係」をあの手この手を使って追求していくことになる。また、もし 2 つの変数間に因果関係がある場合、その影響の程度を「因果効果」と呼ぶ。このようにして平均的な因果関係を識別することを目的とした分析アプローチは因果推論と呼ばれ、近年多くのマーケティング研究に応用されている。 因果推論は経済学や公衆衛生の領域を中心に手法が発展、応用されており、日本においては一般向けの書籍から研究者向けの専門書まで幅広く出版されている。因果推論に関する詳細な説明はこれらの図書に譲るものの、近年の因果推論の重要性はマーケティング領域としても無視できない。そのため、以降のコラムを通じて因果推論における主要な方法とそのマーケティング研究への応用例を紹介していく。具体的には、ランダム化比較試験、操作変数法、差の差の分析法について紹介する。なお、因果推論のアプローチとしては、回帰不連続法や、マッチング法、傾向スコアを用いた手法も多く活用されている。本書では紙幅の都合上これらの方法について言及しないので、関心のある方は川口・澤田（2024）や西山ほか（2019）を参照してほしい。 "],["参考文献-13.html", "15.1 参考文献", " 15.1 参考文献 川口康平・澤田真行 (2024)「因果推論の計量経済学」，日本評論社. 立森久照（2016）「因果推論ことはじめ」『岩波データサイエンス vol.3』, 7-27. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. 中村牧子・津川友介（2017）『「原因と結果」の経済学 データから真実を見抜く思考法』,ダイヤモンド社. "],["causation2.html", "Chapter 16 コラム2-2（ランダム化比較試験とカゴ落ちメール） ", " Chapter 16 コラム2-2（ランダム化比較試験とカゴ落ちメール） "],["ランダム化比較試験概要.html", "16.1 ランダム化比較試験概要", " 16.1 ランダム化比較試験概要 前のコラム（15）にて、相関関係と因果関係を区別するためには、主に逆方向の因果関係と疑似相関に注意が必要であると話した。また、真の（個人レベルでの）因果関係は特定できず、我々があくまで平均的な因果効果について分析を通じて推察するということも強調した。 因果効果を測定するうえで最も基本的な手法が、原因にあたる施策や選択を処置として調査対象者にランダムで割り当て、その後結果変数についての差を検討する、ランダム化比較試験（Randomized Controlled Trial; RCT）という手法である。RCTでは、原因について対象者に選ばせることなく、観察者（研究者）が確率的に均等になるように介入（例、くじ引きや乱数を利用）し処置を割り当てる。 先行要因をランダムに割り当てる一番の目的は、「セレクション（自己選択）バイアス」を除去することにある。セレクションバイアスは、処置のなかったグループの結果変数の平均値（観察可） と、処置群が非処置に変化したときの結果の平均値（観察不可）の差として定義される。言い換えると、処置に対する個人の選択によって生じるバイアスと考えられる。処置が個人の意思決定に任せられる場合、処置を受けるような人たちと受けない人たちとがそれぞれ持っている特徴によって差が生じてしまう可能性がある。例えば、栄養サプリメントが健康状態へ与える効果を検証したいと考える場合に、自主的にサプリメントを摂取しているか否かを処置の有無として捉えてしまうと、自主的にサプリメントを摂取している人がもつ特徴（健康志向など）が結果（健康状態）に影響を与えてしまう可能性がある。また、このバイアスはその定義上数値として観察できるものではない。 セレクションバイアスが発生しないような状況を人工的に作り、単純なグループ間の比較によってセレクションバイアスを除いた平均的な因果効果を分析しようと試みるための代表的な方法がRCTである。RCTによって処置をランダムに課した場合、処置が成果と独立であると仮定することができる。そのため、以下の重要な仮定を満たせば、RCTと平均の差の比較を通じて、平均因果効果を推定できると考える。 Positivity 個人が処置される確率（\\(P(T_i=1)\\)）が \\(0\\leq P(T_i=1)\\leq 1\\) を満たす。 すべての個人に対して処置群に割り当てられる可能性が与えられてることを意味する。 Independence Assumption 処置状況は潜在的な成果とは統計的に独立である。 この仮定によって、観測できない潜在的な結果をデータから分析することを可能になる。 Stable Unit Treatment Value Assumption (SUTVA) 以下の2つの性質に分けられる： No interference（干渉はなし） 個人 i の処置状況は、他の個人の処置状況には影響しない。 処置効果のスピルオーバーがない状況を想定している。 No hidden variations of treatment 各個人 i に対して、（異なる潜在的成果につながる）処置のバリエーションはない。 これは、同一の処置条件である状況を想定していて、異質または曖昧な処置定義（例えば薬の処方量など）は不適切であることを意味している。 RCTを実社会で行うことを「フィールド実験」と呼び、マーケティングを含む様々な領域で利用されている。しかしながら、実践的にはただ闇雲に調査対象をランダムにグループ分けして処置の有無を決定すれば良いわけではなく、「誰と誰を比較しているのか」という点を明確にし、実務的・学術的に意義のある比較を行う必要がある。この、「誰と誰を比較するのか」という点の重要性を強調するために本コラムではLi et al. (2021)による研究を紹介する。 "],["li-et-al.-2021-によるカゴ落ちメールに関するフィールド実験.html", "16.2 Li et al. (2021) によるカゴ落ちメールに関するフィールド実験", " 16.2 Li et al. (2021) によるカゴ落ちメールに関するフィールド実験 これを読んでいる皆さんは、Eコマース利用時にショッピングカートに製品を入れたが購入手続きをせずにそのまま放置してしまった経験はないだろうか？このような消費者行動は cart abandonment（カート放棄またはカゴ落ち）と呼ばれ、多くのEコマース客が行っている行為だということが知られている。 これに対して企業側は、E-commerce cart retargeting（ECR）と呼ばれるリマインダー（カゴ落ち製品があることのお知らせ）を当該客に送ることでカゴ落ち製品の購買を促す方策を取っている。このECRについて実務的に言われていたのが、「通知は早いほうが良い」というRecency bumpと呼ばれる原則である。この原則に基づき、従来では早いリマインダー広告は、より多くのクリック反応やサイトへの再来につながると考えられていた。しかし、これは本当だろうか、というのが Li et al. (2021) の論点である。 Li et al. (2021) はRecency bumpの強調に対して、理論的・方法論的批判を行う形で研究課題を設定している。理論的批判として、既存の理解ではECRの負の側面（広告への苛立ち）が検討されていないことを挙げた。早いリマインダーによって顧客がカゴ落ち品の購入を辞める原因になる可能性もあるが、既存の知見ではその点を捉えきれていない。方法論的批判としては、Recency bumpの根拠となっている知見が用いている手法の問題点を指摘している。既存の方法では、以下の図のようにすべてのカゴ落ちしている顧客のプールを調査対象者の全体とし、このプールに対してリマインダー（ECR)のタイミングをランダム化している。うまり、カゴ落ちしている顧客のうち、どのタイミングでECRを受け取るかがランダム化されており、30分後にECRを受け取る顧客もいれば、72時間後に受け取る顧客もいるという設計になっている。 既存のランダム化フレームワーク つまり、既存の方法では、30分以内にECRを受け取ったグループと3日後（72時間後）にECRを受け取ったグループとで反応率を比較しているということになる。しかしながらこれはフェアな比較なのだろうか。カゴ落ちから3日経っているということは、3日間その製品を放置するという選択肢を選んでいることになり、そもそも買う可能性が低いグループかもしれない。一方でカゴ落ちから30分以内の顧客には、まだ買うことを意図している顧客も含まれているかもしれない。そのため、既存の方法はランダム化されているものの、セレクションバイアスを排除できていない可能性が残っている。 このような問題に対して Li et al. (2021) では、以下の図のように、それぞれのタイミンググループで、処置（ECRあり）群とコントロール群（ECRなし）群をランダムに分ける方法でRCTを実施した。 Li et al. (2021) のRCTフレームワーク つまり、カゴ落ち顧客を特定の時間幅ごとに分類し、その時間幅ごとに処置群とコントロール群とをランダム化する方法をとっている。具体的には、カゴ落ち後30分以内の顧客群の中で、ECRを受ける顧客と受けない顧客をランダムで分けるランダム化に加え、カゴ落ち後72時間の顧客群の中でECRについてランダムで割り当てるという実験デザインを用いている。 この論文は、このような方法を用いることで既存の知見が内包していたバイアスの可能性を補正した形でECRの効果を検証した。この方法は方法論的な改善だけではなく、より具体的な理論的発見及び実務的含意を得ることにもつながっている。著者たちは、Memory decay theory （忘却理論）を適用し、製品に関する記憶は時間とともに消えることから、ECRによる記憶の再生と広告への苛立ちの効果が時間経過によって変化することを予測し、それと整合的な結果を得た。具体的には、ECRの効果は送付されるタイミングによって異質であり、早いタイミング（0.5または1時間）では、ECRがより低い購買確率につながる一方で、遅いタイミング（24または72時間）では、リマインダーがより高い購買確率につながることが示された。これは、ECRに関する正負両方の効果を内包したメカニズムの解明であり、大きな理論的貢献を有している。また、マーケティング意思決定者が、従来強調されていた recency bump について盲信することへの警鐘を鳴らし、時間を置いてからのECR送付を推奨するという実務的含意も有している。 フィールド実験を行うことは、企業の分析担当者にとっては実現可能性が高い方法かもしれない。日々のオペレーションの中で蓄積される顧客データを用いて、ランダム化された処置の効果を測定することが可能であると考える実務家も多いだろう。しかしながら、リサーチデザインについては慎重に考える必要がある。特に、「誰と誰を比較するのか」という比較群の定義については慎重に検討する必要がある。自身が想定していないバイアスが内包されている調査デザインになっていないか（きちんと識別ができるデザインになっているか）、について他の研究者と議論することも非常に重要である。そのため、より効果的かつ意義のあるRCTを実行するためには、手法その分野（例えば、マーケティング）に関する理論への理解が重要であると考えられる。 "],["参考文献-14.html", "16.3 参考文献", " 16.3 参考文献 Li, J., Luo, X., Lu, X., &amp; Moriguchi, T. (2021). The Double-Edged Effects of E-Commerce Cart Retargeting: Does Retargeting Too Early Backfire?, Journal of Marketing,85 (4), 123-140. "],["causation3.html", "Chapter 17 コラム3（非遵守と操作変数法）", " Chapter 17 コラム3（非遵守と操作変数法） 因果推論の代表的な方法として、コラム16 ではRCTを実施した。しかしながら、着目するリサーチクエスチョン次第では、被験者がランダムに割り当てた処置に従わないような状況も考えられる。例えば、企業がクーポンが購買確率や購買額に与える影響について分析したいと考える際、自社の保有している顧客個人情報のプールから各顧客をランダムにクーポンを受け取るグループ（処置群）と受け取らないグループ（コントロール群）とに割りあて、実験を行うような方法が考えられる。しかしながら、実際に処置群の全ての顧客がクーポンを使うとは限らない。 このような非遵守（ランダムな割り当てと処置の有無が必ずしも一貫しない）の問題を含む状況においては、通常のRCTデザインを仮定した平均の差に関する分析では平均因果効果を分析できない。このような状況においては、「操作変数（Instrumental variable: IV）法」と呼ばれる手法を用いることで、ランダムな割り当てと処置が一貫している遵守者に着目した局所平均因果効果（Local Average Treatment Effect: LATE）を推定できることが知られている（川口・澤田, 2024）。一方でIV法は、因果推論の文脈以外でも活用されてきた方法である。特に、経済学を中心に議論が展開され、マーケティング領域においても広く用いられている手法である（例えば、Lo et al., 2012; Hui et al., 2013）。そのため因果推論に限らないIV法そのものの理解もマーケティング研究においては重要である。 これらの背景を踏まえここでは、IV法を用いた因果推論研究の内容を詳細に伝えるのではなく、より一般的なIV法についての知識を提供することを目的とする。具体的には、IV法の概要について説明した後マーケティングの文脈を例に取り、非遵守者が生じる状況におけるIV法の応用について紹介する。 "],["操作変数法の概要.html", "17.1 操作変数法の概要", " 17.1 操作変数法の概要 操作変数法は、分析モデルにおける内生変数（endogenous variable）によって生じるバイアスに対応することに主眼を置いた手法として活用されてきた。内生変数とは、回帰モデルにおいて誤差項と相関を持っているような説明変数のことである42。 例えば、以下の回帰モデルにおいて説明変数と誤差項との間に相関がある（\\(Cov(x,u)\\neq0\\)）場合、回帰係数の推定量にバイアスが生じてしまう。この問題を内生性（endgogeneity）と呼ぶ。 \\[ Y_i=\\beta_0+\\beta_1X_i+\\epsilon_i \\] 内生性の生じる原因には、（1）欠落変数、（2）同時性（説明変数と被説明変数に両方向の因果関係がある）、（3）測定誤差、（4）セレクションバイアスがある。このような問題に対して、 IVと呼ばれる変数を用いた推定を行うことで、対応することが多い。今回は主に操作変数法の中でも最も広く使われている推定方法の一つである二段階最小二乗法（2SLS）という方法について説明する。 IVは、以下の条件を満たす変数のことを指す。 操作変数 \\(Z\\) は内生変数 \\(X\\) の先行要因になっている \\(Z\\) は元々の回帰式における誤差項 \\(\\epsilon\\) と相関しない。 この条件は、以下の図のように表すことができる。 ここでは簡単化のために単回帰モデルを想定して、２SLSによるIV法について説明する。２SLSでは、その名の通り、操作変数 \\(Z\\)を用いた以下のような二段階の推定を行うことで、内生性を克服した分析を試みる： \\(Z\\) を \\(X\\)に回帰する： \\(X_i = \\gamma_0+ \\gamma_1Z_i+u_i\\) 1で得た\\(X\\)の予測値 \\(\\hat{X}\\) を \\(Y\\) に回帰する。 \\(Y_i= \\theta_0+\\theta_1 \\hat{X}_i+\\epsilon_i\\) \\(\\hat{X}\\) は \\(\\epsilon\\) とは相関を持たないと仮定される \\(Z\\) により説明される \\(X\\) を捉えている。そのため以下の図で示されるように、2段階目で用いられる\\(\\hat{X}\\) は操作変数 \\(Z\\) を通じてのみ説明される内生変数 \\(X\\) の変動を抽出したものであると直感的には理解できる。 2SLSイメージ このようなプロセスによってIV法を用いて内生性（誤差項との相関）を克服することが広く用いられる。また、IV法においては、内生性の克服という統計的性質の他にも、理論的経路の判断に応用されるという特徴を持つ。2SLSによる推定では\\(Z\\rightarrow X\\rightarrow Y\\) という経路が推定されている。そのため、どのような経緯（\\(Z\\)）で変化した \\(X\\) が \\(Y\\) に影響を与えるのかというストーリーについての含意を得ることも、研究上では活用される。しかしながら、実際の分析においては、採用した操作変数が本当に誤差項と相関を持たないのか、操作変数と内生変数の相関は十分に強いのか等、実践的に気をつけるべきポイントがいくつかあり、有効な操作変数を見つけ、分析を行うことは容易ではない。詳細については西山ほか（2019）など計量経済学の教科書を参照してほしい。 内生変数はもともと、数理的な経済学理論モデルにおいてモデル内で定まる変数のことを指していた。それに対してモデルの外部で決まる外生的な変数は外生変数と呼ばれていた。この内生変数を実証的に分析する際に生じる統計的な問題について議論が洗練化していく過程で、内生変数は「誤差項との相関」に基づき定義されるようになった（西山ほか, 2019）↩︎ "],["実験非遵守における操作変数法の活用.html", "17.2 実験非遵守における操作変数法の活用", " 17.2 実験非遵守における操作変数法の活用 本節冒頭で述べた通り、クーポンに関する実験のように実験の参加者がランダムに割り当てられた処置に必ず従うとは限らないような状況を考える。例えば、クーポンの配布をランダムに割り当て、1ヶ月間の購買状況を分析するような研究を考える。この時、クーポンを受け取っても利用しない個人もいれば、コントロール群にいながら別の方法でクーポンを入手し利用する個人もいるかも知れない。つまり、ランダム割り当て（\\(T_i=1\\): 処置群; \\(T_i=0\\): コントロール群）と実際のクーポン利用の有無（\\(D_i=1\\): クーポンあり; \\(D_i=0\\): クーポンなし）が一致しないような状況だと考えられる。 この時、\\(T\\) と \\(D\\) の関係から人々を以下の4つのタイプに分類することができる： 常に利用者（Always Takers: AT): \\(D_i=1~\\text{when}~T_i=0~\\text{or}~T_i=1\\) 常に非利用者（Never Takers: NT）: \\(D_i=0~\\text{when}~T_i=0~\\text{or}~T_i=1\\) 遵守者（Compliers: C）: \\(D_i=0~\\text{when}~T_i=0;~~D_i=1~\\text{when}~T_i=1\\) 天邪鬼（Defiers: De）: \\(D_i=0~\\text{when}~T_i=1;~~D_i=1~\\text{when}~T_i=0\\) この時、\\(T\\)を操作変数\\(D\\) を内生変数とする IV法の枠組みで分析することを考える。前節で参照したように、ランダム化（\\(T\\)）を通じてクーポン利用を決めた（\\(D\\)）個人の成果変数（\\(Y\\)）について考える。そのためには、IVについての仮定（\\(Cov(Z,X)\\neq0\\), \\(Cov(Z,\\epsilon)=0\\)）、除外制約（\\(Z\\)は\\(X\\)を通じてしか\\(Y\\)に影響しない）に加えて、単調性（monotonicity）が満たされると仮定する必要がある。単調性とは、\\(D_i(T_i = 1)\\geq D_i(T_i = 0)\\) と定義され、操作変数（ランダム割り当て）と内生変数（クーポンの利用）との関係には一貫性があることを示唆している。言い換えると、De のようにクーポンが割り当てられた（抽選にあたった）場合にはクーポンを利用せず、割り当てられなかった場合にはクーポンを利用するというような、天邪鬼な行動は取らないという仮定である。このような仮定が理論的ないしは現実的に満たされる場合、\\(T\\) と \\(D\\) の関係は以下の表のように示される。 Table 17.1: 仮定のもとでのTとDの関係 \\(D = 1\\) \\(D = 0\\) \\(T = 0\\) AT C or AT \\(T = 1\\) C or NT NT また、操作変数に関する仮定より、操作変数（\\(T\\)）は内生変数（\\(D\\)）を通じてのみ成果へ影響する（\\(Y\\)）。そのため、AT や NT のようにつねにクーポン利用行動について取る値が決まっている個人についてはランダム化されたクーポンの効果（観察可能な平均因果効果）はゼロになる。 これらの仮定を満たし、IVが2値である場合（例えば、ランダム割り当ての有無）におけるIV推定モデルを以下のように考える： \\(D_i=\\gamma_0+\\gamma_1T_i+u_i\\) \\(Y_i=\\beta_0+\\beta_1\\hat{D}_i+\\epsilon_i\\) このとき、\\(\\beta_1\\) は以下のように示される。 \\[ \\beta_1 = \\frac{E[Y_i|T_i=1]-E[Y_i|T_i=0]}{E[D_i|T_i=1]-E[D_i|T_i=0]}=E(Y_{1i}-Y_{0i}|D_{1i}-D_{0i}=1) \\] ただし、\\(D_{1i}-D_{0i}=1\\)はランダム割り当てというIV（\\(T\\)）に応じてクーポンの利用（\\(D\\)）を変化させた遵守者を示す条件であり、Deでは\\(D_{1i}-D_{0i}=-1\\)、ATやNTでは\\(D_{1i}-D_{0i}=0\\)になる。そのため、仮定を満たしたうえでIV推定をすることで、遵守者のみに着目した局所平均処置効果（LATE）を求めることができる。この性質は、遵守の強制が難しいトピックや状況における効果測定の際に役立つ手法であり、マーケティング領域での応用も期待できる。 "],["参考文献-15.html", "17.3 参考文献", " 17.3 参考文献 川口康平・澤田真行 (2024)「因果推論の計量経済学」，日本評論社. 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Hui, S. K., Inman, J. J., Huang, Y., &amp; Suher, J. (2013). The Effect of In-Store Travel Distance on Unplanned Spending: Applications to Mobile Promotion Strategies. Journal of Marketing, 77(2), 1-16. Lo, D., Ghosh, M., &amp; Lafontaine, F. (2011). The Incentive and Selection Roles of Sales Force Compensation Contracts. Journal of Marketing Research, 48(4), 781-798. "],["causation4.html", "Chapter 18 コラム4（パネルデータと固定効果：差の差の分析法準備）", " Chapter 18 コラム4（パネルデータと固定効果：差の差の分析法準備） これまでは、ランダム化比較試験（RCT）や操作変数法という、ランダム化された実験構造を取り入れた調査方法およびデータの活用を前提とした因果効果の分析を紹介してきた。しかしながらマーケティングおよび企業経営においてRCTを行うことが現実的ではない状況も多く存在する。例えば本節で紹介する研究では、Chief Marketing Officer の有無が企業成果に与える影響を分析している。しかしながら、このような研究者の目的のために、実在する複数の企業を用いてRCTを行う（ランダムにCMOを配置する企業を選び、それ以外には配置しない）ことは現実的ではない。 このように実験を行うことが現実的ではない状況において研究者はアクセス可能なデータや変数、分析手法を用いてできるだけバイアスを回避した結果を求める。本節では、そのような分析手法を理解するための準備として、パネルデータ分析における固定効果モデルを紹介する。固定効果モデルでは、時間で変化しない（データ上の時間を通じて一貫した）観測個体の特徴によって生じる被説明変数の変化をコントロールしたうえで分析を行うことができる。固定効果のコントロール自体、マーケティング領域で広く使われている。本節ではまず、いくつかのデータ構造の種類を紹介した後、固定効果モデルの概要を説明する。その後、研究紹介として Germann et al. (2015) による研究成果を整理する。 "],["データのタイプ.html", "18.1 データのタイプ", " 18.1 データのタイプ 本書で扱う分析手法は、ある特定の時点における複数の観測を捉えたデータセットに対する分析であった。このようなデータは横断面（クロスセクション）データと呼ばれ、7 で使用したような、特定年度における複数企業の財務データ（例、firmdata19）がその代表例である。そしてこのようなデータは以下のような構造を持っている。各企業が特定の年度（2019年）における観測をひとつずつ有している。 Table 18.1: クロスセクションデータ例 legalname fyear sales sga net_profit ハウステンボス株式会社 2019 808510 126594 13875 株式会社オリエンタルランド 2019 464450 66986 62217 株式会社東京ドーム 2019 91557 5789 8002 タリーズコーヒージャパン株式会社 2019 483360 212814 7945 株式会社 コメダ 2019 31219 4300 5369 株式会社サンマルクカフェ 2019 68908 49691 1486 一方で特定の対象を複数の時点にわたって観測することも可能である。このようなデータはパネルデータと呼ばれる。例えば、時系列データの例として株式会社オリエンタルランドの財務情報を2010年度から2019年度まで観察すると、以下のような構造で示すことができる。 Table 18.2: 時系列データ例 legalname fyear sales sga net_profit 株式会社オリエンタルランド 2010 356180 47428 22897 株式会社オリエンタルランド 2011 360060 44680 32105 株式会社オリエンタルランド 2012 395526 48113 51484 株式会社オリエンタルランド 2013 473572 58012 70571 株式会社オリエンタルランド 2014 466291 59762 72063 株式会社オリエンタルランド 2015 465353 63778 73928 上記のクロスセクションと時系列両方の側面を持つデータをパネルデータと呼ぶ。つまり、複数の対象に対して複数時点の観測を有するデータである。本書で取り込んだfirmdataはもともと、複数年度にわたる複数企業の情報を有したパネルデータである。パネルデータは以下のような構造を持つ。 Table 18.3: パネルデータ例 legalname fyear sales sga net_profit ハウステンボス株式会社 2011 380805 61158 8958 ハウステンボス株式会社 2012 431483 65654 10881 東武タウンソラマチ株式会社 2012 577223 125033 30448 株式会社オリエンタルランド 2010 356180 47428 22897 株式会社オリエンタルランド 2011 360060 44680 32105 株式会社オリエンタルランド 2012 395526 48113 51484 株式会社東京ドーム 2011 73208 5943 362 株式会社東京ドーム 2012 80763 5810 3914 タリーズコーヒージャパン株式会社 2010 351692 156020 7681 タリーズコーヒージャパン株式会社 2011 369284 158164 9235 パネルデータは各対象に関する異なる年次の観測を、行を増やす形で縦長に構成する。このようなデータの形式をロング型（5章参照）と呼び、分析においてはロング型のデータを用いる。ここでは、回帰分析における欠落変数問題を回避しうる方法としてパネルデータ分析について紹介する。 "],["パネルデータ分析と固定効果.html", "18.2 パネルデータ分析と固定効果", " 18.2 パネルデータ分析と固定効果 パネルデータにおける線形回帰モデルは以下のように示される（任意の\\(k\\)個の説明変数を含むモデルに拡張可能である）。 \\[ Y_{it}=\\beta_0+\\beta_1X_{1it}+u_{it} \\] ただし、\\(Y_{it}\\) 観測（個人や企業）\\(i\\)（\\(i=1,...,N\\)）、時点 \\(t\\)（\\(t=1,..,T\\)）の被説明変数、\\(X_{1it}\\) は観測 \\(i\\)、時点 \\(t\\) の説明変数を表す。このようなパネルデータに基づく回帰モデルが利用可能な場合、変数を追加することなく欠落変数バイアスに対応することができる。具体的には、時間で変化しない観測対象固有の特徴については追加的に変数を観測することなく、回帰分析上でコントロールすることができる。 例えば、自社顧客1,000人の購買行動に関する月次データを1年分持っている場合、合計で12,000観測（個人-月）のデータを得たことになる。このとき、顧客個人の価値観のような情報も購買行動に影響を与えうるため、欠落変数バイアスの可能性が存在する。価値観のような情報は心理尺度を利用したアンケート結果と接合しなければ観察は難しく、顧客の購買行動に紐づいたパネルデータとの接合は容易ではない。しかしながら、個人の根本的な価値観が1年間では変化しないと考えられるのであれば、パネルデータを利用した分析によって価値観による欠落変数バイアスを避けることができる。ただし、価値観が時間不変の個人特性であるという点については、理論的・現実的観点から議論する必要がある。パネルデータによって回避できる欠落変数バイアスは時間不変の個人（観測個体）特性についてのみである。 上記のような観測個体に対して時間を通じて一定な（しかし観察できない）変数を固定効果（fixed effects）と呼ぶ。固定効果モデルは以下のように示すことができる。 \\[ Y_{it}=\\mu_i+\\beta_1X_{1it}+u_{it} \\] このモデルには、\\(\\mu_i\\) という項が入っている。この項には \\(i\\) という添字のみが入っており、観測個体ごとに異なるが、時間を通じて一定であることが示されている。この項には被説明変数に影響を与える時間不変の要素がまとめて含まれており、先述の価値観もその他の（\\(Y_{it}\\)に影響を与える）時間不変の個人特性もこの中に含まれている。 続いて、固定効果を用いたモデルの推定量（固定効果推定量）の求め方とその解釈について説明する。固定効果モデルの推定では、固定効果を除去したモデルに変換したうえで、そのモデルをOLS推定する（西山ほか, 2019）。モデルの変換においては、被説明変数、説明変数と誤差項について観測個体ごとの平均を取る。例えば、被説明変数であれば\\(\\bar{Y_i}=\\sum^T_{t=1}\\frac{Y_{it}}{T}\\)となる。同様に、\\(\\bar{X_i}\\) と \\(\\bar{u_i}\\) を得る。この個体ごとの平均値を元の回帰モデルから引くことで以下のような固定効果変換後のモデルを得る。 \\[ Y_{it}-\\bar{Y_i}=\\beta_1X_{1it}-\\bar{X_i}+u_{it}-\\bar{u_i} \\] 固定効果モデルに基づく分析では、このような着目する変数の持つ変動のうち、時間を通じて変化する変動のみを捉えた形（つまり、時間不変の固定効果は除去した形）で回帰モデルを推定することになる。 なお、固定効果推定量については、各観測個体に対するダミー変数を用いた別の解釈と計算方法も存在する。しかしながら、上記の固定効果変換のものと数学的には同値であることが知られている（Wooldridge, 2013）。例えば、観測個体 1 であれば 1 をとり、それ以外なら 0 を取るようなダミー変数（\\(D^1_i\\)）を作成する。同様のダミー変数を全ての観測個体に対応する形で作成し、モデルに含むことで、いかのようなダミー変数回帰モデルを得る。 \\[ Y_{it}=\\mu_1D^1_i+\\mu_2D^2_i+...+\\mu_ND^N_i+\\beta_1X_{1it}+u_{it} \\] この場合、各個体ごとに（それぞれのダミー変数に対応する形で）定数項がモデルに含まれる様になる。そして、このダミー変数回帰モデルによって推定されたOLS推定量は先述の固定効果推定量と等しくなることが知られている。 また、研究の実践では、個体の固定効果に加え、時間固定効果の両方を含むモデルを用いた分析も活用されている。この場合、全ての観察個体に共通している個体不変であり観察できないものの、被説明変数に影響を与える変数について時間固定効果としてコントロールすることが可能になる。例えば、経済や社会的なマクロトレンドなどの全ての観測個体に共通すると考えられる時系列変化による影響は、このモデルによってコントロールが可能になる。二方向の固定効果を含む回帰モデルは以下のように示すことができる。 \\[ Y_{it}=\\mu_i+\\lambda_t+\\beta_1X_{1it}+u_{it} \\] このとき、\\(\\lambda_t\\) が時間固定効果を表している。このような定式化を用いることで個体固定効果と時間固定効果の両方に関する欠落変数バイアスを（それが観察できないものであっても）回避できるという特性を持っている。 パネルデータ推定についての議論において、変量効果（Random effects）モデルというモデルも存在する。変量効果モデルは、固定効果と同様、\\(\\mu_i\\) という時間不変の個人レベルの要素を含むモデルである。しかし変量効果モデルは、この \\(\\mu_i\\) がモデル内の説明変数とは無相関であるという仮定を満たす場合のモデルである。言い換えると、\\(\\mu_i\\) が説明変数と無相関であるという性質を満たす場合、この効果は変量効果と呼ばれる。固定効果か変量効果かを判別するために統計的な分析を用いる方法が歴史的に採用されてきた。しかしながら、固定効果推定量は、固定効果と変量効果いずれの場合にも一致性を満たすことが知られている。さらに、変量効果と固定効果を判別するための統計的な分析手順に対する問題も提起されている（西山ほか, 2019）。そのため、多くの場合固定効果を用いて分析される。 変量効果を用いるのは、時間を通じて不変な要素が研究上重要な説明変数である場合である。そのような場合に固定効果モデルを採用すると、着目している変数の影響が固定効果に吸収されてしまう。つまり、研究上の問いから固定効果モデルが不適切である場合には、変量効果モデルを採用せざるを得ない。 "],["germann-et-al.-2015-によるcmoの影響に関する分析.html", "18.3 Germann et al. (2015) によるCMOの影響に関する分析", " 18.3 Germann et al. (2015) によるCMOの影響に関する分析 「企業の取締役会にChief Marketing Officer (CMO) は必要か？」という問いは、2000年代後半から2010年代にかけて広く議論が展開されてきた。これは、企業経営という広範な視点から見た際のマーケティングの重要性につながる重要な問いである。 CMOの存在が企業成果や価値につながる理由としては、（1）組織にてマーケティング的視点の影響力が強まることで企業レベルの基幹ビジネスプロセスと顧客の価値の創造・維持とを結びつけることにつながることや、（2）そのような意図や特徴を持っていることを伝えるシグナルとして機能する、と考えられた。しかしながら先行研究ではCMOの存在が企業価値（トービンのQ）へ与える影響については相反する結果が混在していた。特にNath and Mahajan (2008) は、CMOの存在による有意な効果が確認できなかった研究成果として非常に注目を集めた。 このような研究背景に対してGermann et al. (2015) は方法論の観点から先行研究の課題克服を試みた。具体的には、以下の三点からNath and Mahajan (2008) の拡張を試みた： 時間軸： 先行研究では、5年間のデータを使っていたが、CMOの効果より長期的な視点で観察すべきかもしれない。 Germann et al. (2018) では、先行研究の情報も内包する12年間のデータを収集した。 産業（サンプルサイズ）： 先行研究では除外されていたいくつかの産業を追加したより網羅的なデータセットの作成した 分析モデル： 先行研究では、Between-effects モデルと呼ばれる、各個体の時間を通じた平均値を用いた回帰モデルを推定していた。この方法では、パネルデータ構造を利用した推定方法を活かせていない。 Germann et al. (2015) では、Between-effects モデルに加え、様々な推定手法を用いた。 Germann et al. (2015) は、様々な推定方法についての解説も含む内容で構成されており、教育的な貢献も含まれる論文だと言える。その中でGermannでは、観察できない変数による効果を含むモデルとして固定効果モデルが紹介されている。その他にも、操作変数法（17）やランダム効果モデルに加え、ダイナミックパネルモデルを用いた一般化積率法（Generalized Method of Moment: GMM）も行っている。この方法は、モデルに被説明変数（\\(y_{it}\\)）のラグ項（\\(y_{it-1}\\)）を含むようなモデル化を行い、その特定化によって生じるバイアスについて、被説明変数と内生変数のラグ項を操作変数として用いつつGMMにより推定することで対応しようというものである（Germann et al., 2015）。 分析の結果、分析手法やデータセットによってCMOの有無が成果に与える影響が異なって推定されることが示された。より具体的には、先行研究で用いられたBetween-effectsでは全ての条件において有意な結果は確認できなかった。これは、各企業の時間を通じた平均値を計算することで、各企業内で生じた変化による影響を排除してしまうためではないかと考えられる。一方で、Nath and Mahajan (2008) と異なるデータセット（時間軸、サンプルサイズ）の場合、Between-effects を除く全ての推定方法でCMOによる有意に正の効果が確認された。また、Nath and Mahajan (2008)と同じ条件のデータセットを用いた分析でも、操作変数を含むような推定方法であればCMOによる有意な影響が確認された。しかしながら、このデータセットでは固定効果推定による有意な影響は確認されなかった。これは、CMOの存在が内生変数であることを示唆している。ダイナミックパネルによる効果の確認に加え、「同業他社におけるCMOの普及率」という操作変数（ピアプレッシャーを通じてCMOが配置されるというメカニズムを考慮している）の採用によってもCMOの効果は検出されている。 これらの結果を探索することによって得た実務的・学術的貢献は以下のものである。実務的貢献として、CMOの存在が企業価値に与える正の影響は（特定の推定方法やデータの取り方を除いて）頑健であり、CMOを活用している企業のトービンのQ値はそうでない企業に比べておおよそ15%高いことが示された。これは、企業経営におけるCMOの価値を示す重要な知見の一つであると言える。学術的貢献として、CMOに関わる概念的議論と手法の両面から示すことができる。この研究では、CMOの効果を検証したことに加え、CMOの存在が内生変数である（外生的とは言えない）ことも分析から示している。企業は何らかの理由に基づき、CMOを配置するか否かを判断していることになる。そのため、内生性に対応した分析においてはCMOの効果をより正確に検証できているとかbが得られる。そのうえで、同業他社からの影響を受けてCMOを配置するというメカニズムに基づくモデルも検証している。これらの知見は、CMOを巡るメカニズムと整合的な分析手法を考えるための学術的示唆として重要な知見である。 多くのCMO、ひいてはマーケティング部門の方々が取締役会や企業経営の視座における自身の価値についてアピールすることに苦労しているかもしれない。学術領域においても、マーケティングとファイナンスの接合点に着目し、マーケティングの価値を確認する研究が積極的に行われている（例えば、Mizik, 2010）。Germann et l. (2015) はこの研究成果を通じて、企業がCMOを取締役会に配置する意義を示す知見としてマーケティング実務家の存在感を高めることにつながることを期待していた。 "],["参考文献-16.html", "18.4 参考文献", " 18.4 参考文献 西山慶彦・新谷元嗣・川口大司・奥井亮（2019）「計量経済学」，有斐閣. Germann, F., Ebbes, P., &amp; Grewal, R. (2015). The Chief Marketing Officer Matters! Journal of Marketing, 79(3), 1-22. Mizik, N. (2010). The Theory and Practice of Myopic Management. Journal of Marketing Research, 47(4), 594-611. Nath, P., &amp; Mahajan, V. (2008). Chief Marketing Offi- cers: A Study of Their Presence in Firms’ Top Management Teams. Journal of Marketing, 72 (January), 65–81. "],["causation5.html", "Chapter 19 コラム5（差の差の分析法）", " Chapter 19 コラム5（差の差の分析法） 本節では、疑似実験と呼ばれる構造を捉えた、差の差の分析法という手法を紹介する。ランダム化ができない中でもデータの構造をうまく使い、ある仮定のもとで因果効果に迫るのが、因果推論と呼ばれる方法である。特に、完全なランダム化ではないが観測個体にとって処置の選択権がなかったような状況を疑似実験と呼ぶ。そして、この構造を活用した分析手法が本節で紹介する差の差の分析法（Difference-in-Differences: DID）である。 DIDは企業や政府の行動による構造変化や、自然などによる外生ショック（感染症、異常気象、災害）を利用した分析手法である。自身の研究課題に関連する構造変化や外生ショックを見つけることができれば、フィールド実験よりも実現可能性が高い方法として、マーケティング領域でも多くの研究で採用されている。 DIDを行うためには、研究上着目する処置がある個人やグループには割り当てられているがそれ以外には割り当てられていない状況を特定し、その状況に対応する観察対象者の処置前と処置後のデータを有する事が必要となる。つまり、複数の観察個体に対する複数時点のデータを得てパネルデータセットを構築する必要がある。 DIDを理解するためには、処置に与える変化、処置群と統制（コントロール）群の特定、処置前・処置後のデータという、構造を踏まえる必要がある。ここで、DIDの必要性を理解するために、（1）前後比較と（2）群間比較という比較方法について説明する。まず前後比較とは、処置を受けた個体群を集め、このグループの処置前と処置後の結果を比較する方法である。例えば、企業業績の前年比較は典型的な前後比較である。しかしながら、前後比較では処置とは関係のないマクロトレンドなど影響をコントロールできないという限界がある。例えば、企業の取り組みや変化にあたる処置の有無に関わらず、景気や気象条件などの環境変化が企業業績が影響を与えることも考えられる。そのような場合、前後比較では自社にとって有利な環境の変化と処置による効果を見誤ってしまう。 一方で群間比較は、処置を受けた群と受けていない群間で着目する変数の値を比較する方法である。しかしながらこの方法では、16 で紹介したセレクションバイアスによる影響を見過ごしてしまう。ランダム化を介した処置の割り当てができない場合、処置の有無と（潜在）成果変数とが独立ではなくなってしまい、因果効果を推定できない。 これらの問題に対応するためにDIDでは、以下の図で示すように「差の差」に着目する。DIDにおいては（A）処置群の事前データ、（A’）処置群の事後データ、（B）統制群の事前データ、（B’）統制群の事後データ、の４種類のデータを用いる。このデータのうち、それぞれの群における事前事後の差（\\(A-A&#39;\\) と \\(B-B&#39;\\)）を求めたあとに、これらの差の差（\\((A-A&#39;)-(B-B&#39;)\\)）を捉えることで平均処置効果を求める。 DIDの概要 処置郡内の前後比較（\\(A-A&#39;\\)）には、処置による効果に加え、セレクションバイアスは含まれていないがマクロトレンドなどに代表される時系列変化による影響が含まれている。そして処置による効果と時系列変化の影響はどちらも観察できない。一方で統制群内の前後比較（\\(B-B&#39;\\)）には処置による効果は含まれておらず、（他の要素を一定だとするならば）時系列変化による影響だけが含まれているはずである。そのため、差の差を捉えることで、前後比較（\\(A-A&#39;\\)）に含まれている情報のうち時系列変化に関する影響だけ排除し、処置による効果を抽出しようという方法である。下図における点Cは「もし処置がなかった場合の処置群の成果」を示しており、観察できない情報である。\\(A-A&#39;\\)から\\(B-B&#39;\\)を引くことで、仮想的に \\(A-C\\) を分析しようというのがDIDの直感的な説明である。 分析においては、処置群であれば1を取るような処置群ダミー（\\(D^{treat}_{i}\\)）と、処置後の観測であれば1を取るような事後ダミー（\\(D^{post}_{t}\\)）を用いて、以下のような回帰モデルを採用する： \\[ Y_{it}=\\alpha+\\beta(D^{treat}_{i}\\times D^{post}_{t})+\\gamma D^{treat}_{i}+\\delta D^{post}_{t}+u_{it} \\] 上記のモデルにおける係数\\(\\beta\\)を確認、検定することで疑似実験の効果についての知見を得る。 なお、分析においては 18 で紹介した、以下の二方向の固定効果（\\(\\mu_i\\)と\\(\\lambda_t\\)）を含む回帰モデルで推定することも可能である。 \\[ Y_{it}=\\mu_i+\\lambda_t+\\beta(D^{treat}_{i}\\times D^{post}_{t})+\\gamma D^{treat}_{i}+\\delta D^{post}_{t}+u_{it} \\] このとき、処置群ダミー（\\(D^{treat}_{i}\\)）と事後ダミー（\\(D^{post}_{t}\\)）はそれぞれ、観測個体固定効果（\\(\\mu_i\\)）と時間固定効果（\\(\\lambda_t\\)）に吸収されてしまうため、結果的に以下のモデルでDID推定を行うことができる。 \\[ Y_{it}=\\mu_i+\\lambda_t+\\beta(D^{treat}_{i}\\times D^{post}_{t})+u_{it} \\] このように、回帰モデルを用いてDID推定を行う場合には、パネルデータ分析、特に固定効果モデルに対する知識も必要になる。 なお、DIDも他の手法と同様、平均処置効果を得るためにはいくつかの仮定を満たす必要がある。具体的には、SUTVAなど（16 参照）因果推論に関わる基本的な仮定に加え、並行トレンド（Parallel trend）の仮定を満たす場合、DIDによって（処置群における）平均処置効果（Average Treatment Effects on Treated）を分析することが可能であることが知られている。並行トレンドとは、処置がない場合、処置群と統制群の成果の平均のトレンド（時間変化）が等しいという仮定である。そのため、処置がない場合、横軸を時間、縦軸を成果変数（の平均値）とした際の両群の成果に関する曲線（直線）は並行になる（上図におけるA-CとB-B’の直線は並行になる）ことを意味する。 また、処置効果の異質性がないことも重要である。平均処置効果がグループや時間によって変化する場合、推定結果にバイアスがかかることが知られている。例えば、成果等による介入効果が時間によって変化する（長期的に効果が弱まるなどの）場合や、処置効果が特に処置群に強い影響を与えうる（影響を強く受けそうな群に処置が行われている）場合には異質な処置効果（Heterogenous treatment effects）として、問題になる。これに加えて、SUTVAでも議論したスピルオーバーがないこともDIDでは重要かつ満たすことが容易ではない仮定となる。自然災害や紛争など地理的要因に紐づいた構造変化を捉える場合、少なからず地理的な近接性に伴うスピルオーバー効果が生まれる可能性があるため、注意が必要となる。 上記の仮定を満たしているかについて、正当化のための理想的な方法はない。図示化や論理的説明（産業的知識）が用いて、上記の仮定が満たされていることを説得的に記述することが求められる。また、その説明の補助として統計的検定が併用されることも多い。 "],["fisher-et-al.-2019-による配送能力と小売成果の研究.html", "19.1 Fisher et al. (2019) による配送能力と小売成果の研究", " 19.1 Fisher et al. (2019) による配送能力と小売成果の研究 現代の小売業では、オフライン（実店舗）とオンライン（Eコマース、モバイル）を結合したオムニチャネルと呼ばれる小売環境を構築することが増えている。オムニチャネル環境においては、物流自動化などに代表される物流能力（速さと正確さ）の向上が重要な投資であることが論じられてきた（田頭, 2024）。物流システムへの投資は高いコストがかかる一方で、オンラインとオフライン両方の販売チャネルを有する企業の成果へ与える影響は明示的に示されていなかった。 Fisher et al. (2019) は企業による新たな物流倉庫（Distribution Center: DC）の設立に着目し、オムニチャネル小売文脈において、製品の配送スピードの向上が企業成果へ与える影響について分析を行った。この研究では、米国のオムニチャネルアパレル小売企業のEコマースおよび店舗データと地域の配送状況データを結合しデータセットを構築した。その企業は元々、米国東側にDCを設置しており、その東側のDCから米国全土からの注文と顧客への配送に対応していた。そのため、西部の顧客からのEC注文では、製品の配送完了に7営業日を要していた。 このような背景のもと、当該企業は2012年に米国西側に新しい「オンライン用」DCを設立した。そしてこの新DCは西側からの注文にのみ対応していた。この新DC設立を擬似実験として捉え、新DC（西側地域の）注文エリアに入る顧客と店舗を処置群、それ以外（東側地域の）顧客と店舗を統制群とした。なお、新DCの設立によってほとんどの西側地域のEコマース注文製品は配送完了に要する日数が短縮された。また、この企業はデータ観測の期間中に配送時間の短縮について広告や宣伝を行っていないため、情報伝達による影響はコントロールされている。 Fisher et al. (2019) ではDIDに基づくモデル化に加え、傾向スコア（Propensity Score）ウェイトを計算とそのコントロールを行っている。傾向スコアとは、処置に影響を与えうる先行要因に基づき、「処置の受けやすさの程度」についてスコア化したものである。このスコアを用いて、似た人を探し出し比較対象を特定化したり（傾向スコアマッチング）、回帰分析における重み付けとして活用することで処置の受けやすさについてコントロールする（傾向スコア回帰）ことが行われる（松浦, 2024）。傾向スコアについいての詳細は省略するが、Fisher et al. (2019) では完全にランダムではない擬似実験で生じうる問題として、処置群と統制群が事前に持っていた違いについて、傾向スコア回帰を応用することで対応することを試みている。 分析の結果、配送時間の短縮（新DCの設立）は、Eコマースにおける、週当たりの新規顧客数、取引数、バスケットサイズ（一回注文あたりの平均額）が上昇することを示した。これは、配送時間というEコマースサービス品質の向上が当該販売チャネルの成果につながることを示した結果だといえる。また分析結果では、配送時間の短縮が実店舗の週当たり取引数の増加につながることも示した。この結果は、Eコーマスへの投資が実店舗の成果向上にもつながる、販売チャネル間スピルオーバー効果を示唆している。 このような分析結果に対して Fisher et al. (2019) ではそのメカニズムに迫るような探索的データ分析を実施している。その結果、以下の三つの点を明らかにした。 配送時間短縮による影響は、新DCからの累積出荷数が多いエリアほど強い。 これは、顧客の（早い配送に対する）経験学習効果がある可能性を示唆している。 配送時間短縮による影響は、Eコマース店舗普及率や実店舗数といった当該小売ブランドの存在感が強いエリアほど強い。 この結果は、ブランド力による向上効果を示唆している。 Eコマース店舗の購買経験が少ない顧客は、配送時間の短縮に反応し、短期的にはそれが顕著なものとして観察されるが、長期的には、購買経験の高い顧客の反応が顕著になる。 観察されたDID推定の結果について、短期的には学習効果がそれを牽引し、長期的にはブランド力による影響が起因していると解釈できる。 このように、因果推論を用いた係数の推定や検定自体は、その効果の背後にあるメカニズムを説明することには繋がらない場合もある。その場合には、メカニズムに迫るような追加的な調査や分析を行うことで、「なぜそのような結果に繋がるのか」というという問いについて検討する必要がある。 "],["参考文献-17.html", "19.2 参考文献", " 19.2 参考文献 Fisher, M. L., Gallino, S., &amp; Xu, J. J. (2019). The Value of Rapid Delivery in Omnichannel Retailing. Journal of Marketing Research, 56(5), 732-748. 田頭拓己（2024）「オムニチャネル環境下での小売戦略」, 『一橋ビジネスレビュー』, 71巻, 4号, 18-31. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
