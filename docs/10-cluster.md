# セグメントとクラスター分析{#cluster}
## 本章の概要
本書ではこれまで、統計的な推定や検定を用いて仮説を検証する方法を説明してきた。これに対して \@ref(empirics) 章では、データを探索的に分析することでマーケティングに関する洞察を得ることへ関心が集まっていることを紹介した。データ分析のアプローチにおいても、検証のようなプロセスを経ずに、データの持つ情報を集約・整理することで探索的に分析結果を得るものが存在する。このような探索的なデータ分析手法として、マーケティング領域で広く使われているものに、クラスター分析がある。

クラスター分析は、複数の量的変数情報に基づいて、データのサンプルをいくつかのグループに分類する方法である。この手法は、「セグメンテーション」というマーケティング実務的枠組みと関連している。マーケティングの基本的な戦略方針として、セグメンテーション、ターゲティング、ポジショニングがある。これは、市場を構成する消費者を細分化し、標的とするグループを特定化したうえで、具体的なマーケティング要素をそのグループに合わせて調整することで、市場におけるポジションを確立するという、実務的方針である。しかしながら人間が自身の認知能力によって数多く存在する消費者の中からセグメントを発見・弁別するのは容易ではない。

このような限界を克服するため、消費者に関する情報を集めたデータセットを用いて、消費者間の類似性をもとにグループ分けを行うことに活用できるデータ分析手法がクラスター分析である。クラスター分析では、回帰分析のように着目する目的変数を用いず、入力されたデータそのものに着目する。このようなアプローチでは、分析者の判断が重要な役割を担う。そのため、本章では、クラスター分析の実行方法に加え、分析手法そのものの概要についても理解することを目的とする。

この目的を達成するために、本章では、セグメンテーションとマーケティング意思決定についての復習を述べた後、主なクラスター分析アプローチとして、階層的クラスター分析と非階層的クラスター分析（K-means法）を紹介する。階層的クラスター分析はデータの中から類似している観測値を段階的にクラスター（観測値の集団）としてまとめていき、最終的にすべてのデータが1つのクラスターになるまでそれを繰り返す方法である。この方法では、観測値同士の類似性（距離）やクラスター同士の類似性に基づき似たものから順にまとめていく。階層的クラスター分析では、デンドログラムと呼ばれる樹形図のような図が主な結果として出力され、この結果をもとに、研究者がいくつのクラスター数でこのデータをまとめ上げることが良いのかを判断する。

しかしながら、いくつのクラスター数が良いのかという点については、多くの場合研究者が分類結果の「効率性」と「有効性」のバランスから判断することになる。効率性は分類によってどれだけ多くの情報を集約し説明できているかを表しており、より少ないクラスター数で多くのデータを説明できたほうが情報の集約による効率性が高いと考えられる。しかしながら、得たクラスター分類が役に立たないと意味がない。そこで、分類結果がどの程度現実的な含意につながるかを捉えたのが有効性である。効率性を意識しすぎて少ないクラスター数による分類を採用しても、あまりに大雑把過ぎる分類だと分析結果が有益にならないため、ある程度クラスター数を増やしたほうが有効な分類になるかもしれない。クラスター分析では、これらの基準によっていくつのクラスター数でデータをまとめ上げることが好ましいのかを研究者自身が判断することになる。

非階層的クラスター分析は、階層的クラスター分析などによってクラスター数の目ぼしをつけた後に実行することが一般的である。それは、非階層的クラスター分析を実行するためには、研究者が事前にクラスター数を指定することが必要なためである。そのため、非階層的クラスター分析に加え、エルボー法などの手法を併用し、クラスター数を決定すること多い。そのうえで非階層的クラスター分析では、指定されたクラスター数を所与として、各観測に対してクラスターを割り当てる計算・分析が実行される。その際、似た（距離の近い）観測同士は同じクラスターに、似ていない観測同士は違うクラスターに割り当てられ、全ての観測データがどれか一つのクラスターに所属するような結果を得る。その割り当て結果を用いて、各クラスターの特徴を確認することが可能になる。

クラスター分析の概要を説明したあとは、Rを用いた分析手法を紹介する。本書では、吉田秀雄記念事業財団によって2023年に実施され、オンライン上に公開されている消費者調査アンケートデータを用いて消費者クラスターを発見することを試みる。ここでは、クラスター分析の実行に関するコードに加え、プレゼンテーション等で活用できる図示化の方法についても紹介する。

## セグメンテーションとマーケティング意思決定
セグメンテーションは、セグメンテーション・ターゲティング・ポジショニング（STP）に代表される基本的なマーケティング枠組みを構成する重要な要素である。マーケティングにおいて企業は、製品やサービスを通じて顧客に対し価値を提供することを目指すのだが、そのためには、対象となる市場においてどの部分（セグメント）を狙うのかを特定化する必要がある。市場を構成する消費者は、多様な好みや特徴を持っている。その中から似た好みや特徴を持つ消費者グループを見つけ出し特定化することがセグメンテーション（市場細分化）である。

市場におけるセグメントを明らかにすることは、より効果的に標的への価値提供を行うことにつながると期待できる。セグメンテーションを行わない場合、様々な消費者の好みからバランスを取るように、平均的な特徴をもつような製品・サービスを開発することになる。例えば、ホットティーが好きな人もいれば、アイスティーが好きな人もいる。しかしながら、これらの間を取った「ぬるいお茶」を提供しても、誰の好みにも響かない可能性がある。つまり、万人受けを狙って中途半端な製品・サービスを提供することを避けるためにも、市場を細分化して、共通の好みを持つ消費者グループを判別することはとても重要になる。

セグメンテーションを行うための分類基準として、企業はいろいろな情報を活用すると考えられる。理想的には、企業は消費者が求めている便益や好みに基づき市場を細分化したいと考えるだろう。例えば自動車市場において、ある消費者は技術的革新性を、別の消費者は快適性を求めるかもしれない。このような消費者のニーズに基づき市場を細分化することができれば、より効果的な標的市場を決定できる。しかしながら、消費者のニーズに関するデータを直接的に入手できない場合も多い。そのようなときには、（1）消費者属性、（2）心理的情報、（3）行動的情報についてのデータをニーズにつながる代理指標として用いてセグメンテーションを行うことが多い。

消費者属性としては、人口統計的情報や地理的情報が用いられる。人口統計的情報には、年齢、職業、所得、世帯人数、教育水準などが含まれる。これらの情報が消費者のニーズと連動している場合には、人口統計的情報に基づく市場細分化は効果的であると言える。例えば、高品質だが高価格な製品と低価格だが質の低い製品がある場合、所得による市場細分化は効果的になるかもしれない。また、子供がいる世帯といない世帯では、異なるニーズを持っていることも予想できる。

地理的情報として、消費者の居住地域によって異なる好みを表すことがある。例えば、関東と関西のように、食品の味付けに関する好みについて日本国内の地域間で違いが伺える。カップうどんやスナック菓子においては、このような味付けに関する地域差を反映させた製品開発を行っており、製品を販売する地域に応じて味付けを変えている。このような人口統計・地理的情報は、測定可能性・容易性が高いという利点を有している。自社内部データや自治体の人口動態情報などによって、自社顧客や特定地域における消費者特性の情報について得ることができる。これらの情報が消費者のニーズや好みと連動している場合には、非常に有力なデータとなるものの、マーケティングの本来的な目的はこれらの情報に基づくセグメントを発見することではないことには、注意が必要である。あくまで、これらの情報の背後にある消費者のニーズの代理指標として利用していることを理解することが求められる。

心理的情報としては、ライフスタイルや価値観、志向といった、特定の商品への評価そのものではない消費者の心理的特性を捉えた指標が用いられる。これらの情報については、心理尺度を用いた質問紙調査などによって把握することができる。例えば、低価格志向や環境意識などは製品・サービスに求める価値観と近い心理的特性であるため、例えば自社の既存顧客や店舗を出店しているエリア内において環境問題への関心に基づきセグメントを確認することは、製品開発上重要な情報になりうる。

また、行動につながる心理的な態度（ブランドに対する好意）を捉えた類型も可能になる。これにより、どのようなセグメントが何を好むかを把握することができる。例えば、英国における "comfortable green" というセグメントは、Fairtrade や Green & Black's（チョコレート菓子ブランド） を好むが、Asda（スーパーマーケットチェーン）やコカ・コーラは嫌っているだと言われている（Jobber and Elis-Chadwick, 2020）。

<!-- ライフスタイル類型の代表例が、1980年代にスタンフォード大学研究センターによって開発された Values and Lifestyles (VALS) があり、日本においても、日本版VALSが開発されている（池尾ほか,2010）。また近年の類型では、ライフスタイルによるグローバル・セグメントの類型も提示されている。 例えば、ユーロモニター -->

行動的情報の代表的なものとしては、購買行動に関する情報が挙げられる。例えば、製品やサービスの購買・利用頻度を顧客によるブランドロイヤルティの代理指標として用いることも多い。セグメンテーションを行うためにはまず、「どの情報に基づきセグメンテーションを行うのか」という基準となる情報の選定が求められる。

セグメンテーションは、図\@ref(fig:segment) に示されるように、市場に存在する多様な消費者をいくつかのグループに分類することである。セグメンテーションが可能であることの前提には、（1）市場における消費者の好みが異質であることと、（2）共通した好みを持つ消費者のまとまりは見いだせるが、同時に見出したまとまり間での好みの差異も見いだせることである。つまり、異なる好みを持つ消費者の中から、「グループ内類似性」と「グループ間差異」とを基準に複数のグループを見出すことがセグメンテーションであるといえる。

<div class="figure">
<img src="cluster/segment.jpg" alt="セグメンテーション概要"  />
<p class="caption">(\#fig:segment)セグメンテーション概要</p>
</div>

### STPその後

セグメンテーションが完了すると企業はターゲティングを行う。ターゲティングにおいては、発見したセグメントの中から標的とする特定のセグメントを選ぶ。その際、自社にとって魅力的なセグメントを選ぶのだが、その魅力は主に以下の要因によって規定される（Jobber and Ellis-CHadwick, 2020）：

- 市場要因：セグメントのサイズ、成長率、収益性
- 競争要因：競争の状態（数量的情報だけでなく、質的情報も重要）、新規参入者、差別化可能性
- 政治・社会・環境要因：政治的・社会的傾向、環境問題、技術的変化

基本的に企業にとっては、規模が大きく、成長性も収益性も高いセグメントは魅力的である。しかし、そのようなセグメントに対しては競争も激しくなると予想できる。そこで、競争の状態を観察するのだが、ここでは競合の数だけではなく競合の持っている強みが自社とどのように異なるかを検討することも重要になる。例えば、自動車メーカーにとってアメリカ市場は非常に魅力的なマーケットである。しかしながら、伝統的に欧米の自動車会社が市場を席巻しており、他地域の企業にとっては厳しい競争環境に見えるかもしれない。しかしながら、欧米の自動車会社が持っていた弱点をうまく利用し、競争を優位に進めたのは日本の自動車会社である（Jobber and Ellis-CHadwick, 2020）。つまり、競争の状態に関する質的な側面も含めてセグメントの魅力を評価すべることが重要となる。

政治・社会・環境要因の例として、近年のジェンダーや性役割に関する消費者の認識の変化が挙げられる。このような変化は、新たなセグメントの成長性へ影響を与えるかもしれない。伝統的な価値観のもとでは、女性が育児の中心を担うと思われる傾向があったが、それとは異なる価値観が台頭することによって、育児に関する製品群において男性も魅力的なセグメントになるかもしれない。例えば、自転車小売業者のサイクルベースあさひは、直線的で無骨なデザインながら、チャイルドシートや荷台を追加しやすい機能を有した「88サイクル（ハチハチサイクル）」という製品を販売している。あさひはこの製品を「パパチャリ」としてフレーミングし、家庭内での役割を果たす男性のに向けた自転車としての市場を捉えようと試みている。

セグメントの選択においては、セグメントの魅力だけでなく、自社の能力も考慮する必要がある。どれだけ魅力的なセグメントがあったとしても、そのセグメントのニーズに応えるための能力がなければ、そのセグメントを狙うことは適切ではない。また、自社能力を評価するときには、既存もしくは潜在的な競合との比較のもとで相対的に能力を評価することが大切になる。例えば、自社として製品品質に自信があったとしても、同程度の価格帯でより品質の高い製品を製造できる競合がいた場合には、相対的に能力は低いことになる。

ターゲットを決めた後企業は、製品や提供物を決定し、市場において特有のポジションを専有しようと試みる。この段階をポジショニングという。ポジショニング段階にある企業は、競争的な優位性を得ることに焦点を合わせるのだが、この時消費者に自社製品が優れていると認識されるような情報伝達も必要になる。ポジショニングと消費者の認識との関係については \@ref(factor) 章にて説明を行う。

次節では、定量的なデータを用いたセグメントの探索・発見方法としてのクラスター分析を紹介する。なお、本資料においてはクラスター分析の概要とRを用いた基礎的な分析方法に着目する。そのため、理論的背景や発展的手法については他の資料や著書を参照してほしい。

## セグメンテーションの実行とクラスター分析
セグメンテーションでは、基準となる情報を選定し、その情報を用いて消費者の類型を発見する。その発見の方法として照井・佐藤（2023）では、（1） 経験、（2）クラスタリング、（3）潜在クラス、の3つの方法を紹介している。経験によるセグメンテーションでは、マーケティング担当者の経験や既存のリサーチによる知見を参考に消費者を類型する。この方法は、十分な知識や経験が蓄積されており、基準となる情報が限定的かつニーズと関連的である場合には有効になる。例えば、消費者の年齢がニーズと関連していることがわかっている場合、年齢に基づくセグメンテーションは特別な分析を介さずとも有効な手法となりうる。しかしながら、経験的な知見が不足している場合には、セグメンテーションの信頼性を損なうことに加えて、考慮すべき情報が複数ある場合には、情報処理が複雑になり、類型化が困難になる。

クラスタリングによるセグメンテーションは、クラスター分析を用いたセグメンテーションである。基準となる情報を（多くの場合複数）選択し、その情報をもとに各観測の類似性を求めることで、類型化を行う。この方法は、多くの情報をセグメンテーションの基準として用いることができる点や、分析方法についての資料やソフトウェアが充実しているため、分析の実現可能性が高いという利点を持つ。本資料ではこのクラスター分析を中心に議論を進める。第三の潜在クラスによるセグメンテーションでは、潜在クラスモデルと呼ばれる統計モデルを用いた方法である。これは発展的な手法であり、セグメントに関する統計的推測も行えるという利点を持っている。また、この方法では消費者が確率的に複数のグループに属することも許容するため、より現実的な手法とも評価されている（照井・佐藤,2023）。しかしながら、この方法には相対的に多くのデータを必要とし、発展的な統計的知識が必要になるため、本書では扱わない。

本書では、消費者や顧客のセグメントを発見するための方法としてクラスター分析を紹介する。クラスター分析は、2つ以上の基準となる情報（変数）に基づいて、対象または人を相互に排他的で網羅的なグループに分類するために使用される統計的手法である。クラスター分析にはいくつかのアプローチが存在するが、それらに共通するのはサンプル間の類似性を確認し、グループとして分割していくというプロセスを有しているということである。ここでは主に階層的クラスター分析と、非階層的クラスター分析を紹介する。階層的クラスター分析は類似する観測同士を段階的にまとめていき、グループ（クラスター）を形成していく方法である。一方で非階層的クラスター分析は、分析者の定めた前提のもと、非階層的にクラスターを形成する方法である。

### 階層的クラスター分析{#Hier}
本節では、階層的クラスター分析について説明する。階層的クラスター分析では、データの中から類似している観測値を段階的にクラスターとしてまとめていき、最終的にすべてのデータが1つのクラスターになるまでそれを繰り返す方法である。

このプロセスにおいて類似度は観測値同士の距離として測られる。距離の測定方法には色々とあるが、ここではユークリッド距離とマンハッタン距離を紹介する。ユークリッド距離は、観測値同士の各座標の差の二乗和の平方根であり、マンハッタン距離は観測値同士の各座標の差の絶対値の合計であるといえる。例えば、以下の図\@ref(fig:distance) における2点をつなぐ赤い線がユークリッド距離、青い線がマンハッタン距離だといえる。以降では、実際の分析においても頻繁に用いられるユークリッド距離に主に焦点を合わせ、手法を紹介する。

<div class="figure">
<img src="cluster/dis.jpg" alt="距離定義概要" width="60%" />
<p class="caption">(\#fig:distance)距離定義概要</p>
</div>

階層的クラスター分析では、図\@ref(fig:dendrogram) で示されているデンドログラム（樹形図）を得ることで、各観測が段階的に集約されていく様子が可視化される。例えば、図 \@ref(fig:dendrogram)の左（1）では、6個の観測が3つのクラスターにまとめ・分類されている一方で、右（2）では、2つのクラスターに集約されている事がわかる。これらの図を見ると、aはbと最も近く、cはdと最も近いことがうかがえる。また、aとbで構成されたクラスターは、cdクラスターとは近いが、efとは遠いことがうかがえる。デンドログラムの解釈方法については後述するが、まずはデンドログラムを得るプロセスについて説明する。

<div class="figure">
<img src="10-cluster_files/figure-html/dendrogram-1.png" alt="デンドログラム" width="672" />
<p class="caption">(\#fig:dendrogram)デンドログラム</p>
</div>

ここでは、以下の表\@ref(tab:exampledata) で示されている、2つの変数（x, y）に関する6個のデータが与えられた場合を考える。この場合の各観測値は図  \@ref(fig:distanceplot)のように示される。


Table: (\#tab:exampledata)データ例

|   |  x|  y|
|:--|--:|--:|
|a  |  1|  2|
|b  |  2|  3|
|c  |  2|  5|
|d  |  3|  5|
|e  |  7|  2|
|f  |  6|  3|

<div class="figure">
<img src="10-cluster_files/figure-html/distanceplot-1.png" alt="階層的クラスター分析" width="672" />
<p class="caption">(\#fig:distanceplot)階層的クラスター分析</p>
</div>

この時、各データ同士の類似度をユークリッド距離で測るとする。例えば、a と最も近いデータは b である。a と b の距離は、以下のように求まる。
$$
\sqrt{(2-3)^2+(1-2)^2}=\sqrt{2}=1.414
$$
同様に、c（e）と最も近いデータは d（f）であり、その類似度もユークリッド距離で求めることができる。これによって、各観測点から最も近い観測点同士を結びつける形で、3つのクラスター（図\@ref(fig:dendrogram) 左に対応）が初期段階のものとして形成される。次に、まとめた3つのクラスターと、他の観測点もしくは他クラスターとの距離を計算し、より大きな（多くの観測点が含まれる）クラスターを形成する。例えば、クラスター（a, b）は、（e, f）よりも（c, d）のほうに近いため、図\@ref(fig:distanceplot) に示されているように、（a, b, c, d）というクラスターとしてまとめる（合併する）ことができる（図\@ref(fig:dendrogram) 右に対応）。このように段階的にデータをまとめていくと、最終的にはすべてのデータを一つのクラスターとしてまとめることができる。これが、階層的クラスター分析の直感的プロセスである。

図 \@ref(fig:dendrogram) のようなデンドログラムでは、一番下に全観測値が表示される。図における水平な線は、クラスタとしての併合を意味しており、下でのつながりほど初期に併合されたクラスタであることを示す。そのため、最終的には（一番上では）すべてが一つのクラスタにまとまっていることがうかがえる。この時、デンドログラムの高さ（縦軸）は距離を示している。したがってデンドログラムは、どの程度の離れ具合を許容するかによって何組でデータをまとめられるかが変わることを表している。例えば図\@ref(fig:dendrogram) では、高さを2に定めれば3つのクラスターにまとめることができ、4を設定すれば2つのクラスターにまとめることができる。

階層的クラスター分析ではクラスター同士を段階的に合併させていくのだが、クラスター同士の類似性を測るための方法もいくつか存在する。ここでは、代表的なものをいくつか紹介する。クラスター同士の距離の決め方として、図 \@ref(fig:clusterdis) に示すように、最短距離法（Single linkage method）、再遠距離法（Complete linkage method）群平均法（Group average method）がある。最短（遠）距離法はクラスタ間の最も近い（遠い）観測の組み合わせの距離を測るものである。一方で群平均法は、クラスタ間のすべての観察の組についての距離を計算する方法である。

<div class="figure">
<img src="cluster/cluster_comb.jpg" alt="クラスター類似性測定手法"  />
<p class="caption">(\#fig:clusterdis)クラスター類似性測定手法</p>
</div>

また、非常に頻繁に使われる手法としてウォード法も存在する。ウォード法は最小分散法とも呼ばれ、クラスター内の平均までの二乗距離を最小化する方法である。ウォード法では、異なるクラスター間の類似性について、まずそれらを構成する観測値を一括にして捉えた仮のクラスターを形成し、そのまとめられたクラスター内の観測値同士の距離が近い（分散が小さい）ものから併合される。図 \@ref(fig:wards)ではその直感的な概略図を示している。ウォード法ではこのようにクラスター間の類似性を分散の形式で測定することで段階的に併合するグループを決定していく。

<div class="figure">
<img src="cluster/wards.jpg" alt="ウォード法概要"  />
<p class="caption">(\#fig:wards)ウォード法概要</p>
</div>

階層的クラスター分析におけるクラスター数の決定は、分析者の判断に依存し、絶対的な基準は存在しない。しかしながら、多くの場合、分類の「効率性」と「有効性」のバランスから効果的なクラスター数が決定される。効率性は分類によってどれだけ多くの情報を説明できているかを表しており、より少ないクラスター数で多くのデータを説明できたほうが効率性が高いと考えられる。例えば、表 \@ref(tab:exampledata) で示されているような6つのデータを6つのクラスターで説明しても、クラスター分析としては不適切であり、より少ないクラスター数でデータを捉えたいと考えるだろう。一方で、得たクラスター分類がどの程度現実的に含意のある分類をできているか、を捉えたのが有効性である。効率性を意識しすぎて少ないクラスター数による分類を採用しても、あまりに大雑把過ぎる分類だと分析結果が有益にならない。例えば、図 \@ref(fig:distanceplot) において、1つのクラスターで6個のデータを説明するよりも、2つか3つのクラスターで説明したほうがより直感的かつ実務的な含意を得ることができるかもしれない。このように、階層的クラスター分析は、クラスターがどのように形成されていくかの段階を示すことによって、おおよそどのようなクラスター分類を行うことが良さそうかを判断することにつながる。

## 非階層的クラスター分析：K-means法{#k-means}
非階層的クラスター分析では、段階的なクラスターの分類ではなく、事前に決められたクラスター数に基づき、データを分類する。本資料では代表的な非階層的クラスタリング法である、K-means法について紹介する。K-means法の分析では、分析者が事前にクラスター数を決める必要がある。その仮定の下、以下のようなプロセスによってクラスターが決定される。

1. 初期クラスターの形成
2. クラスターセンター（中心）のアップデートと更新
3. アップデートの収束と最終クラスターの決定

図 \@ref(fig:kmeans1)と\@ref(fig:kmeans2) はK-means法のプロセスに関する直感的説明を段階的に図示化したものである。ここでは、図 \@ref(fig:kmeans1) (1) のようなデータに対し、3つのクラスター数を仮定した場合を考える（このプロセスは任意の $k$ 個のクラスターに対して適応可能）。 

<div class="figure">
<img src="cluster/kmeans1.jpg" alt="K-means法直感１"  />
<p class="caption">(\#fig:kmeans1)K-means法直感１</p>
</div>

まず初めに図\@ref(fig:kmeans1) (2) に着目してほしい。ここでは、与えられたデータの空間に対して3つのクラスターセンター初期値（初期シードとも言う）をランダムにとる。次に各観測個体をそれぞれの最も近い初期センターに割り当てる形で3つのクラスターを作成する（図\@ref(fig:kmeans1) (3)）。ここで、作成された3つのクラスター（の観測値）と、初期値とを比較し、適切に中心を表しているかを検討する（図\@ref(fig:kmeans1) (4)）。例えば、図\@ref(fig:kmeans1) (4) における赤いクラスターについてはもう少し右上の方向にセンターを移動したほうがいいかもしれない。このように、各クラスターの観測値に基づき、クラスターセンターが計算され、アップデートされる。

<div class="figure">
<img src="cluster/kmeans2.jpg" alt="K-means法直感２"  />
<p class="caption">(\#fig:kmeans2)K-means法直感２</p>
</div>

しかしながら、クラスターセンターがアップデートされると、新たなセンターから各観測個体までの距離も変化する。例えば、図\@ref(fig:kmeans2) (5) を見ると、これまで赤クラスターに属していた個体のひとつが、クラスターセンターのアップデートに伴って赤のセンターよりも青のセンターから近くなっている。そのため、この個体は青クラスターに分類されるべきである（図\@ref(fig:kmeans2) (6)）。各観測個体のクラスターへの分類が変更されたため、クラスターセンターも当然修正されるべきであり、新たな分類に基づくクラスターセンターが再度計算される（図\@ref(fig:kmeans2) (7)）。K-means法においては、このようなプロセスをクラスターセンターが動かなくなるまで繰り返すことで、観測個体を分類する。

K-means法において分析者は、分析実行前にクラスター数の決定を行わなければならない。クラスター数の決定においては、\@ref(Hier)節でも述べた通り、分析者の恣意性を含む判断によって決定される。クラスター数の決定においては、伝統的には（1）ハーティガンルールと、（2）エルボー法という方法のいずれかもしくはその両方を用いることが多い。

ハーティガンルールとは、$k$ 個と $k+1$ 個のクラスターにおける内部平方和（クラスター内部の分散）を比較するための数値を計算し、その値が10を越えた場合は、$k+1$個のクラスター数を採用したほうが良いという考え方である。この値が10を越える状況は、$k+1$個の場合に比べ$k$個のクラスターを採用した場合に内部平方和（クラスター内部の分散）が大きくなることを意味している。

一方でエルボー法とは、$k$ 個のクラスター数それぞれに対応する内部平方和をプロットすることで、好ましいクラスター数を解釈する方法である。図\@ref(fig:elbow)に示すように、クラスター数の変化に伴い、内部平方和（クラスター内部の分散）がどのように変化するかをチェックすることができる。この図を用いた判断では、変化量（傾き）の変化に着目することも多い。変化量が小さい、つまりクラスター数が増えてもあまり内部平方和が減らない場合、効率性と有効性の観点からより少ないクラスター数を選ぶことが判断されやすい。例えば図\@ref(fig:elbow) の場合、4つのクラスターまではクラスター数の増加とともに内部平方和が下がっているものの、4を越えてからは傾きがほぼ水平になっていることがうかがえる。そのため、このような結果の場合には、4クラスターを仮定した非階層的クラスター分析を実行することが多い。


<div class="figure">
<img src="10-cluster_files/figure-html/elbow-1.png" alt="エルボー法例" width="672" />
<p class="caption">(\#fig:elbow)エルボー法例</p>
</div>

ここで、セグメントの発見を目的とするような探索的なクラスター分析に関する手順を紹介する。非階層的クラスター分析は、階層的クラスター分析との組み合わせる形で実行されることも多い。顧客セグメントを探索するような目的で実行されるクラスター分析として、以下のような手順を紹介する：

1. 階層クラスター分析、ハーティガンルールもしくはエルボー法などの実行
2. クラスター数 (k) を決定
3. k個のクラスター数に対する K-means クラスター分析の実行
  - $\rightarrow$ クラスターの図示化と記述による検討
4. クラスターのプロファイル情報を整理し、各クラスターを評価

次節では、この4つのプロセスに基づき、Rを使ったクラスター分析方法を紹介する。

## Rによるクラスター分析の実行
ここからは、Rを用いたクラスター分析の実行方法について紹介する。なお、本節では、吉田秀雄記念事業財団によって2023年に実施された消費者調査アンケートデータ、[https://www.yhmf.jp/aid/data/data_aid_2023_later.html](https://www.yhmf.jp/aid/data/data_aid_2023_later.html) のうち、（簡単化のために）特定の変数と回答者（兵庫と東京在住者のみ）を抽出して利用する^[なお、データの読み込みエラーを防ぐために、自由回答設問項目は削除している。]。ここでは、以下のリストにある変数を活用する。

- q12_4（ブランドロイヤリティ性向）：「たとえ多くのブランドを利用できる状況にあっても、何時も同じブランドを選ぶ。」
- q13_3（価格感度）：「大抵、一番安いものを買う。」
- 性別
- 年齢
- 結婚有無
- q5: 職業
- q7_2: 世帯年収

なお、q12_4とq13_3はどちらも5点リッカート尺度で回答を得ている。その他消費者属性情報の回答項目の詳細はエクセルファイルを参照してほしい。上記の条件に沿うデータは以下のように抽出することができ、その結果、1973件の回答を得た。


``` r
df_cons <- readxl::read_xlsx("data/回答データ【消費者調査2023年度下期調査】.xlsx", sheet = "回答データ【共通調査2023年度下期】",na = " ")
library(tidyverse)
#東京と兵庫の県番号リスト作成
list <- c(13, 28)
#回答者と項目を抽出
df_cons <- df_cons %>% 
  select(県番号, q12_4, q13_3, 性別, 年齢, 結婚有無, q5, q7_2) %>% 
    filter(県番号 %in% list,
           q12_4 != 999,
           q13_3 != 999) %>% 
  mutate(Pref = ifelse(県番号 == 13, "Tokyo", "Hyogo"),
         Gender = case_when(性別 == 1 ~ "Male",
                            性別 == 2 ~ "Female",
                            TRUE ~ "Others"),
         MaritalSt. = case_when(結婚有無 == 1 ~ "Married",
                            結婚有無 == 2 ~ "Not Married",
                            TRUE ~ "Others"))
```

本節でのクラスター分析の実行にあたっては、以下のパッケージをインストールし利用する。なお、クラスター分析自体は `cluster` パッケージで実行可能だが、`factoextra` を用いるともう少し洗練された可視化が可能になるため、こちらの紹介も行う。



``` r
install.packages(c("cluster", "factoextra","ggrepel", "useful"))
library(cluster)
library(factoextra)
library(ggrepel)
library(useful)
```

本節で我々は`df_cons`を用いて分析を行いたいのだが、クラスタリングの関数はデータセットに含まれているすべての変量間の類似性・相関を計算してしまうため、必要な変数のみを抽出し、分析に利用する。なお、クラスタリングの関数は文字列にも対応していないため、もしデータセットにそのような変数が含まれている場合には、この段階で取り除く必要がある。

ここではブランドスイッチ（q12_4）と価格志向（q13_3）に関する変数の抽出と、クラスター分析の実行を行うために、以下の手順を経る：

1. 階層的クラスター分析を実行し、デンドログラムを確認
2. デンドログラムとエルボー法によりクラスター数を決定
3. 2.で決定したクラスター数に従い、K-means法を実行
4. クラスター情報と元データ（df_cons）を結合し、プロファイル情報の整理と検討

まずは以下の通り、階層的クラスター分析を実行する。なお、階層的クラスター分析の実行においては、`agnes()` 関数を用いる。その際に用いる距離の計測方法は `metric =` という引数で設定できる。その後、階層的クラスター分析の結果を用いて `pltree()` 関数を実行することで、デンドログラムが出力される。

``` r
clus_cons <- df_cons %>% 
  select(q12_4, q13_3)
Hier1 <- agnes(clus_cons, metric = "euclidian", method = "ward", stand = TRUE)

pltree(Hier1)
```

<div class="figure">
<img src="10-cluster_files/figure-html/consdendrom-1.png" alt="消費者デンドログラム" width="672" />
<p class="caption">(\#fig:consdendrom)消費者デンドログラム</p>
</div>

図\@ref(fig:consdendrom) の通り、本データは1973件の観測個体を有するため、デンドログラムも下部については識別が難しくなっている。しかしながら、例えば高さを40ぐらいに設定した場合、クラスターは4つか5つに分かれることがうかがえる。例えば4クラスターの場合、どのような分け方になるかについて、直感的に示すために、`factoextra` パッケージを用いると図\@ref(fig:factodendrogram)のようにデンドログラムを出力できる。この時、`fviz_dend()` 関数における `k = 4` という引数で、ここでのクラスター数を定義している。

``` r
alt_Hier <- clus_cons %>% 
  dist("euclidian") %>% 
  hclust("ward.D")
alt_Hier %>% 
  fviz_dend(k = 4, rect = TRUE,
            rect_border = TRUE) 
```

<div class="figure">
<img src="10-cluster_files/figure-html/factodendrogram-1.png" alt="4クラスターデンドログラム" width="672" />
<p class="caption">(\#fig:factodendrogram)4クラスターデンドログラム</p>
</div>


続いて、上記の結果を踏まえ、エルボー法を実施する。


``` r
fviz_nbclust(clus_cons, kmeans, method = "wss")
```

<div class="figure">
<img src="10-cluster_files/figure-html/conselbow-1.png" alt="エルボー法結果" width="672" />
<p class="caption">(\#fig:conselbow)エルボー法結果</p>
</div>

図 \@ref(fig:conselbow) を確認すると、4もしくは5クラスターで傾きが小さくなっているように見える。ちなみに、5クラスターを採用したケースは、デンドログラムでは図\@ref(fig:factodendrogram2)のように示すことができる。

``` r
alt_Hier %>% 
  fviz_dend(k = 5, rect = TRUE,
            rect_border = TRUE) 
```

<div class="figure">
<img src="10-cluster_files/figure-html/factodendrogram2-1.png" alt="5クラスターデンドログラム" width="672" />
<p class="caption">(\#fig:factodendrogram2)5クラスターデンドログラム</p>
</div>

<!-- そこで、ハーティガンルールを用いてもう少し詳細にクラスター数について検討する。この分析においては、`useful` パッケージに含まれる `FitKMeans()` という関数を用いて以下のように結果を確認する。 -->

<!-- ```{r hartigan} -->
<!-- Clus1Best<- FitKMeans(clus_cons, max.clusters = 10, nstart = 3, seed = 123) -->
<!-- Clus1Best -->
<!-- ``` -->

4クラスターモデルと5クラスターモデルのどちらがいいのかについての判断は難しいが、ここでは便宜的に5つのクラスターを仮定する。K-means法の実施には、`cluster`パッケージの `kmeans()`関数を用いる。なお、初期クラスターセンターの割り振りはランダムで行うため、`set.seed()` を用いる。以下では、5つのクラスターを仮定した分析結果を出力する。なお、紙幅の関係から `clustering vector` に関する結果は省略している。


``` r
set.seed(343)
K_shopping <- kmeans(clus_cons,5) 
K_shopping
```

```
## K-means clustering with 5 clusters of sizes 80, 298, 760, 592, 243
## 
## Cluster means:
##      q12_4    q13_3
## 1 5.000000 2.150000
## 2 3.261745 1.781879
## 3 2.582895 3.207895
## 4 3.447635 4.123311
## 5 1.699588 1.777778
## 
## Clustering vector:
##    [1] 5 5 5 4 3 3 3 4 2 3 3 1 3 3 3 3 4 3 3 4 2 4 5 3 2 4 3 4 5 4 3 3 3 4 3 4 3
##   [38] 4 3 3 4 5 4 3 3 4 3 3 4 2 4 2 2 4 4 2 3 3 3 3 4 3 4 3 2 1 2 1 3 4 3 4 2 4
##   [75] 2 3 5 3 4 4 4 3 4 3 2 4 3 2 3 3 3 3 4 2 3 3 3 4 4 3 3 3 4 3 3 3 4 4 3 1 3
##  [112] 3 3 3 4 4 3 4 5 4 4 5 3 3 4 3 4 4 4 3 3 3 4 1 2 4 3 2 3 1 4 3 2 4 3 3 4 4
##  [149] 3 2 5 2 4 2 3 3 3 4 3 3 4 3 4 4 3 4 4 3 3 4 4 3 2 4 5 3 2 4 3 4 3 3 3 3 3
##  [186] 4 3 3 3 3 3 2 4 3 3 2 4 4 4 3 2 3 3 4 4 1 3 3 2 4 3 5 3 3 3 4 4 5 3 2 1 4
##  [223] 5 3 5 3 4 4 5 3 4 5 3 4 3 3 4 4 5 3 3 3 5 4 3 4 2 3 2 4 3 4 3 3 4 5 4 2 3
##  [260] 3 3 3 5 2 5 4 3 2 3 5 1 4 2 3 4 3 4 3 4 3 2 1 3 3 3 2 3 4 4 2 3 4 3 4 4 3
##  [297] 3 3 2 4 3 5 4 2 2 4 4 4 4 5 3 1 3 4 4 4 3 3 5 4 5 4 4 3 2 2 4 4 5 4 3 4 5
##  [334] 3 3 4 4 3 3 3 2 1 4 3 4 3 4 3 5 5 2 3 1 2 1 3 3 3 4 2 3 4 4 2 3 3 2 2 3 3
##  [371] 4 3 3 3 3 3 3 5 3 3 4 4 3 4 3 2 4 3 4 5 3 2 4 2 3 3 4 3 3 3 3 4 4 4 1 4 3
##  [408] 4 3 2 3 3 3 3 3 4 3 1 2 4 2 5 2 3 2 3 2 4 2 3 2 3 4 3 3 3 3 3 2 3 4 3 3 3
##  [445] 3 3 3 5 3 3 3 1 3 3 4 4 3 2 3 5 3 4 1 5 4 4 3 2 3 3 4 3 5 4 3 4 4 3 3 4 5
##  [482] 4 3 4 2 3 5 4 4 4 3 4 3 4 3 3 4 5 5 3 3 3 5 3 3 1 5 3 4 3 4 5 3 2 2 5 5 3
##  [519] 2 3 3 4 2 4 3 3 3 2 5 2 4 5 5 3 3 5 3 4 2 3 3 3 3 3 3 4 4 1 4 3 3 4 2 5 4
##  [556] 3 4 3 3 2 5 4 5 3 3 4 5 4 3 2 2 4 4 3 2 4 2 3 2 3 3 2 3 3 3 4 5 3 4 3 3 4
##  [593] 2 4 3 3 4 3 4 3 4 3 3 4 3 2 2 3 3 3 2 4 3 4 2 5 5 3 5 3 3 5 3 4 3 1 3 2 3
##  [630] 3 5 1 4 3 5 3 2 3 2 4 5 2 3 1 4 4 3 2 4 3 4 3 4 3 2 4 3 4 4 3 4 3 5 4 3 3
##  [667] 3 3 3 5 2 3 2 3 4 4 3 5 3 3 4 1 3 4 1 3 5 3 5 4 5 3 3 4 2 3 3 5 2 3 3 3 4
##  [704] 3 3 4 3 3 3 3 3 5 4 1 2 3 2 4 2 2 3 5 3 3 3 5 3 4 3 5 3 2 3 4 4 5 4 3 1 3
##  [741] 4 3 5 4 3 1 3 3 4 4 3 5 4 3 4 4 4 2 3 4 3 5 3 3 3 4 4 3 1 3 5 2 4 1 2 3 2
##  [778] 3 2 3 4 3 5 5 4 3 4 4 3 2 3 3 3 1 3 3 3 3 2 4 4 3 3 2 3 4 4 3 3 4 3 5 4 3
##  [815] 4 3 4 1 4 1 4 3 4 5 2 4 2 3 4 3 2 5 4 3 4 4 3 4 2 5 3 2 2 4 3 4 4 4 4 3 5
##  [852] 3 2 4 5 4 4 5 2 2 3 1 3 4 5 5 3 2 3 3 5 2 4 3 3 2 4 2 3 1 3 2 2 2 3 2 5 1
##  [889] 4 4 5 5 5 4 2 5 2 3 4 1 4 3 5 5 3 2 4 4 4 3 2 2 4 3 3 4 3 3 3 3 3 2 3 2 3
##  [926] 5 5 4 3 4 3 4 4 3 1 2 4 5 4 3 5 1 2 4 5 4 4 5 3 3 4 3 4 4 3 4 2 4 2 4 5 4
##  [963] 3 2 4 5 4 2 4 2 3 4 3 4 3 2 4 2 3 2 3 4 2 4 2 3 3 3 2 2 4 1 3 4 1 4 3 4 4
## [1000] 3 5 4 2 5 4 5 2 3 3 3 4 3 4 3 3 4 3 3 4 4 2 3 3 5 3 2 5 3 3 3 3 4 3 4 4 5
## [1037] 2 4 3 4 2 3 2 2 4 3 3 3 4 4 2 5 3 3 3 4 3 3 5 4 4 4 2 3 4 5 1 4 3 1 2 5 4
## [1074] 4 3 4 5 4 4 3 4 3 3 4 4 4 3 3 3 4 2 4 2 4 3 2 4 4 3 3 2 3 5 5 4 3 3 5 3 4
## [1111] 2 2 3 4 5 3 4 5 4 4 3 3 3 3 3 3 4 4 3 3 4 4 3 3 4 3 2 4 5 5 5 5 2 4 3 4 3
## [1148] 4 4 2 5 4 3 2 2 3 2 4 3 3 4 4 5 3 2 2 3 4 4 2 4 5 3 5 1 4 3 2 5 3 4 4 3 3
## [1185] 4 1 5 2 4 5 3 3 4 4 3 4 3 5 5 2 3 2 3 4 4 3 2 4 4 3 2 4 3 3 3 2 5 4 1 3 5
## [1222] 4 4 3 3 3 3 4 4 2 3 4 2 5 5 4 5 4 5 2 5 2 5 4 3 3 4 3 5 3 4 4 3 4 4 4 3 3
## [1259] 2 4 2 3 4 4 4 3 4 2 4 2 4 3 4 3 2 2 4 5 5 4 3 3 3 4 4 2 3 3 4 4 3 3 5 1 4
## [1296] 3 4 3 2 3 4 3 3 4 5 3 3 4 3 4 3 2 4 4 3 3 3 5 3 4 3 5 4 4 3 4 3 5 1 4 3 1
## [1333] 2 4 4 5 3 4 4 3 3 3 4 3 3 5 4 3 1 3 3 5 3 3 3 4 2 3 5 3 2 4 3 4 1 3 4 1 5
## [1370] 3 4 3 3 3 4 3 3 5 4 5 4 3 4 2 4 5 2 4 3 3 5 5 3 2 4 5 4 4 3 3 2 3 2 2 3 3
## [1407] 4 4 5 3 4 2 3 2 3 1 2 1 2 3 2 3 3 3 4 4 1 4 2 5 3 5 2 4 4 3 4 4 4 5 3 2 3
## [1444] 1 5 3 4 3 3 3 2 4 3 4 4 3 4 3 2 4 3 4 2 4 3 3 5 4 3 5 4 2 5 4 4 2 3 2 3 3
## [1481] 3 2 5 5 5 4 4 1 3 4 3 4 3 2 5 3 4 3 3 5 1 4 4 4 3 3 3 3 4 3 4 3 4 2 3 3 3
## [1518] 3 5 4 5 5 3 2 4 3 4 4 3 3 4 3 4 3 3 2 4 4 4 5 2 4 4 1 3 3 3 5 3 3 5 3 3 3
## [1555] 3 5 4 3 2 2 4 3 3 2 3 3 3 3 3 4 4 5 5 4 2 4 3 2 3 4 4 3 1 4 2 1 5 4 2 4 3
## [1592] 3 2 3 3 4 5 3 2 2 4 1 3 2 5 3 3 3 2 4 5 5 5 4 3 5 5 4 2 4 3 5 3 1 4 5 3 2
## [1629] 4 5 5 3 2 3 3 4 1 3 5 2 3 3 2 4 3 2 3 2 3 5 3 2 2 5 3 2 4 4 4 3 4 2 4 2 3
## [1666] 2 4 2 5 5 1 3 4 2 3 5 3 3 5 5 3 4 4 2 2 4 2 4 3 4 4 4 3 3 2 2 2 4 5 2 3 3
## [1703] 5 1 3 4 5 2 3 5 5 3 2 2 4 3 3 5 3 4 4 3 3 5 3 4 4 4 3 3 4 4 5 4 4 3 3 3 3
## [1740] 4 3 3 4 2 2 5 3 4 4 2 3 4 4 2 4 3 3 2 3 2 4 2 4 4 4 4 5 2 2 3 4 3 2 4 2 4
## [1777] 4 4 4 4 3 2 4 4 3 4 4 5 4 5 2 2 2 2 3 5 4 5 2 2 1 4 4 2 4 1 3 5 2 4 5 3 1
## [1814] 3 5 4 2 3 3 3 3 2 4 3 5 3 5 3 4 1 5 5 2 3 4 2 5 4 3 5 4 5 3 4 3 4 5 3 4 4
## [1851] 4 3 3 5 1 2 5 3 3 1 2 4 3 4 3 3 4 4 1 5 3 3 3 4 1 2 3 5 5 2 4 3 2 4 4 2 3
## [1888] 2 4 5 4 3 2 3 5 3 1 3 2 3 1 2 3 2 4 3 3 3 3 4 4 3 4 4 3 4 2 4 2 3 3 5 3 4
## [1925] 5 3 4 3 4 4 5 4 1 4 3 3 3 2 2 5 1 4 4 3 3 3 4 4 4 4 2 4 3 4 1 4 5 2 4 3 5
## [1962] 1 3 3 3 4 5 2 3 3 4 3 3
## 
## Within cluster sum of squares by cluster:
## [1]  54.2000 108.4060 401.9303 622.3750 127.0700
##  (between_SS / total_SS =  66.7 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
## [6] "betweenss"    "size"         "iter"         "ifault"
```
分析の結果表示される `Cluster means:`では、分析に用いた変数に関する各クラスターごとの平均値が出力される。例えば、クラスター1は、ブランドスイッチ（q12_4）については高いが、価格志向（q13_3）については低いことがうかがえる。結果の下部で表示される、`Within cluster sum of squares by cluster:`には、各クラスターの内部平方和（分散）が出力されている。その下の `between_SS / total_SS` は、全体の平方和に占めるクラスター間平方和の比率であり、全体の分散の何%を5つのクラスターが説明しているかを示している。

以下では、上述の結果について、`factoextra::fviz_cluster()`によって図示化する。図\@ref(fig:kmeansplot1) では、ブランドロイヤリティと価格志向のどちらも高い（低い）グループや、どちらか一方のみ高いグループに加え、価格志向は低いがブランドスイッチについては平均的というグループも確認できた。このように図示化することで、クラスター分析の結果についての解釈が容易になる。なお、4クラスターを採用した場合の結果については、図 \@ref(fig:kmeansplot2)で示されている。また、4クラスターでの分析を実行すると、`between_SS / total_SS =  65.6 %` という結果を得るはずなので、興味のある読者は自身で実行してみてほしい。なお、そのためには4クラスターの仮定でK-means法を実施し直す必要がある。


``` r
fviz_cluster(K_shopping, data = clus_cons, geom = "point") +
  labs(title = "k = 5", x = "Brand switching (Std. score)", 
       y = "Price orientation (Std. score)")
```

<div class="figure">
<img src="10-cluster_files/figure-html/kmeansplot1-1.png" alt="消費者価値観クラスター" width="672" />
<p class="caption">(\#fig:kmeansplot1)消費者価値観クラスター</p>
</div>

<div class="figure">
<img src="10-cluster_files/figure-html/kmeansplot2-1.png" alt="4クラスターモデル図" width="672" />
<p class="caption">(\#fig:kmeansplot2)4クラスターモデル図</p>
</div>


続いて、各クラスターに属する消費者についての情報を整理、検討する。ここでは、5クラスターモデルの結果に基づき、クラスター情報と元データとを結合する。結合した結果として以下の表 \@ref(tab:merged) に、元データにクラスター番号に関する変数 `cluster_id` が追加されていることがわかる。


``` r
df_cons$cluster_id <- factor(K_shopping$cluster)
knitr::kable(head(df_cons),caption = "結合データ")
```



Table: (\#tab:merged)結合データ

| 県番号| q12_4| q13_3| 性別| 年齢| 結婚有無| q5| q7_2|Pref  |Gender |MaritalSt.  |cluster_id |
|------:|-----:|-----:|----:|----:|--------:|--:|----:|:-----|:------|:-----------|:----------|
|     13|     2|     2|    2|   38|        2|  8|    4|Tokyo |Female |Not Married |5          |
|     13|     2|     1|    1|   42|        2|  2|    9|Tokyo |Male   |Not Married |5          |
|     13|     1|     2|    1|   30|        1|  3|   NA|Tokyo |Male   |Married     |5          |
|     13|     4|     5|    1|   33|        1|  3|   NA|Tokyo |Male   |Married     |4          |
|     28|     3|     3|    1|   25|        1|  3|   10|Hyogo |Male   |Married     |3          |
|     28|     3|     3|    1|   32|        1|  3|   NA|Hyogo |Male   |Married     |3          |

以下では、各クラスターの平均的な消費者像について理解するために、個人属性情報をまとめる。その作業方法と結果は、以下のコードと表\@ref(tab:clusSummary) のとおりである。ここでは、表\@ref(tab:clusSummary)に基づき、いくつかのクラスターに絞り、それらの特徴を整理し解釈を行う。なお、本データでは東京在住者の観測数（1465）が兵庫在住者の観測数（508）を大きく上回っているため、居住エリアの比率については元データの比率（$1465/1973\approx 0.742$）を基準に、この比率と同等の東京在住者がいれば1を、それより多ければ1より大きい値を取るような比率で示している。

表\@ref(tab:clusSummary)におけるクラスター１は最も人数が少なく、ロイヤルティ（Loyalty_m）は高く、価格志向（Price_m）はやや低いことがうかがえる。そのため、このグループに属する消費者は、価格に基づいて選んだブランドを買い続けるわけではないかもしれない。クラスター5は、ロイヤルティも価格志向もどちらも低いことがうかがえる。そのため、このクラスターに属する消費者は価格以外の属性に重視していて、自社へのロイヤルティも低い可能性が伺える。また、このクラスターは既婚者率が高く、このような属性の特徴も価値観に影響しているのかもしれない。他のクラスターに関する解釈はここでは割愛するが、ぜひ読者においてもそれぞれのクラスターについての解釈を展開してみてほしい。


``` r
clus_summary <- df_cons %>% 
  group_by(cluster_id) %>% 
  summarize(N = n(),
            Loyalty_m = mean(q12_4),
            Price_m = mean(q13_3),
            Age_m = mean(年齢),
            Male_r = sum(Gender == "Male")/n(),
            Tokyo_r = (sum(Pref == "Tokyo")/n())/(1465/1973),
            Married_r = sum(MaritalSt. == "Married")/n())

knitr::kable(clus_summary, caption = "クラスターサマリー")
```



Table: (\#tab:clusSummary)クラスターサマリー

|cluster_id |   N| Loyalty_m|  Price_m|    Age_m|    Male_r|   Tokyo_r| Married_r|
|:----------|---:|---------:|--------:|--------:|---------:|---------:|---------:|
|1          |  80|  5.000000| 2.150000| 42.61250| 0.5125000| 0.8753925| 0.5250000|
|2          | 298|  3.261745| 1.781879| 38.56040| 0.5134228| 0.9400188| 0.6107383|
|3          | 760|  2.582895| 3.207895| 41.88816| 0.5065789| 1.0100683| 0.5618421|
|4          | 592|  3.447635| 4.123311| 43.79730| 0.4645270| 1.0191680| 0.4712838|
|5          | 243|  1.699588| 1.777778| 34.91770| 0.5102881| 1.0363938| 0.6213992|

本節では、実際の消費者アンケートデータを用いてクラスター分析の実行手順を紹介した。クラスター分析を用いて探索的にセグメントを発見するためには、階層的クラスター分析と非階層的クラスター分析を組み合わせることが重要となる。ただし、クラスター分析ではクラスター数の決定や結果の解釈などにおいて、分析者の恣意性に依存することになる。しかしながら、このような限界も理解した上でうまく利用すれば、有益なセグメントを発見することにもつながりうる。

本節では観測数2000件弱のデータを利用したが、実際に我々がこれだけのデータを目視し、セグメントを発見することは困難である。クラスター分析は人間では処理困難な情報量を集約し、解釈可能にしてくれるという強みを持つ。もちろんもっと多量のデータを用いてクラスター分析を実行することも可能であるし、機械学習への応用や潜在クラスモデルの利用など、より発展的な手法も展開されているため、興味・関心のある学生においてはさらなる学習を進めてほしい。

## 練習問題
前節で活用した `回答データ【消費者調査2023年度下期調査】.xlsx` に含まれる別の変数を活用し、データの探索およびクラスター分析を実行しよう。そして、（1）データ・変数の概要、（2）実行した分析アプローチ、（3）結果（図を含む）、（4）結果に基づく実務的助言、をまとめた短いサマリーを書いてみよう。その際、どんな分析を実施し、どんな結果を得たのか、明確かつ簡潔に書くことをこころがけよう。


## 参考文献
Jobber, D., & Ellis-Chadwick (2020). *Principles and Practices of Marketing 9th Edition*, McGraw-Hill Education.

照井伸彦・佐藤忠彦（2022）「現代マーケティング・リサーチ[新版]」,有斐閣.

