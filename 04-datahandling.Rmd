# データ処理と記述分析{#handling}

## 本章の概要
本章では、Rを用いたデータの処理と記述的な分析について紹介する。マーケティング領域では、様々なタイプのデータを扱うが、どのようなデータであってもデータを取り込み、分析可能な形に処理した後、データの特徴について確認することが必要になる。とくに、最終的に高度な統計分析を行うことを想定していたとしても、自身の獲得したデータの特徴を確認することは非常に重要である。そのため、本章ではデータの読み込みやデータ処理といった、分析の前に必要な技術的過程を紹介する。

これまでの本書の内容は、データを収集するまでの注意点や方法を説明した。しかしながら、収集したデータをただ眺めているだけでは、定量的な知見を得ることはできない。そのため、以降の節では主にデータ処理や分析手法について説明する。まず我々は、データセットの構築から学ぶ。例えばあなたがアンケートを実施したならば、そのアンケートからデータセットを構築する努力が必要になる。アンケート結果に基づくデータセット構築において研究者はコーディング、トランスクライビング、データクリーニングのプロセスを経る。

コーディングは、回答を分析可能なフォーマットへ変換する作業であり、通常回答に対して数値を当てはめる作業を伴う。例えば、回答者が男性ならば 1 を、女性ならば 0 をとるようなダミー変数を作成する作業がこれに当てはまる。コーディングは、不必要な情報を減らすことでデータ化プロセスを担う。トランスクライビングは、質問紙に記載された回答をデータ入力していく作業である。入力に関するヒューマンエラーは起こるものとして考える必要があるため、通常このプロセスは二人一組でダブルチェックをしながら行う。なお、オンラインアンケートの際はこのプロセスは自動で行われるため、不要になる。データクリーニングでは、研究者は不適切な回答のチェックを行う。例えば、回答可能範囲から外れた回答（例、7点尺度における8点回答）や、論理的に非整合的な回答（例、回答者が利用したことないと答えているサービスについて評価している場合） がないかをチェックする。また、欠損値という回答がない観測についてもチェックする必要がある。マーケティング分野においては欠損値のあるサンプルを削除するという方法も用いられるが、欠損値の扱いは奥が深く、いくつかの対応法がある。本書ではその詳細については扱わないが、欠損値に対応するためのデータ処理についての専門書も存在するため、関心のある読者はそれを参照してほしい（高橋・渡辺,  2017）。


データセットの構築が完了したあとは、本書では基本的にRを通じて様々な作業を行う。Rには、様々な計算を実行するための関数が用意されており（例、mean, median, sqrt 等）、これらを使えば、実際に我々分析者が各コマンドもシンプルになる。関数の利用においては `f(argument)` のように関数名 `f` のあとにカッコをつけて表記する。なお、`argument` は日本では引数とよばれ、計算に必要な情報の指定である。関数の利用において作業者は具体的な関数名とそれに対応する引数を指定する必要がある。また、我々は通常、パッケージをインストール・起動することで他者が作った関数を利用することが多い。関数とパッケージについての説明や実行例は「Rの基本操作」節で紹介しているのでそちらを参照してほしい。

ここで学ぶRでの作業は主に以下の通りである。

1. データの読み込み（csv, excel, etc.）
2. dplyrの利用とデータ整形
3. パイプ演算子を用いた複数処理の実行
4. Wide vs. Long data format (おまけ)

なお、これらの作業は、統計的な分析を実行する前のデータ前処理にも使われるものなので、データ分析をしたいと考える人達にとってはとても重要なスキルになる。

分析可能な形にデータを処理した後は、データの特徴を確認することが必要になる。具体的には、記述統計や図示化を用いて、特定の変数の分布や変数間の関係について確認を行うことが重要である。この過程により、調査の背景にある実情を把握できるとともに、入手したデータ（のコーディングなど）にエラーがないかを確認することにもつながる。

## データの読み込み

本節で用いるパッケージをまだインストールしていない読者は、以下のコマンドを用いてインストールしてほしい。また、インストールを完了したら、library()関数によって各パッケージを起動すること。

```{r installPackages, eval=FALSE, message=FALSE}
install.packages(c("tidyverse","readr","readxl"))
```

```{r, warning=FALSE,message=FALSE}
library(tidyverse)
library(readr)
library(readxl)
```


ここからは、データセットを用いた分析を行う。基本的な操作においては、R外部で作成されたデータを取り込み利用するのだが、あるソフトウェアで作成・保存されたデータセットが他の環境で利用できるとは限らないという点に注意が必要である。具体的には、エクセルファイルが誰にでも開けるとは思ってはいけない。そのため、ソフト特性に依存しない汎用的な形式を使うことが好ましい。汎用性の高いファイル形式の代表的な例がCSV (comma separated values) である。以下は、mktData.csvという架空のファイルをdfというオブジェクト名で取り込むための、見本コードである。ここで用いる関数は、readrというパッケージのread_csv() という関数である。なお、以下のコードは、実在しない 'mktData.csv'というデータセットを引数に利用した見本コードであるため、このコードをそのまま実行してもエラーを返すだけであることに注意をしてほしい。実際には、自身が利用するファイル名を指定してファイルを読み込むことになる。なお、以下のコードの2行目は、データの1行目に変数名（列名）が含まれていない場合の引数の指定方法である。

```{r readcsv, eval=FALSE, message=FALSE}
df1 <- readr::read_csv("mktData.csv")
df2 <- readr::read_csv("mktData.csv", col_name = FALSE)
```

なお、デスクトップ版を利用している場合には、ファイルが格納されているディレクトリ名も指定する必要がある。Rにおいては様々なファイルを入力・出力することになるため、利用するディレクトリが一貫していないとそれだけで作業が煩雑になる。そのため、「（補足）デスクトップ版の利用とプロジェクト機能」節で紹介している「プロジェクト機能」必ずを活用するようにほしい。

本講義では、大学の学務ポータル（manaba）を通じて教員が配布したデータを学生各自のコンピュータにダウンロードし、それを post.cloudに各自がアップロードするという手順によって分析用データを利用する。Manabaからのデータのダウンロードは各自で済ませてほしい。Posit.cloudはR studio 画面を表示する段階でプロジェクトが作成される。ここではまず、分析に利用するデータを格納するディレクトリを作成するコードを紹介する<デスクトップ版を使用している場合も、Project を指定していれば、以下のコードで全く同じ結果を得ることができる。> 具体的は、以下の通りdir.create() を使って新たに data というディレクトリ（フォルダ）を作成する。

```{r directory, eval=FALSE}
dir.create("data")

```

新たなディレクトリを作成したら、そこに、ポータルよりダウンロードしたデータを入れてほしい。ここではまず "2022idpos.csv"というデータを用いる。データが無事 data ディレクトリに含まれたら、以下のコマンドによってそのデータファイルをR の作業スペースに読み込み、それに "idpos" というオブジェクト名を定義する。なお、ここで分析実行社はディレクトリを指定することも必要になる。また、コード内の na は、欠損値がどのように保存されているかを指定するための引数であり、もし欠損値が空欄であればnaによる指定は必要ない。

```{r importidposdata, message=FALSE}
idpos <- readr::read_csv("data/2022idpos.csv", na = ".")
```

問題なくデータを読み込むことができたら、そのデータの冒頭数行を head() 関数によって表示する。head() 関数の結果によると、このidposデータは、3000行、5列のデータセットであることがわかる。なお、同様の情報は R studio 画面内の Environment タブから確認することできる。

```{r headidpos}
head(idpos)
```

また、読み込んだデータ特徴の確認は他の関数でも実行できる。例えば、name() 関数を使えば、データ内の変数名 (列名) を確認できるし、tidyverseに含まれる glimpse() 関数によってもデータの冒頭数行を含むいくつかの情報を返してくれる。

```{r idposcheck}
names(idpos)
glimpse(idpos)
```

なお、このidposデータは、POS (Point of sales) という小売店レジでの取引データとロイヤルティプログラムなどの会員IDを含むID-POSと呼ばれるデータを想定し作成した、簡易的な人工データである。データには、小売店舗での取引日（date）、金額（spent）、クーポン利用の有無 (coupon)、性別 (gender) が含まれている。本来のPOSデータは、より詳細な日時や具体的な製品単品レベルの取引品目など、より詳細な情報が含まれているはずだが、ここでは簡単化のためにこのようなデータにしている。また、もしスプレッドシート形式で表示したい場合には View() 関数をconsoleに直接入力することでそれが可能になる。例えば、idposデータを用いて以下のようなコードを入力することで、Sourceウィンドウに新しいタブができ、そこにデータセットが表示される。

```{r view, echo=FALSE, message=FALSE}
View(idpos)
```

## データの整形
データの整形には、tidyverseパッケージ群に含まれるdplyrというパッケージを用いる。しかしながら、tidyverseのインストール・起動しておけばdplyrも利用できるため、特に心配する必要はない。dplyr には、いくつもの便利な関数がふくまれているが、本節では主に以下の関数および機能を紹介する。

1. summarize()
2. mutate()
3. filter()
4. select()
5. arrange()
6. パイプ演算子 %>%

summarize は、ある変数の平均値や標準偏差などの記述統計量を計算することができる関数である。例えば、dataというデータセットに含まれる var_name という変数の平均値を計算し、それを M という変数名として定義する場合、以下のコマンドを用いる（以下のコマンドは見本コードである）。
```{r summirize, eval=FALSE, message=FALSE}
summarize(data, M = mean(var_name))
```

mutate は、データセットに引数内で指定した定義の変数（列）を追加する関数である。例えば、dataというデータセットに対し、definition で定義した変数をnew_varとして追加するには、以下のコマンドを用いる（以下は見本コードである）。実際にdefinitionを定義する場合には、様々な関数や論理式を利用する必要がある。例えば、"new_var = var1/100" という定義を用いれば、var1を1/100倍した値をnew_varとして定義することになる。また、"new_var = var1 – mean(var1)"という定義を用いれば、var1の観測値からvar1の平均値を引いた値をnew_varとしている。なお、このような操作化を一般的に「中心化」と呼ぶ。

```{r mutate, eval=FALSE, message=FALSE}
mutate(data, new_var = definition)
```

mutate関数の利用においては、条件分岐を用いた変数の作成を行うこともある。そのように、研究者がある変数の値に応じて異なる値を変数を作成するときには、mutate内で、ifelse()関数を用いるのが良い。ifelse() 内の第一引数は条件、第二引数は条件が満たされたときの処理、第三引数は条件が満たされないときの処理をそれぞれ表す。なお、特定の条件の指定には "==" （同値）, ">="（以上）, "<="（以下） を使う。具体的には、var1 が2ならば1をとり、それ以外であれば０をとるという条件でnew_varを作成するという指示は、以下のようになる（以下は見本コードである）。

```{r mutateIfelse, eval=FALSE, message=FALSE}
mutate(data, new_var = ifelse(var1 == 2, 1, 0))
```

filter関数は、データから特定の条件に合致する行だけ取り出す場合に用いる関数である。例えば、男性（gender == "male"）のサンプル情報のみ抽出したい場合には以下のような指示になる。

```{r filter, eval=FALSE, message=FALSE}
filter(data, gender == "male")
```

なお、特定の条件以外のものを指定したいときは、 という論理式 "!=" (not equal) を使う。男性以外の行を選ぶための指示は、以下の通りになる。

```{r filterNotequal, eval=FALSE, message=FALSE}
filter(data, gender != "male")
```

select関数は、特定の変数（列）を選んで新たなデータフレームを作成することができる関数である。例えば、dataというデータセットから、var1、var2、var3 という変数（列）を抽出して、data2というdataframeとして定義するには、以下のような指示になる。

```{r select, eval=FALSE, message=FALSE}
data2<- select(data, var1, var2, var3)
```

反対に、取り除きたい変数を指定するときには、以下のように "-" を使う。

```{r selectNegative, eval=FALSE, message=FALSE}
data2<- select(data, -var1)
```

列の指定方法には、いくつかのやり方が存在する。並んでいる列をまとめて指定するときは:（コロン）を使う。例えば、var1からvar5までの列をまとめて抽出し、それをdata2として定義するのは以下のようにできる。

```{r selectcolon, eval=FALSE, message=FALSE}
data2<- select(data, var1:var5)
```

また、tidyverseのstarts_with()（ends_with()）を使うことで、変数名の冒頭（末尾）が特定の文字列から始まる変数を指定するようなことも可能である。例えば、"v" という文字から始まる変数を取り出すための指示は、いかのようになる。
```{r selectstring, eval=FALSE, message=FALSE}
data3<- select(data, starts_with("v"))
```

arrangeは、データの並べかえを可能にする関数である。例えば、以下ではvar1の値が小さい順（昇順）に並べ替えるような指示を示す。一方で、降順にする場合は、desc(var1)と引数を指定する必要がある。

```{r arrange, eval=FALSE, message=FALSE}
data2 <- arrange(data, var1) 
data2 <- arrange(data, desc(var1)) 
```

また、tidyverse環境において、変数名を変更することも、rename() 関数で可能になる。

```{r rename, eval=FALSE, message=FALSE}
data3 <- rename(data2, var1 = sales)
```

Tidyverse 内の dplyr を使うことでパイプ演算子（%>%）が使える（ショートカット: Command (control) + Shift + m）。パイプ演算子は、左側の処理結果を演算子右側の関数の第一引数として利用するための指示である。たとえば、以下のコマンドではまず $\small 10-6$ が計算され、その結果である "4" が `sqrt()` の引数として利用される（sqrt(4) は 2）。

```{r pipe}
(10-6) %>% sqrt()
```

パイプ演算子は、複数のデータ操作処理を連続して行う際に便利である。例えば、顧客の情報を含むデータセット(data)から、男性に該当する情報のみを抽出し、var1(例、購買額)についてのランキングを作成したうえでいくつかの変数を含んだデータセット（new_data）を作成する場合を考える。その際に実行すべき作業とそれらに対応する関数は以下のように示すことができる。

1. 男性の情報だけ抜き出す(filter)
2. Var1の値について降順に並べ替える(arrange)
3. 第一位から最下位までの順位を割り当てた　ranking 変数を作る(mutate)
4. var1 , var2, var3, var4, orderだけ残し(select) new_dataとして定義する

上記の作業を一気に行うためのコードをパイプ演算子を使わずに書くと以下の様になる（以下は見本コード）。

```{r wopipes, eval=FALSE}
new_data <- select(
　mutate(
　　arrange(
　　　filter(data, gender == "male"),
　　　desc(var1)),
　　　ranking = 1:n()),
　 var1, var2, var3, var4, order)
```

パイプ演算子を使わない場合、先に実行する処理が内側に来ており、一見して何を行っているのか理解するのが難しい。一方でパイプ演算子を使い、左側の処理結果を演算子右側の関数の第一引数として利用すると、以下のように書き換えることができる。

```{r withpipes,eval=FALSE}
new_data <- data %>%
 filter(gender == "male")%>%
 arrange(desc(var1)) %>%
 mutate(ranking = 1:n()) %>% 
 select(var1, var2, var3, var4, order)
```

パイプ演算子の利用により、各関数の処理を一つの行で示せる。また、処理の順番通りに関数を記載することが可能なので、コードの記述容易性と可読性の両方が高まる。また、パイプ演算子による操作は次の関数の第一引数以外に反映されることも可能である。第一引数以外の引数に左側の処理結果を反映させる際には、該当する箇所に "." （ドット）を使う。たとえば、$\small 10-2$の計算結果を用いて2から8の偶数で構成されるベクトルを返すためのコードは以下のように書くことができる。

```{r pipeExample}
(10-2) %>% 
  seq(from = 2, to = ., by = 2)
```

データの整形・処理作業が終わったら、そのデータを自身のコンピュータ内のストレージに保存したいと考えるかもしれない。Rでは、外部への書き出しという形でデータを保存することが可能である。例えば、df という名前のデータフレームをnew_dataというファイル名で、dataというディレクトリにcsv形式を用いて保存するためには、以下のようなコードを用いる（以下は見本コード）。また、csv以外にもファイル形式は選択可能であり、例えばRのデータ形式(.Rds)で保存する場合には、"#Rds" 以降のコードを用いる。


```{r writecsv, eval=FALSE}
readr::write_csv(df, path = "data/new_data.csv")

#Rds
readr::write_rds(df, path = "data/new_data.Rds")
```

### 企業データの処理

ここで学んだデータ処理の手法を実行するために、
本節では、先ほど読み込んだ `MktRes_firmdata.xlsx`データを用いた分析を行う。このデータをＷｅｂサイトより `data` ディレクトリにダウンロードし、以下の要領で読み込んでほしい。

```{r firmdataimport, warning=FALSE}
firmdata <- readxl::read_xlsx("data/MktRes_firmdata.xlsx")
```


このデータは、小売・サービス分野の企業約160社（企業数は年によって異なる）に関する2010年から2019年までの財務データである（計1440件）。このデータは、日本生産性本部における顧客満足度調査の対象になっている企業リストを作成し、その企業の中から金融領域の企業や、データを入手できなかった一部の企業を教育的意図から排除したものである。したがって、日本の小売・サービス分野において全国的に知名度のある代表的な企業の財務データ（の一部）だと考えられる。

なお、本データには以下の変数が含まれており、データ内の単位は従業員数（人）を除き百万円である。

- fyear: 決算年
- legalname: 企業名
- ind_en: 日経業種名（英文）
- parent:親会社名（もしあれば）
- fiscal_month: 決算月
- current_liability: 流動負債
- ltloans: 長期借入金
- total_liability: 負債合計
- current_assets: 流動資産
- ppent: 有形固定資産
- total_assets: 資産合計
- net_assets_per_capital: 純資産合計／資本合計
- sales: 売上高
- sga: 販売費及び一般管理費
- operating_profit: 営業利益
- net_profit: 当期純利益
- pnet_profit: 親会社株主に帰属する当期純利益（連結）／当期利益（単独）
- re: 利益剰余金
- adv: 広告・宣伝費
- labor_cost: 人件費
- rd: 研究開発費
- other_sg: その他販売費及び一般管理費
- emp: 期末従業員数
- temp: 平均臨時従業員数
- tempratio: temp/(emp+temp)
- indgrowth: 産業成長率
- adint: 広告集中率（adv/sales）
- rdint: 研究集中率（rd/sales）
- mkexp:  (sga - rd) / sales
- op: operating_profit / sales
- roa: pnet_profit / total_assets 

本データセットは、複数年にわたる複数サンプルからのデータであり、一般的にこのような構造のデータをパネルデータという。パネルデータの分析の概要は \@ref(causation4) 節で紹介している。

ここではこのデータを用いて、以下の作業を行う。

1. 2018年度のデータのみを抽出する。
2. 企業名、年、売上高、人件費、期末従業員数、平均臨時従業員数のみの変数を含むデータセットにする。
3. 労働単価（人件費/（期末従業員数+平均臨時従業員数））変数を作成する。
4. 労働単価の高い順に並び替えてトップ10企業を出力する。

```{r firmdatahandling, message=FALSE, warning=FALSE}
firm2018_check <- firmdata %>% 
  filter(fyear == 2018) %>% 
  select(legalname, fyear, sales, labor_cost, emp, temp) %>% 
  mutate(wage = labor_cost/(temp+emp), na.rm=TRUE) %>% 
  arrange(desc(wage))

head(firm2018_check, n = 10)
```

このように、データの中から研究課題と整合的な情報を抽出したり、変数を作成したりすることができる。ただし、研究者にとって都合の良い結果を得るために恣意的に用いるデータを制限、操作することは、研究不正となる。そのため、実際の研究では、どのようなデータ・情報を用いるかについて事前に計画しておく必要がある。

## (おまけ) Wide型とLong型データセット
インターネットを通じて、とても都合の良いかつ信頼できるデータセットが入手できたとしても、それが分析のために望ましい形で保存されているとは限らない。特に、横長(wide)と縦長(long)データが存在し、それらのデータの型の違いには注意が必要である。我々人間がデータを眺め、解釈を与える場合にはwide型データのほうが扱いやすいのだが、コンピュータやソフトウェアがデータを分析する際には、long型のほうが好ましい。例えば、下図は4店舗のある年の6月から10月までの売上情報（単位：千円）を示したデータセットである。これは、複数サンプル-複数時点という構造のデータだが、各時点の観測値が横に並んでおり、wide型データだといえる。


![Wide型データ](datahandling/wide.png){width=70%}

手元にあるWide型データをLong型に変換したい場合の対応策として、ここでは tidyverseに含まれているtidyrのgather()関数を用いる方法を紹介する。この関数の利用方法を実演するためにManabaにアップされている sales_wide.csv をダウンロードし、プロジェクト内のdataディレクトリに移して欲しい。データを読み込むと、以下のようにデータ構造を確認することができる。

```{r sales_wide}
sales_wide <- readr::read_csv("data/sales_wide.csv", na = ".")
sales_wide

```

ここで用いる gather()関数は、以下の引数を指定する：
- data: 変換元のデータ
- key: 変数を１列にまとめたあと、元の列を区別するための列につける名前
- value: 変数を１列にまとめたあと、値が入る列につける名前
- どの範囲を一列にまとめるかの範囲指定

その上で、さきほどのsalesデータをLong型に変換するために、以下のようなコマンドを利用する。

```{r gather}
sales_long<- gather(data = sales_wide, key = "month",
               value = "sales", starts_with("sales"))
sales_long

```

編集された縦長データは上記の通り示されるが、その中身を見ると、monthの列にsales06などの情報が記載されており、好ましくない。この問題は、元のデータにおける変数名に該当する情報を新しいkey列の値に使うというgather関数の仕様に影響するものである。この問題に対しては、パイプ演算子を使ってgatherの実行前に、列名を年だけの形に変更することで対応可能である。以下が、修正版のコードであり、出力結果より、先述の問題点が解決されたことが確認できる。ただし、gather関数の実行において、下記コード内では範囲の引数の指定についても修正されていることに注意が必要である。

```{r gatherV2}
sales_long <- sales_wide %>% 
  rename(`06` = sales06,
         `07` = sales07,
         `08` = sales08,
         `09` = sales09,
         `10` = sales10) %>% 
  gather(key = "month",value = "sales",
         `06`:`10`) %>% 
  arrange(store)
sales_long
```

一方で、Long型データからWide型データへ変換するためには、tidyrのspread()を用いることが多い。spread() では、主に以下の引数を用いる。

- data: 変換元のデータ
- key: 変数を複数列にわけるとき、列を区別するための変数
- value: 複数列に分ける値

例えば、以下のコードで先程の sales_long データをwide型に変換することができる。

```{r spread}
wide_test <- sales_long %>% 
  spread(key = "month", value = "sales") %>% 
  rename(sales06 = `06`,
         sales07 = `07`,
         sales08 = `08`,
         sales09 = `09`,
         sales10 = `10`)
wide_test
```

## データの要約と可視化

本章では、記述統計や可視化によってデータを要約する方法について説明する。記述統計では、統計量と呼ばれる指標を用いてデータの特徴を数値から把握する。一方で可視化においては、図表を作成することでデータの特徴を視覚的に理解することを目的とする。実証的なマーケティング研究においては、データを用いた仮説の検証という方法が主流かもしれないが、仮説検証に用いるデータはどのようなものなのかを要約し、それを（論文やレポートの）読者へ伝えるプロセスは必要である。記述統計やデータの可視化は、このプロセスにおいて機能する方法である。なお、本章の作業においても `tidyverse` を用いるので、以下のように `tidyverse` を起動してほしい。

```{r tidy, message=FALSE}
library(tidyverse)
```

## 記述統計
記述統計の利用においては、データのタイプ別に利用すべき統計量が異なることに注意が必要である。「データのタイプ」という節で確認したように、データには量的変数とカテゴリ（を示す質的）変数がある。量的変数は数値で測定できるものであり、その計算結果を解釈することも可能である。一方でカテゴリ変数は、各観測個体が属している状態やグループを表す指標であり、それを計算してもそこから含意を得るのが難しい。Rのような統計ソフトは非常に素直なので、たとえカテゴリ変数であってもそこに数値が入力されていれば、記述統計に必要な計算を実行し、結果を返してくれる。しかしながら研究においてはそれらの結果を適切に解釈する必要があり、自身が用いている変数のタイプに応じた分析を実行する必要がある。

その上で本節ではまずひとつの量的変数の情報を要約するための記述統計を紹介する。一つの数値によってデータ全体を代表させるような数値を代表値と呼ぶ。代表値はおもにデータの中心を示す指標と考えられる。本節ではデータの中心を表す指標として中央値 (median) と平均値 (mean) を紹介する。中央値は、データのすべての観測値において、その値より小さな観測値の数と大きな観測値の数が等しくなるような真ん中の値を表す。そのため、（1, 3, 2, 5, 4）というデータにおける中央値は3である。これは、このデータを、1, 2, 3, 4, 5 と並べ替えると、3よりより小さな観測値の数と大きな観測値の数が等しくなっていることから確認できる^[一方でデータの観測数（ $n$ ）が偶数である場合、$\small n/2$ 番目と、$\small (n/2)+1$ 番目が中央となるため、n個のデータの観測値を、$x_1,x_2,...,x_n$ とすると、これらふたつの値の平均値（ $\small \frac{x_{\frac{n}{2}}+x_{ \frac{n}{2}+1}}{2}$ ）が中央値となる。Rにおいては`median()` 関数によって以下のように計算することができる。]。
```{r median}
d <- c(1, 3, 2, 5, 4)
median(d)
d2 <- c(1, 3, 2, 5, 4, 6)
median(d2)

```

平均値（算術平均と呼ばれる）は、最もよく使われる代表値の一つである。平均値は、n個のデータ、$\small x_1,x_2,...,x_n$ に対して以下のように定義される。

$$\bar{x} = \frac{1}{n}\sum_i^n x_i$$

観測値と平均値の差（$x_i - \bar{x}$）は偏差と呼ばれ、偏差の和はゼロである（$\sum_ix_i - \bar{x}=0$）という性質を持つ。つまり、平均値を中心として、データの正の方向へのばらつきと負の方向へのばらつきが釣り合いが取れているということが伺える。この点が、平均値がデータの中心を表す代表値として用いられるひとつの理由である。また、平均値にはいくつかの好ましい統計的性質があるのだが、それについては後述する。Rにおいては、`mean()` 関数を用いることで分析が可能である。例えば、9人の生徒に対して行われた数学(x)と国語(y)のテスト(10 点満点)の結果が、それぞれ以下の通りであったとしよう。

- 数学: (3,3,5,5,5,5,5,7,7)
- 国語: (2,3,3,5,5,5,7,7,8)

このときの平均値は以下のように求まる。

```{r meanexam}
math <- c(3,3,5,5,5,5,5,7,7)
jpn <-  c(2,3,3,5,5,5,7,7,8)

mean(math)
mean(jpn)
```

計算の結果、どちらも平均値は5であった。データの中心を表す代表値の値が等しかったため、これら2科目のテスト結果は同じ分布を持つと判断して良いのだろうか。自明かもしれないが、そのような解釈は不適切である。具体的には、データの「ばらつき」についても確認する必要がある。分布のばらつきは、平均値からの離れ方(平均値からの偏差) によって判断される事が多く、これが大きなデータが多い場合は、よりデータは散らばっ て分布していると解釈される。一方でデータが平均の近くに集まって分布している場合、ばらつきが小さいと捉えられる。この分布のばらつきは主に、分散や標準偏差という指標で測られる。

分散 (Variance, $S^2$で定義する) は以下のように、平均からの偏差の二乗の和をデータ数で割ったものだと定義される。平均からの偏差の和を計算すると、正の方向へのズレとマイナス方向へのずれがあるので、互いに相殺しあって合計は 0 になる。そこで、偏差の二乗和を用いることでデータ全体がどの程度平均からばらついているかを把握する。
$$S^2 = \frac{1}{n}\sum_i^n (x_i-\bar{x})^2$$

しかしながら、分散は元の値を二乗しているのでもとのデータと単位が異なる。そのため、分散の正の平方根 ($\sqrt{\cdot}$) を取った値を標準偏差と呼び、この標準偏差を用いることも多い^[偏差の二乗和のかわりに偏差の絶対値を用いた平均偏差という指標も存在する。しかしながら、分散や標準偏差のほうが好ましい統計的性質を持つことから、二乗和が用いられることが多い。]。なお、Rでは `var()` と `sd()` によって分散と標準偏差をそれぞれ求める。ただし、Rの関数による計算では $s^2=\frac{1}{n-1}\sum_i^n (x_i-\bar{x})^2$ で定義される「不偏標本分散」および「不偏標準誤差」という指標を用いる。これは、これらの指標のほうが統計的に好ましい性質を持っているためであるが、Rを用いた分散の計算値が、nで割った際の手計算値と異なることがあるのでその点には注意が必要である。

```{r varsd}
var(math)
var(jpn)
```

先程の数学と国語のテスト結果データを用いて分散を計算すると、国語の方が分散が大きいことがわかる。つまり、両テストとも平均値は同じであるものの、国語のほうがそのスコアのばらつきが大きいことがわかる。このように、代表値とともにデータのばらつきに関する情報も踏まえてデータの特徴を把握することが好ましい。

観察されたデータと標準偏差を用いて、特定の観測結果がデータ内において「相対的に」どのような位置にいるのかを捉えることも可能になる。具体的には、任意の量的変数 $x_1,...,x_n$ に対して、標準化されたスコア $z_1,..,z_n$ は以下のように定義できる。

$$
z_i=\frac{(x_i-\bar{x})}{\sqrt{(S^2)}}
$$

ただし、 $S^2$ は変数 $x$ の分散である（不偏標本分散を用いることもある）。上記定義の通り、標準化スコアは観測値の平均からの偏差を標準偏差で割っており、ある観測が平均値から標準偏差何個分ズレているかを示していると解釈できる。なお、標準化スコアは、平均が0、分散が1になることも知られている。

## カテゴリ変数の要約
一方でカテゴリ変数は、代表値や分散によって含意を得るのではなく、頻度のカウント（集計）や、クロス集計を用いることが多い。これにより、各カテゴリにどれぐらいの観測数があるのかを確認することが可能になる。カテゴリ変数の内容（出現頻度）の確認には、`table()` 関数を用いる。また、`with()`関数を用いて同様の結果を得ることも可能である。ここでは、先ほど用いた `firmdata` から2018年度の情報を抽出し、日経業種に基づく産業の違いから、どのカテゴリの企業がどれだけデータ内にいるのかを確認する。なお、tidyverseを起動していない場合には、必要に応じて `library(tidyverse)` を事前に指示してほしい。

```{r table, message=FALSE}
firm2018 <- firmdata %>% 
  filter(fyear == 2018)

table(firm2018$ind_en)
with(firm2018, table(ind_en))
```

また、table関数にて2つのカテゴリ変数を指定することで、両変数に対応するカテゴリの出現頻度を返してくれる。このような表のことをクロス集計表とよぶ。例えば、同データにおける広告集中的な企業を把握するため、広告集中度が中央値よりも高ければ1、それ以外であれば0を取るダミー変数（\@ref(dummy)節参照）を作成し、各産業カテゴリとの関係を確認する。

```{r}
firm2018 <- firm2018 %>% 
  mutate(ad_dummy = ifelse(adint > median(adint),1, 0))
with(firm2018, table(ind_en,ad_dummy))
```
これらのデータを確認すると、広告集中度が高い企業は鉄道会社やアミューズメント、ホテル、トラック運送業において少ないことがわかる。それ以外の産業では産業内でも広告集中度の高い企業と低い企業とが比較的バラけている。

特定のカテゴリ（例、デシルランク）に着目して、カテゴリ変数（例、性別）についての集計を行うことも可能である。例えば、広告集中度が高い企業における産業のばらつきを調べたいときには、`filter()` 関数を用いれば良い。

```{r}
firm2018 %>% 
  filter(ad_dummy == 1) %>% 
  with(table(ind_en))
```


カテゴリ変数と量的変数の関係を調べることも、グループ別に量的変数の要約を行う形で可能である。また、そのための手法はすでに我々は学習済みである。具体的には、前節で利用した `group_by()` 関数を用いる。例えば、売上高と広告集中度の平均と標準偏差を産業ごとに確認することは、以下のような指示で可能になる。

```{r}
firm2018 %>% 
  group_by(ind_en) %>% 
  summarize(obs = n(),
            sales_m = mean(sales),
            sales_sd = sd(sales),
            adint_m = mean(adint),
            adint_sd = sd(adint))
```

このように、カテゴリごとの量的変数の要約も実行可能である。なお、標準偏差が `NA` となっている箇所は、観測数が 1 であり、標準偏差を計算できない状況を表している。

## データの可視化

本書でのデータの可視化では、主にtidyverse内に含まれる ggplot2 というパッケージを用いる。データは一般的に、円グラフ、折れ線グラフ、帯グラフなどの様々なグラフを用いて視覚化される。しかしなが本節では、主にヒストグラム、箱ひげ図、バイオリンプロットをRでの実行例とともに紹介する。これらの図は、量的変数の分布を視覚的に示すことについて優れた可視化の方法だと言える。ここでは、ggplot2に内包されている diamonds データを用いて可視化を学ぶ（tidyverseを起動することで自動的に ggplot2も起動されるため、このタイミングでtidyverseを起動していない場合には、必要に応じて `library(tidyverse)` によってパッケージを起動してほしい）。diamonds データについては以下のように確認できる。

```{r diamond}
head(diamonds)
```

なお、Macのデスクトップ版でggplot2等を使うと日本語が文字化けするので、Macユーザーは別途以下のコマンドを実行する必要がある。

```{r, message=FALSE}
##For mac users
theme_set(theme_gray(base_size = 10, base_family = "HiraMinProN-W3"))
```


本書の可視化では、まず、ggplot2の `ggplot()` 関数を用いて図示化のためのオブジェクトを作成する。この関数では、以下の引数を指定する。

- data: 可視化に用いるデータフレームの指定
- mapping: データから抽出する変数と画面に表示される図との関係の指定
  - mapping内で、`aes()` 関数（aesthetics）で視覚化に用いる変数とプロット要素間の接続を図ることも多い。

ggplot関数で作成された図示化オブジェクトには、着目するデータと変数が特定されている。続いて、ggplot()で作られたオブジェクトに対して、geom (geometry) 用いてグラフィックの層(layer)を加えることで図を作成する。このプロセスでは、`geom_point()` による散布図や、`geom_histogram()` によるヒストグラムなど、具体的な図表のタイプに対応する関数を利用することで、図を作成できる。また、geomに関する関数以降に `labs()` というラベルに関する関数を追加することで、図に必要な情報を加筆することが可能になる。

ggplot2を用いたデータ可視化の例として、まず本書はヒストグラムを描画する。ヒストグラムはデータの分布を離散的に示すものであり、連続変数を階級で分けて各階級の頻度を図示化する。一つの変数を扱った図なので、mapping引数ではひとつの変数を指定する。その上で作成した図示化オブジェクトに `geom_histogram()` を追加することでヒストグラムを描画する。以下では、ダイアモンドの価格の観測頻度についての可視化例である。価格の程度を離散的に区切り、その区切られた各範囲の価格を取る観測がデータ内にどれだけ存在するかを示している。


```{r histogram, message=FALSE, }
p1 <- ggplot(diamonds, mapping = aes(x = price))
p1 + geom_histogram() +
  labs(x = "価格", y = "頻度",
       title = "ヒストグラム1: ダイアモンド価格")
```

なお、縦軸を確率密度(density)に変えるときは、geom_density()を用いる。その際、fillという引数を設定すると、密度を範囲に色を塗ることができる (なお、"p1" というオブジェクトは再利用できるので、再びggplot()によって指定する必要はない)。

```{r density, message=FALSE}
p1 + geom_density(fill = "black", alpha = 0.5) +
  labs(x = "価格", y = "頻度",
       title = "ヒストグラム2: ダイアモンド価格（geom_density）")
```

箱ひげ図は、四分位数と四分位範囲等を図示化したもの。四分位数はデータを4等分する区切りの値であり、第一四分位はQ1、第二四分位はQ2、第三四分位はQ3、最大値はQ4で示される。四分位範囲はQ3-Q1の範囲で示されるものである。ここでは、Cutの質（Fair, Good, Very Good, Premium, Ideal）ごとに価格の分布を比べるため、複数の箱ひげ図を並べる例を提示する。

```{r boxplot,message=FALSE}
p2 <- ggplot(diamonds, mapping = aes(x = cut, y = price))
p2 + geom_boxplot() +
  labs(x = "Cutの質", y = "価格",
       title = "箱ひげ図1: ダイアモンド価格")

```

箱ひげ図を作成すると、ひげの上下に点が表示されることがある（上図では上部が太線のように見えている）。これは、外れ値の候補として全体の分布から離れて存在する観測値が示されている。ここで示される外れ値の候補は、Q1よりも四分位範囲$\times 1.5\times 1.5$ 以上小さい、ないしは、Q3よりも四分位範囲$\times 1.5\times 1.5$ 以上大きいかで特定される。外れ値がある場合、入力ミスなどのエラーではないか、異質な観測値でないか、を検討、確認することが必要になる。

バイオリンプロットは、箱ひげ図よりももう少し詳しくデータの分布を確認できる図である。ggplot2では、`geom_violin()` を用いる。例えば、先程の箱ひげ図をバイオリンプロットで示すと、以下のようになる。以下の図は、バイオリンプロット内に箱ひげ図を示すことでよりわかりやすい図を作成するように工夫している。

```{r ciolin,message=FALSE}
p2 + geom_violin() +
  geom_boxplot(fill = "gray", width = 0.1) +
  labs(x = "Cutの質", y = "価格",
       title = "バイオリンプロット: ダイアモンド価格")

```

バイオリンプロットで横に広がっているところは、ヒストグラムで言う山が高いところを意味しており、そこに多くのデータが集まっていることを示している。

## 二変数間の関係の要約
ここまでの内容は（カテゴリ変数に関する一部の説明を除き）、一つの変数に関する要約と可視化を扱っていた。しかし、データ分析では二つの異なる変数間の関係を捉えたいと考えることも多い。二変数間の関係を数量的に要約するための指標の代表例が共分散や相関係数である。データ数をnとする変数xとyの共分散（$S_{xy}$）は、以下のように定義される。なお、Rで共分散を求める際には `cov()` 関数を用いる。

$S_{xy}=\frac{1}{n}\sum_i^n (x_i-\bar{x})(y_i-\bar{y})$

また、$S_x$と$S_y$をそれぞれxとyの分散とし、相関係数（$\rho_{xy}$）は以下のように定義される。Rで相関係数を求める際には `cor()` 関数を用いる。

$\rho_{xy}=\frac{S_{xy}}{\sqrt{S_x^2}\cdot \sqrt{S_y^2}}$

共分散は、二つのデータ間の共変動を示す指標であるものの、この数値を持って我々研究者が二変数の関係について（例えばその強弱などを）解釈するのは困難である。そこで、二変数間の関係を数値的に解釈する場合には、一般的に相関係数を用いる。相関係数は、-1 から 1 までの値を取り、正の値を取る場合は正の相関、負の値を取る場合は負の相関を、着目している二つの変数が持つことが知られている。また、相関係数が正（負）の値かつ1に近いほど強い正（負）の相関であることが知られている。ただし、相関係数で表される二変数間の関係は、線形関係の程度である。言い換えると、相関が高いとはデータがどれだけ直線上に集まって分布しているかを示しており、グラフ等で示される線形関係の傾きについては何も回答することができないという点に注意が必要である。
例えば、以下のようなデータセットを考える。

```{r datasetup}
X <- tibble(x1 = c(-3, -1, 0, 2, 5), y1 = c(16, 12, 10, 6, 0), y2 = c(8, 6, 5, 3, 0))
X
```

このデータセットにおける x1 と y1 の相関係数は -1 であり、両者の関係を図で示すと、すべてのデータが直線上（$y=-2x+10$）に並ぶことがわかる。一方で、x1 と y2 との相関係数も -1 であるものの、両者の線形関係は $y=-x+5$である。このことからも、相関係数が線形関数の傾きや切片についての情報は何も持たないことがわかる。

```{r, message=FALSE}
cor(X$x1, X$y1)

ggplot(data = X, mapping = aes(x = x1, y = y1)) + 
  geom_point() +
  geom_smooth(method = lm)
```

また我々は、二変数間の相関係数がゼロであることが、両者が無関係であることを意味しないことにも注意をしなければならない。例えば、以下のようなデータセットにおけるA と B の相関は 0 になる。

```{r, echo=FALSE}
AB <- tibble(A = c(-2, -1, 0, 1, 2), B = c(4, 1, 0, 1, 4))
AB
```

```{r}
cor(AB$A, AB$B)
```
しかしながら、両者の関係を描画すると、$y = x^2$ という二次関数の関係にあることがわかる。つまり、相関係数がゼロだからといって、二つの変数間に関係がないと結論づける事はできず、相関ではなく異なる複数の分析アプローチによって関係を特定していくことが必要になる。

```{r}
ggplot(data = AB, mapping = aes(x = A, y = B)) + 
  geom_point() + 
  geom_smooth(method = lm, formula = y ~ x + I(x^2), se = FALSE)
```

勘の良い読者であればすでに気づいているかも知れないが、二変数間の関係についての可視化もggplot2にて対応できる。具体的には、`geom_point()`という関数を用いるのだが、それだけではなく、mappingに対する引数として、x と y 二つの変数を指定することが必要になる。ダイアモンドの価格は、カラット数に大きく依存すると考えられる。そこで、以下のようにカラット数と価格との間の共分散と相関を計算する。

```{r diamondcov}
cov(diamonds$carat,diamonds$price)
cor(diamonds$carat,diamonds$price)
```

これらの変数間の相関係数は約0.92であり、高い正の相関関係であることが確認された。続いて、これらの変数の関係を可視化する。二変数間の関係を端的に可視化する方法が散布図である。散布図は、一方の変数を横軸に、もう一方の変数を縦軸に取り、各データのそれぞれの値の組み合わせをプロットしたものである。

```{r plotnormal, message=FALSE}
p3 <- ggplot(diamonds, mapping = aes(x = carat, y = price))
p3 + geom_point() +
  labs(x = "カラット", y = "価格",
       title = "散布図1: カラット：価格")

```

研究目的次第では、二つの変数間の関係をカテゴリごとに比較したい場合もあるだろう。例えば、我々はカラットと価格の関係は、カットの質によって変わるのか、という問いに関心があるとしよう。その場合には、(1) 同一図内にてカテゴリごとに色分けする方法と、(2) カテゴリごとに分割して図示化する方法がある。それぞれのggplot2での実行方法は、以下のとおりである。

1. Mapping = aes() 内に、 color = categ_varと指定することで、categ_var変数のカテゴリに基づき色分けする。
2. facet_grid() や facet_wrap() を用いる。

まず、(1) の図内での色分け方法は、以下のようなコマンドで実行できる。

```{r plotcolor, message = FALSE}
p4 <- ggplot(diamonds, mapping = aes(x = carat, y = price, color = cut))
p4 + geom_point() +
  labs(x = "カラット", y = "価格", color = "カット",
       title = "散布図 2: カット別、カラット：価格")

```

このように、`mapping = aes()` 内にて色付けに関する引数を設定することで散布図内の観測値を色分けできる。ただし、ここで重要なのは、`color = `という引数では、カテゴリ変数を指定すべきであり、色そのもの（例えば、redやblue）を指定するものではないということである。しかしながら、散布図 2のように多くのカテゴリが含まれる場合には、この可視化の方法だと逆に見にくいかもしれない。そこで、以下の方法を紹介する。`facet_wrap()` を用いた図の作成では、散布図 2のように color 引数を指定する必要はなく、p3 を再利用できる。`geom_point()` で散布図作成の指示を与えたあとに、`facet_wrap()` のレイヤーを重ねる指示を与えれば、散布図 3が作成される。

```{r plotfacet, message = FALSE}
p3 + geom_point() + facet_wrap(~cut) +
  labs(x = "カラット", y = "価格",
       title = "散布図 3: カット別、カラット：価格")

```

散布図 3をみると、基本的にはカラット数と価格には正の相関があるものの、カットの質が低い（例、Fair）場合にはばらつきが大きいことがうかがえる。

これまでに学んだdplyrによるデータ処理方法をパイプ演算子でつなげることで、特定の群のみを対象にした図示化も容易になる。ここでは例として、1.00カラット以上と未満とで分けて、それぞれのヒストグラムを作成してみる。

```{r pipegg, message=FALSE}
p5 <- diamonds %>% 
  filter(carat >= 1.0) %>% 
  ggplot(mapping = aes(x = price))
p5 + geom_histogram() +
  labs(x = "価格", y = "頻度",
       title = "ヒストグラム:1.00カラット以上")
p6 <- diamonds %>% 
  filter(carat < 1.0) %>% 
  ggplot(mapping = aes(x = price))
p6 + geom_histogram() +
  labs(x = "価格", y = "頻度",
       title = "ヒストグラム:1.00カラット未満")

```

Rで図を作成したら保存（出力）したいと考えることも多いだろう。日本語を使っていない図はggsaveを使い簡単に保存できる。具体的には、まず、作成した図そのもの（図示化のためのggplot() オブジェクトではない）をオブジェクトとして定義（例、plot1）する。ggsaveの使用例は以下の様になる (以下は見本コード)。

```{r eval=FALSE}
ggsave(filename = "plot1.pdf", 
 plot = plot1, width = 10, height = 5, units = "cm")
```

日本語を含む頭の場合、`quartz()` を用いた以下の手順を経て図を保存する。
1. quartz()で作図デバイスを起動する。
2. 作図デバイスを開いたまま、Rstudio内で図を表示する。
3. dev.off()という指示で作図デバイスを閉じることで図が保存される。


また、Rstudio内のplotタブから、クリック-バイ-クリックで実行することも可能である（Export -> Save as Image/ Save as PDF -> Directory -> File name）。

## 練習問題

- “reshape2” パッケージに含まれる“tips” データを使い以下の分析を実行しよう。
  1. 支出額とチップのヒストグラムを作成
  2. 支出額とチップの平均値を計算
  3. 支出額とチップの分散を計算
  4. 支出額とチップの共分散と相関を計算し、散布図を作成



## 参考文献
倉田博史・星野崇宏（2011）「入門統計解析」、新世社.

高橋将宜・渡辺美智子 (2017). 「欠測データ処理」, 共立出版.

松村優哉・湯谷啓明・紀ノ定保礼・前田和寛（2021）「改訂2版 RユーザのためのRStudio[実践]入門〜tidyverseによるモダンな分析フローの世界」，技術評論社.

Healy, Kieran (2018) *Data Visualization: A Practical Introduction*, Princeton University Press.
